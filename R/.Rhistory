geom_line() +
geom_point() +
scale_color_manual(values = c("orange", "red","blue")) +
labs(
x = "Number of Splits", y = "Mean Squared Error",
title = "MSE vs Tree Complexity",
color = "MSE Type"
) +
theme_minimal()
adj = mse_data[1,2]
len = length(cp_table[,"nsplit"])
data_conc = data.frame(
rep(cp_table[, "nsplit"],3),
c(cp_table[, "xerror"]*adj,
cp_table[, "rel error"]*adj,
c(mse_values[2,])
),
c(rep("Cross validation Error",len),
rep("Training Error",len),
rep("Test Error",len)
)
)
ggplot(data_conc, aes(x = data_conc[,1], y = data_conc[,2], color = data_conc[,3])) +
geom_line() +
geom_point() +
scale_color_manual(values = c("orange", "red","blue")) +
labs(
x = "Number of Splits", y = "Mean Squared Error",
title = "MSE vs Tree Complexity",
color = "MSE Type"
) +
theme_minimal()
adj = mse_data[1,2]
adj = mse_data[1,2]
len = length(cp_table[,"nsplit"])
data_conc = data.frame(
rep(cp_table[, "nsplit"],3),
c(cp_table[, "xerror"]*adj,
cp_table[, "rel error"]*adj,
c(mse_values[2,])
),
c(rep("Cross validation Error",len),
rep("Training Error",len),
rep("Test Error",len)
)
)
ggplot(data_conc, aes(x = data_conc[,1], y = data_conc[,2], color = data_conc[,3])) +
geom_line() +
geom_point() +
scale_color_manual(values = c("orange", "red","blue")) +
labs(
x = "Number of Splits", y = "Mean Squared Error",
title = "MSE vs Tree Complexity",
color = "MSE Type"
) +
theme_minimal()
# Split the data into training (70%) and validation (30%) sets
split_index <- createDataPartition(sd$G3, p = 0.7, list = FALSE)
train_data <- sd[split_index, ]
valid_data <- sd[-split_index, ]
# Create a complex tree
complex_tree <- rpart(G3 ~ ., data = train_data, control = rpart.control(cp = 0.002, minsplit = 5))
rpart.plot(complex_tree, main = "Initial Complex Regression Tree")
plotcp(complex_tree)
# Split the data into training (70%) and validation (30%) sets
split_index <- createDataPartition(sd$G3, p = 0.7, list = FALSE)
train_data <- sd[split_index, ]
valid_data <- sd[-split_index, ]
# Create a complex tree
complex_tree <- rpart(G3 ~ ., data = train_data, control = rpart.control(cp = 0.002, minsplit = 5))
rpart.plot(complex_tree, main = "Initial Complex Regression Tree")
plotcp(complex_tree)
# Find optimal CP value
opt_cp   <- complex_tree$cptable[which.min(complex_tree$cptable[, "xerror"]), "CP"]
opt_sp_1 <- complex_tree$cptable[which.min(complex_tree$cptable[, "xerror"]), "nsplit"]
print(opt_sp_1) # for xerror using cross validation
print(opt_cp) # for xerror
print(min(complex_tree$cptable[, "xerror"]))
print(min(complex_tree$cptable[, "rel error"]))
# Prune the tree
pruned_tree <- prune(complex_tree, cp = opt_cp)
rpart.plot(pruned_tree, main = "Optimal Pruned Tree")
# Split the data into training (70%) and validation (30%) sets
split_index <- createDataPartition(sd$G3, p = 0.7, list = FALSE)
train_data <- sd[split_index, ]
valid_data <- sd[-split_index, ]
# Create a complex tree
complex_tree <- rpart(G3 ~ ., data = train_data, control = rpart.control(cp = 0.002, minsplit = 5))
rpart.plot(complex_tree, main = "Initial Complex Regression Tree")
plotcp(complex_tree)
# Split the data into training (70%) and validation (30%) sets
split_index <- createDataPartition(sd$G3, p = 0.7, list = FALSE)
train_data <- sd[split_index, ]
valid_data <- sd[-split_index, ]
# Split the data into training (70%) and validation (30%) sets
split_index <- createDataPartition(sd$G3, p = 0.7, list = FALSE)
train_data <- sd[split_index, ]
valid_data <- sd[-split_index, ]
# Create a complex tree
complex_tree <- rpart(G3 ~ ., data = train_data, control = rpart.control(cp = 0.002, minsplit = 5))
rpart.plot(complex_tree, main = "Initial Complex Regression Tree")
plotcp(complex_tree)
# Find optimal CP value
opt_cp   <- complex_tree$cptable[which.min(complex_tree$cptable[, "xerror"]), "CP"]
opt_sp_1 <- complex_tree$cptable[which.min(complex_tree$cptable[, "xerror"]), "nsplit"]
print(opt_sp_1) # for xerror using cross validation
print(opt_cp) # for xerror
print(min(complex_tree$cptable[, "xerror"]))
print(min(complex_tree$cptable[, "rel error"]))
# Prune the tree
pruned_tree <- prune(complex_tree, cp = opt_cp)
rpart.plot(pruned_tree, main = "Optimal Pruned Tree")
# Calculate the MSEs
MSE_complex <- function() {
train_pred_complex <- predict(complex_tree, train_data)
test_pred_complex <- predict(complex_tree, valid_data)
train_mse_complex <- calculate_mse(train_data$G3, train_pred_complex)
test_mse_complex <- calculate_mse(valid_data$G3, test_pred_complex)
print(paste("Complex Tree - Training MSE:", train_mse_complex))
print(paste("Complex Tree -     Test MSE:", test_mse_complex))
}
MSE_pruned <- function() {
train_pred_pruned <- predict(pruned_tree, train_data)
test_pred_pruned <- predict(pruned_tree, valid_data)
train_mse_pruned <- calculate_mse(train_data$G3, train_pred_pruned)
test_mse_pruned <- calculate_mse(valid_data$G3, test_pred_pruned)
print(paste("Pruned Tree  - Training MSE:", train_mse_pruned))
print(paste("Pruned Tree  -     Test MSE:", test_mse_pruned))
}
MSE_complex()
MSE_pruned()
cp_table <- complex_tree$cptable
num_splits <- cp_table[, "nsplit"]
plot(
cp_table[, "nsplit"],
cp_table[, "xerror"]
)
# Calculate MSE for each CP value
mse_values <- sapply(cp_table[, "CP"], function(cp) {
pruned <- prune(complex_tree, cp = cp)
train_pred <- predict(pruned, train_data)
test_pred <- predict(pruned, valid_data)
train_mse <- calculate_mse(train_data$G3, train_pred)
test_mse <- calculate_mse(valid_data$G3, test_pred)
c(train_mse, test_mse)
})
# Making data for ggplot
mse_data <- data.frame(
num_splits = rep(num_splits, 2),
mse = c(mse_values[1, ], mse_values[2, ]),
type = rep(c("Training MSE", "Test MSE"), each = length(num_splits))
)
temp_mse_data <- mse_data %>% filter(type == "Test MSE")
opt_splits <- temp_mse_data$num_splits[which.min(temp_mse_data$mse)]
min_mse <- min(temp_mse_data$mse)
print(opt_splits)
print(min_mse) # Should both be the same as in previous cells
ggplot(mse_data, aes(x = num_splits, y = mse, color = type)) +
geom_line() +
geom_point() +
scale_color_manual(values = c("blue", "red")) +
labs(
x = "Number of Splits", y = "Mean Squared Error",
title = "MSE vs Tree Complexity",
color = "MSE Type"
) +
theme_minimal() +
geom_vline(xintercept = opt_splits, linetype = "dashed", color = "purple") +
annotate("text",
x = opt_splits + 18, y = min_mse -1 , label = paste("Optimal number of splits =", opt_splits),
vjust = -1, color = "purple"
)
adj = mse_data[1,2]
len = length(cp_table[,"nsplit"])
data_conc = data.frame(
rep(cp_table[, "nsplit"],3),
c(cp_table[, "xerror"]*adj,
cp_table[, "rel error"]*adj,
c(mse_values[2,])
),
c(rep("Cross validation Error",len),
rep("Training Error",len),
rep("Test Error",len)
)
)
ggplot(data_conc, aes(x = data_conc[,1], y = data_conc[,2], color = data_conc[,3])) +
geom_line() +
geom_point() +
scale_color_manual(values = c("orange", "red","blue")) +
labs(
x = "Number of Splits", y = "Mean Squared Error",
title = "MSE vs Tree Complexity",
color = "MSE Type"
) +
theme_minimal()
# Split the data into training (70%) and validation (30%) sets
split_index <- createDataPartition(sd$G3, p = 0.7, list = FALSE)
train_data <- sd[split_index, ]
valid_data <- sd[-split_index, ]
# Create a complex tree
complex_tree <- rpart(G3 ~ ., data = train_data, control = rpart.control(cp = 0.002, minsplit = 5))
rpart.plot(complex_tree, main = "Initial Complex Regression Tree")
plotcp(complex_tree)
# Split the data into training (70%) and validation (30%) sets
split_index <- createDataPartition(sd$G3, p = 0.7, list = FALSE)
train_data <- sd[split_index, ]
valid_data <- sd[-split_index, ]
# Create a complex tree
complex_tree <- rpart(G3 ~ ., data = train_data, control = rpart.control(cp = 0.002, minsplit = 5))
rpart.plot(complex_tree, main = "Initial Complex Regression Tree")
plotcp(complex_tree)
tree_model <- rpart(G3 ~ ., data = train_data, control = rpart.control(cp = 0.025, minsplit = 5))
rpart.plot(tree_model, fallen.leaves = TRUE)
# Split the data into training (70%) and validation (30%) sets
split_index <- createDataPartition(sd$G3, p = 0.7, list = FALSE)
train_data <- sd[split_index, ]
valid_data <- sd[-split_index, ]
# Create a complex tree
complex_tree <- rpart(G3 ~ ., data = train_data, control = rpart.control(cp = 0.002, minsplit = 5))
rpart.plot(complex_tree, main = "Initial Complex Regression Tree")
plotcp(complex_tree)
# Find optimal CP value
opt_cp   <- complex_tree$cptable[which.min(complex_tree$cptable[, "xerror"]), "CP"]
opt_sp_1 <- complex_tree$cptable[which.min(complex_tree$cptable[, "xerror"]), "nsplit"]
print(opt_sp_1) # for xerror using cross validation
print(opt_cp) # for xerror
print(min(complex_tree$cptable[, "xerror"]))
print(min(complex_tree$cptable[, "rel error"]))
# Prune the tree
pruned_tree <- prune(complex_tree, cp = opt_cp)
rpart.plot(pruned_tree, main = "Optimal Pruned Tree")
# Calculate the MSEs
MSE_complex <- function() {
train_pred_complex <- predict(complex_tree, train_data)
test_pred_complex <- predict(complex_tree, valid_data)
train_mse_complex <- calculate_mse(train_data$G3, train_pred_complex)
test_mse_complex <- calculate_mse(valid_data$G3, test_pred_complex)
print(paste("Complex Tree - Training MSE:", train_mse_complex))
print(paste("Complex Tree -     Test MSE:", test_mse_complex))
}
MSE_pruned <- function() {
train_pred_pruned <- predict(pruned_tree, train_data)
test_pred_pruned <- predict(pruned_tree, valid_data)
train_mse_pruned <- calculate_mse(train_data$G3, train_pred_pruned)
test_mse_pruned <- calculate_mse(valid_data$G3, test_pred_pruned)
print(paste("Pruned Tree  - Training MSE:", train_mse_pruned))
print(paste("Pruned Tree  -     Test MSE:", test_mse_pruned))
}
MSE_complex()
MSE_pruned()
tree_model <- rpart(G3 ~ ., data = train_data, control = rpart.control(cp = opt_cp-0.0001 , minsplit = 5))
rpart.plot(tree_model, fallen.leaves = TRUE)
# Create REGR
lm_model <- lm(G3 ~ ., data = train_data)
summary(lm_model)
# Prepare data for BART
x_train <- train_data[, !names(train_data) %in% "G3"]
y_train <- train_data$G3
x_valid <- valid_data[, !names(valid_data) %in% "G3"]
y_valid <- valid_data$G3
# Fit BART model
bart_model <- wbart(
x.train = x_train,
y.train = y_train,
x.test = x_valid,
nskip = 1000,
ntree = 200,
ndpost = 1000,
printevery = 500)
# Print results
comp <- function() {
cat("Training MSE   Tree:", train_mse_tree, "\n")
cat("Validation MSE Tree:", valid_mse_tree, "\n\n")
cat("Training MSE   Regr:", train_mse_regr, "\n")
cat("Validation MSE Regr:", valid_mse_regr, "\n\n")
cat("Training MSE   BART:", train_mse_bart, "\n")
cat("Validation MSE BART:", valid_mse_bart, "\n")
}
# tree error
# Calculate MSE for training set
train_pred_tree <- predict(tree_model, train_data)
train_mse_tree <- mean((train_data$G3 - train_pred_tree)^2)
# Calculate MSE for validation set
valid_pred_tree <- predict(tree_model, valid_data)
valid_mse_tree <- mean((valid_data$G3 - valid_pred_tree)^2)
# regr error
# Calculate MSE for training set
train_pred_regr <- predict(lm_model, train_data)
train_mse_regr <- mean((train_data$G3 - train_pred_regr)^2)
# Calculate MSE for validation set
valid_pred_regr <- predict(lm_model, valid_data)
valid_mse_regr <- mean((valid_data$G3 - valid_pred_regr)^2)
# BART error
# Calculate MSE for training set
train_pred_bart <- bart_model$yhat.train.mean
train_mse_bart <- mean((y_train - train_pred_bart)^2)
# Calculate MSE for validation set
valid_pred_bart <- bart_model$yhat.test.mean
valid_mse_bart <- mean((y_valid - valid_pred_bart)^2)
comp()
tree_model <- rpart(G3 ~ ., data = train_data, control = rpart.control(cp = 0.025, minsplit = 5))
rpart.plot(tree_model, fallen.leaves = TRUE)
# Split the data into training (70%) and validation (30%) sets
split_index <- createDataPartition(sd$G3, p = 0.7, list = FALSE)
train_data <- sd[split_index, ]
valid_data <- sd[-split_index, ]
# Create a complex tree
complex_tree <- rpart(G3 ~ ., data = train_data, control = rpart.control(cp = 0.002, minsplit = 5))
rpart.plot(complex_tree, main = "Initial Complex Regression Tree")
plotcp(complex_tree)
# Find optimal CP value
opt_cp   <- complex_tree$cptable[which.min(complex_tree$cptable[, "xerror"]), "CP"]
opt_sp_1 <- complex_tree$cptable[which.min(complex_tree$cptable[, "xerror"]), "nsplit"]
print(opt_sp_1) # for xerror using cross validation
print(opt_cp) # for xerror
print(min(complex_tree$cptable[, "xerror"]))
print(min(complex_tree$cptable[, "rel error"]))
# Prune the tree
pruned_tree <- prune(complex_tree, cp = opt_cp)
rpart.plot(pruned_tree, main = "Optimal Pruned Tree")
# Calculate the MSEs
MSE_complex <- function() {
train_pred_complex <- predict(complex_tree, train_data)
test_pred_complex <- predict(complex_tree, valid_data)
train_mse_complex <- calculate_mse(train_data$G3, train_pred_complex)
test_mse_complex <- calculate_mse(valid_data$G3, test_pred_complex)
print(paste("Complex Tree - Training MSE:", train_mse_complex))
print(paste("Complex Tree -     Test MSE:", test_mse_complex))
}
MSE_pruned <- function() {
train_pred_pruned <- predict(pruned_tree, train_data)
test_pred_pruned <- predict(pruned_tree, valid_data)
train_mse_pruned <- calculate_mse(train_data$G3, train_pred_pruned)
test_mse_pruned <- calculate_mse(valid_data$G3, test_pred_pruned)
print(paste("Pruned Tree  - Training MSE:", train_mse_pruned))
print(paste("Pruned Tree  -     Test MSE:", test_mse_pruned))
}
MSE_complex()
MSE_pruned()
cp_table <- complex_tree$cptable
num_splits <- cp_table[, "nsplit"]
plot(
cp_table[, "nsplit"],
cp_table[, "xerror"]
)
# Calculate MSE for each CP value
mse_values <- sapply(cp_table[, "CP"], function(cp) {
pruned <- prune(complex_tree, cp = cp)
train_pred <- predict(pruned, train_data)
test_pred <- predict(pruned, valid_data)
train_mse <- calculate_mse(train_data$G3, train_pred)
test_mse <- calculate_mse(valid_data$G3, test_pred)
c(train_mse, test_mse)
})
# Making data for ggplot
mse_data <- data.frame(
num_splits = rep(num_splits, 2),
mse = c(mse_values[1, ], mse_values[2, ]),
type = rep(c("Training MSE", "Test MSE"), each = length(num_splits))
)
temp_mse_data <- mse_data %>% filter(type == "Test MSE")
opt_splits <- temp_mse_data$num_splits[which.min(temp_mse_data$mse)]
min_mse <- min(temp_mse_data$mse)
print(opt_splits)
print(min_mse) # Should both be the same as in previous cells
ggplot(mse_data, aes(x = num_splits, y = mse, color = type)) +
geom_line() +
geom_point() +
scale_color_manual(values = c("blue", "red")) +
labs(
x = "Number of Splits", y = "Mean Squared Error",
title = "MSE vs Tree Complexity",
color = "MSE Type"
) +
theme_minimal() +
geom_vline(xintercept = opt_splits, linetype = "dashed", color = "purple") +
annotate("text",
x = opt_splits + 18, y = min_mse -1 , label = paste("Optimal number of splits =", opt_splits),
vjust = -1, color = "purple"
)
adj = mse_data[1,2]
len = length(cp_table[,"nsplit"])
data_conc = data.frame(
rep(cp_table[, "nsplit"],3),
c(cp_table[, "xerror"]*adj,
cp_table[, "rel error"]*adj,
c(mse_values[2,])
),
c(rep("Cross validation Error",len),
rep("Training Error",len),
rep("Test Error",len)
)
)
ggplot(data_conc, aes(x = data_conc[,1], y = data_conc[,2], color = data_conc[,3])) +
geom_line() +
geom_point() +
scale_color_manual(values = c("orange", "red","blue")) +
labs(
x = "Number of Splits", y = "Mean Squared Error",
title = "MSE vs Tree Complexity",
color = "MSE Type"
) +
theme_minimal()
tree_model <- rpart(G3 ~ ., data = train_data, control = rpart.control(cp = opt_cp , minsplit = 5))
rpart.plot(tree_model, fallen.leaves = TRUE)
# Create REGR
lm_model <- lm(G3 ~ ., data = train_data)
summary(lm_model)
# Prepare data for BART
x_train <- train_data[, !names(train_data) %in% "G3"]
y_train <- train_data$G3
x_valid <- valid_data[, !names(valid_data) %in% "G3"]
y_valid <- valid_data$G3
# Fit BART model
bart_model <- wbart(
x.train = x_train,
y.train = y_train,
x.test = x_valid,
nskip = 1000,
ntree = 200,
ndpost = 1000,
printevery = 500)
# Print results
comp <- function() {
cat("Training MSE   Tree:", train_mse_tree, "\n")
cat("Validation MSE Tree:", valid_mse_tree, "\n\n")
cat("Training MSE   Regr:", train_mse_regr, "\n")
cat("Validation MSE Regr:", valid_mse_regr, "\n\n")
cat("Training MSE   BART:", train_mse_bart, "\n")
cat("Validation MSE BART:", valid_mse_bart, "\n")
}
# tree error
# Calculate MSE for training set
train_pred_tree <- predict(tree_model, train_data)
train_mse_tree <- mean((train_data$G3 - train_pred_tree)^2)
# Calculate MSE for validation set
valid_pred_tree <- predict(tree_model, valid_data)
valid_mse_tree <- mean((valid_data$G3 - valid_pred_tree)^2)
# regr error
# Calculate MSE for training set
train_pred_regr <- predict(lm_model, train_data)
train_mse_regr <- mean((train_data$G3 - train_pred_regr)^2)
# Calculate MSE for validation set
valid_pred_regr <- predict(lm_model, valid_data)
valid_mse_regr <- mean((valid_data$G3 - valid_pred_regr)^2)
# BART error
# Calculate MSE for training set
train_pred_bart <- bart_model$yhat.train.mean
train_mse_bart <- mean((y_train - train_pred_bart)^2)
# Calculate MSE for validation set
valid_pred_bart <- bart_model$yhat.test.mean
valid_mse_bart <- mean((y_valid - valid_pred_bart)^2)
comp()
comp()
bart_minim <- wbart(
x.train = x_train,
y.train = y_train,
x.test = x_valid,
nskip = 100,
ntree = 100,
ndpost = 50,
printevery = 500)
fitmat <- cbind(train_data$G3, bart_model$yhat.train.mean, bart_minim$yhat.train.mean)
pairs(fitmat)
cor(fitmat)
# compute row percentages
percount <- bart_model$varcount / apply(bart_model$varcount, 1, sum)
# mean of row percentages
mvp <- apply(percount, 2, mean)
# quantiles of row percentags
qm <- apply(percount, 2, quantile, probs = c(0.3, 0.7))
# Assuming qm is a matrix or data frame
p <- ncol(qm)
rgy <- range(qm, na.rm = TRUE)
# Create the plot
plot(c(1, p), rgy,
type = "n", xlab = "variable",
ylab = "post mean, percent var use", axes = FALSE
)
# Add x-axis
axis(1, at = 1:p, labels = colnames(qm), cex.lab = 0.7, cex.axis = 0.7)
# Add y-axis
axis(2, cex.lab = 1.2, cex.axis = 1.2)
lines(1:p, mvp, col = "black", lty = 4, pch = 4, type = "b", lwd = 1.5)
# Add vertical lines
for (i in 1:p) {
lines(c(i, i), qm[, i], col = "blue", lty = 3, lwd = 1.0)
}
bart_minim <- wbart(
x.train = x_train,
y.train = y_train,
x.test = x_valid,
nskip = 100,
ntree = 100,
ndpost = 50,
printevery = 500)
fitmat <- cbind(train_data$G3, bart_model$yhat.train.mean, bart_minim$yhat.train.mean)
pairs(fitmat)
cor(fitmat)
# BART error
# Calculate MSE for training set
train_pred_bart <- bart_minim$yhat.train.mean
train_mse_bart <- mean((y_train - train_pred_bart)^2)
# Calculate MSE for validation set
valid_pred_bart <- bart_minim$yhat.test.mean
valid_mse_bart <- mean((y_valid - valid_pred_bart)^2)
print(train_mse_bart, valid_mse_bart)
print(train_mse_bart)
print(train_mse_bart)
print(valid_mse_bart)
print(train_mse_bart)
print(valid_mse_bart)
