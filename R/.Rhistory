geom_text(aes(label = round(G3_pred, 1)), size = 3) +
scale_fill_gradient(low = "darkred", high = "lightblue", guide = "none") +
labs(x = "Alcohol consumption", y = "Absences", title = "Predicting Exam score based on alcohol consumption and Absences") +
theme_minimal() +
geom_point(data = sd_mini, aes(x = Walc_plus_Dalc, y = absences), color = "blue", size = 2, inherit.aes = FALSE)# Add blue points for actual data
tree_model <- rpart(G3 ~ Walc_plus_Dalc + absences, data = sd_mini, control = rpart.control(cp = 0.0026, minsplit = 19 ))
rpart.plot(tree_model)
Walc_plus_Dalc_seq <- seq(min(sd_mini$Walc_plus_Dalc), max(sd_mini$Walc_plus_Dalc), length.out = 13)
absences_seq <- seq(min(sd_mini$absences), max(sd_mini$absences), length.out = 13)
grid <- expand.grid(Walc_plus_Dalc = Walc_plus_Dalc_seq, absences = absences_seq)
grid$G3_pred <- predict(tree_model, newdata = grid)
ggplot(grid, aes(x = Walc_plus_Dalc, y = absences, fill = G3_pred)) +
geom_tile() +
geom_text(aes(label = round(G3_pred, 1)), size = 3) +
scale_fill_gradient(low = "darkred", high = "lightblue", guide = "none") +
labs(x = "Alcohol consumption", y = "Absences", title = "Predicting Exam score based on alcohol consumption and Absences") +
theme_minimal() +
geom_point(data = sd_mini, aes(x = Walc_plus_Dalc, y = absences), color = "blue", size = 2, inherit.aes = FALSE)# Add blue points for actual data
tree_model <- rpart(G3 ~ Walc_plus_Dalc + absences, data = sd_mini, control = rpart.control(cp = 0.0026, minsplit = 22 ))
rpart.plot(tree_model)
Walc_plus_Dalc_seq <- seq(min(sd_mini$Walc_plus_Dalc), max(sd_mini$Walc_plus_Dalc), length.out = 13)
absences_seq <- seq(min(sd_mini$absences), max(sd_mini$absences), length.out = 13)
grid <- expand.grid(Walc_plus_Dalc = Walc_plus_Dalc_seq, absences = absences_seq)
grid$G3_pred <- predict(tree_model, newdata = grid)
ggplot(grid, aes(x = Walc_plus_Dalc, y = absences, fill = G3_pred)) +
geom_tile() +
geom_text(aes(label = round(G3_pred, 1)), size = 3) +
scale_fill_gradient(low = "darkred", high = "lightblue", guide = "none") +
labs(x = "Alcohol consumption", y = "Absences", title = "Predicting Exam score based on alcohol consumption and Absences") +
theme_minimal() +
geom_point(data = sd_mini, aes(x = Walc_plus_Dalc, y = absences), color = "blue", size = 2, inherit.aes = FALSE)# Add blue points for actual data
tree_model <- rpart(G3 ~ Walc_plus_Dalc + absences, data = sd_mini, control = rpart.control(cp = 0.0026, minsplit = 33 ))
rpart.plot(tree_model)
Walc_plus_Dalc_seq <- seq(min(sd_mini$Walc_plus_Dalc), max(sd_mini$Walc_plus_Dalc), length.out = 13)
absences_seq <- seq(min(sd_mini$absences), max(sd_mini$absences), length.out = 13)
grid <- expand.grid(Walc_plus_Dalc = Walc_plus_Dalc_seq, absences = absences_seq)
grid$G3_pred <- predict(tree_model, newdata = grid)
ggplot(grid, aes(x = Walc_plus_Dalc, y = absences, fill = G3_pred)) +
geom_tile() +
geom_text(aes(label = round(G3_pred, 1)), size = 3) +
scale_fill_gradient(low = "darkred", high = "lightblue", guide = "none") +
labs(x = "Alcohol consumption", y = "Absences", title = "Predicting Exam score based on alcohol consumption and Absences") +
theme_minimal() +
geom_point(data = sd_mini, aes(x = Walc_plus_Dalc, y = absences), color = "blue", size = 2, inherit.aes = FALSE)# Add blue points for actual data
tree_model <- rpart(G3 ~ Walc_plus_Dalc + absences, data = sd_mini, control = rpart.control(cp = 0.0026, minsplit = 44 ))
rpart.plot(tree_model)
Walc_plus_Dalc_seq <- seq(min(sd_mini$Walc_plus_Dalc), max(sd_mini$Walc_plus_Dalc), length.out = 13)
absences_seq <- seq(min(sd_mini$absences), max(sd_mini$absences), length.out = 13)
grid <- expand.grid(Walc_plus_Dalc = Walc_plus_Dalc_seq, absences = absences_seq)
grid$G3_pred <- predict(tree_model, newdata = grid)
ggplot(grid, aes(x = Walc_plus_Dalc, y = absences, fill = G3_pred)) +
geom_tile() +
geom_text(aes(label = round(G3_pred, 1)), size = 3) +
scale_fill_gradient(low = "darkred", high = "lightblue", guide = "none") +
labs(x = "Alcohol consumption", y = "Absences", title = "Predicting Exam score based on alcohol consumption and Absences") +
theme_minimal() +
geom_point(data = sd_mini, aes(x = Walc_plus_Dalc, y = absences), color = "blue", size = 2, inherit.aes = FALSE)# Add blue points for actual data
tree_model <- rpart(G3 ~ Walc_plus_Dalc + absences, data = sd_mini, control = rpart.control(cp = 0.0016, minsplit = 44 ))
rpart.plot(tree_model)
tree_model <- rpart(G3 ~ Walc_plus_Dalc + absences, data = sd_mini, control = rpart.control(cp = 0.002, minsplit = 44 ))
rpart.plot(tree_model)
Walc_plus_Dalc_seq <- seq(min(sd_mini$Walc_plus_Dalc), max(sd_mini$Walc_plus_Dalc), length.out = 13)
absences_seq <- seq(min(sd_mini$absences), max(sd_mini$absences), length.out = 13)
grid <- expand.grid(Walc_plus_Dalc = Walc_plus_Dalc_seq, absences = absences_seq)
grid$G3_pred <- predict(tree_model, newdata = grid)
ggplot(grid, aes(x = Walc_plus_Dalc, y = absences, fill = G3_pred)) +
geom_tile() +
geom_text(aes(label = round(G3_pred, 1)), size = 3) +
scale_fill_gradient(low = "darkred", high = "lightblue", guide = "none") +
labs(x = "Alcohol consumption", y = "Absences", title = "Predicting Exam score based on alcohol consumption and Absences") +
theme_minimal() +
geom_point(data = sd_mini, aes(x = Walc_plus_Dalc, y = absences), color = "blue", size = 2, inherit.aes = FALSE)# Add blue points for actual data
tree_model <- rpart(G3 ~ Walc_plus_Dalc + absences, data = sd_mini, control = rpart.control(cp = 0.002, minsplit = 40 ))
rpart.plot(tree_model)
Walc_plus_Dalc_seq <- seq(min(sd_mini$Walc_plus_Dalc), max(sd_mini$Walc_plus_Dalc), length.out = 13)
absences_seq <- seq(min(sd_mini$absences), max(sd_mini$absences), length.out = 13)
grid <- expand.grid(Walc_plus_Dalc = Walc_plus_Dalc_seq, absences = absences_seq)
grid$G3_pred <- predict(tree_model, newdata = grid)
ggplot(grid, aes(x = Walc_plus_Dalc, y = absences, fill = G3_pred)) +
geom_tile() +
geom_text(aes(label = round(G3_pred, 1)), size = 3) +
scale_fill_gradient(low = "darkred", high = "lightblue", guide = "none") +
labs(x = "Alcohol consumption", y = "Absences", title = "Predicting Exam score based on alcohol consumption and Absences") +
theme_minimal() +
geom_point(data = sd_mini, aes(x = Walc_plus_Dalc, y = absences), color = "blue", size = 2, inherit.aes = FALSE)# Add blue points for actual data
###
library(rpart )
###
library(ISLR2)
attach(Carseats)
attach(Carseats)
High <- factor(ifelse(Sales <= 8, "No", "Yes")) # High is yes if at least 8
###
Carseats <- data.frame(Carseats, High)
plot(tree.carseats)
text(tree.carseats)
tree.carseats <- tree(High ~ . - Sales, Carseats)
###
summary(tree.carseats) # 27 terminal nodes RMD=0.4575
tree.carseats <- tree(High ~ . - Sales, Carseats)
tree.carseats <- rpart(High ~ . - Sales, Carseats)
plot(tree.carseats)
text(tree.carseats)
tree.carseats <- rpart(High ~ . - Sales, Carseats)
###
summary(tree.carseats) # 27 terminal nodes RMD=0.4575
# MER = 0.09
###
plot(tree.carseats)
text(tree.carseats, pretty = 0)
###
library(randomForest)
set.seed(1)
bag.boston <- randomForest(medv ~ ., data = Boston,
subset = train, mtry = 12, importance = TRUE)
###
library(rpart )
###
library(ISLR2)
attach(Carseats)
High <- factor(ifelse(Sales <= 8, "No", "Yes")) # High is yes if at least 8
###
Carseats <- data.frame(Carseats, High)
plot(tree.carseats)
text(tree.carseats)
tree.carseats <- rpart(High ~ . - Sales, Carseats)
###
summary(tree.carseats) # 27 terminal nodes RMD=0.4575
# MER = 0.09
###
plot(tree.carseats)
text(tree.carseats, pretty = 0)
###
tree.carseats
###
set.seed(2)
train <- sample(1:nrow(Carseats), 200)
Carseats.test <- Carseats[-train, ]
High.test <- High[-train]
tree.carseats <- tree(High ~ . - Sales, Carseats,
subset = train)
tree.pred <- predict(tree.carseats, Carseats.test,
type = "class")
table(tree.pred, High.test)
(104 + 50) / 200
###
set.seed(7)
cv.carseats <- cv.tree(tree.carseats, FUN = prune.misclass)
names(cv.carseats)
cv.carseats
###
par(mfrow = c(1, 2))
plot(cv.carseats$size, cv.carseats$dev, type = "b")
plot(cv.carseats$k, cv.carseats$dev, type = "b")    # Plots the error rates for size and k
###
prune.carseats <- prune.misclass(tree.carseats, best = 9)
plot(prune.carseats)
text(prune.carseats, pretty = 0)
###
tree.pred <- predict(prune.carseats, Carseats.test,
type = "class")
table(tree.pred, High.test)
(97 + 58) / 200
###
prune.carseats <- prune.misclass(tree.carseats, best = 14)
plot(prune.carseats)
text(prune.carseats, pretty = 0)
tree.pred <- predict(prune.carseats, Carseats.test,
type = "class")
table(tree.pred, High.test)
(102 + 52) / 200
###
set.seed(1)
train <- sample(1:nrow(Boston), nrow(Boston) / 2)
tree_control <- tree.control(nobs = 253, mincut = 1, minsize = 2, mindev = 0)
tree.boston <- tree(medv ~ ., Boston, subset = train,control = tree_control)
summary(tree.boston)
###
plot(tree.boston)
text(tree.boston, pretty = 0)
###
cv.boston <- cv.tree(tree.boston)
plot(cv.boston$size, cv.boston$dev, type = "b")
optimal_size <- cv.boston$size[which.min(cv.boston$dev)]
optimal_size
prune.boston <- prune.tree(tree.boston, best = 5)
plot(prune.boston)
text(prune.boston, pretty = 0)
###
yhat <- predict(tree.boston, newdata = Boston[-train, ])
boston.test <- Boston[-train, "medv"]
plot(yhat, boston.test)
abline(0, 1)
mean((yhat - boston.test)^2)
# Split the data into training and testing sets
set.seed(1)  # for reproducibility
train <- sample(1:nrow(Boston), nrow(Boston)/2)
boston.train <- Boston[train, ]
boston.test <- Boston[-train, ]
# Fit linear regression model
lm.boston <- lm(medv ~ ., data = boston.train)
# Summary of the model
summary(lm.boston)   #        Adjusted R-squared:  0.7528
# Plot residuals
plot(lm.boston)
# Split the data into training and testing sets
set.seed(1)  # for reproducibility
train <- sample(1:nrow(Boston), nrow(Boston)/2)
boston.train <- Boston[train, ]
boston.test <- Boston[-train, ]
# Fit linear regression model
lm.boston <- lm(medv ~ ., data = boston.train)
# Summary of the model
summary(lm.boston)   #        Adjusted R-squared:  0.7528
# Plot residuals
plot(lm.boston)
# Plot predicted vs actual values
plot(yhat, boston.test$medv, main = "Predicted vs Actual Median Value",
xlab = "Predicted", ylab = "Actual")
abline(0, 1, col = "red")  # Add 45-degree line
# Calculate Mean Squared Error (MSE) # [1] "Mean Squared Error: 35.2868818594623"
mse <- mean((yhat - boston.test$medv)^2)
print(paste("Mean Squared Error:", mse))   # [1] "Mean Squared Error: 35.2868818594623"
# Optional: Stepwise regression for feature selection
step.model <- step(lm.boston, direction = "both")
summary(step.model)
###
library(randomForest)
set.seed(1)
set.seed(1)
bag.boston <- randomForest(medv ~ ., data = Boston,
subset = train, mtry = 12, importance = TRUE)
bag.boston
###
yhat.bag <- predict(bag.boston, newdata = Boston[-train, ])
plot(yhat.bag, boston.test)
abline(0, 1)
mean((yhat.bag - boston.test)^2)
###
bag.boston <- randomForest(medv ~ ., data = Boston,
subset = train, mtry = 12, ntree = 25)
yhat.bag <- predict(bag.boston, newdata = Boston[-train, ])
mean((yhat.bag - boston.test)^2)
###
set.seed(1)
rf.boston <- randomForest(medv ~ ., data = Boston,
subset = train, mtry = 6, importance = TRUE)
yhat.rf <- predict(rf.boston, newdata = Boston[-train, ])
mean((yhat.rf - boston.test)^2)
###
importance(rf.boston)
###
varImpPlot(rf.boston)
###
library(gbm)
set.seed(1)
boost.boston <- gbm(medv ~ ., data = Boston[train, ],
distribution = "gaussian", n.trees = 3000,
interaction.depth = 10)
set.seed(1)
boost.boston <- gbm(medv ~ ., data = Boston[train, ],
distribution = "gaussian", n.trees = 3000,
interaction.depth = 10)
###
summary(boost.boston)
###
plot(boost.boston, i = "rm")
plot(boost.boston, i = "lstat")
###
yhat.boost <- predict(boost.boston,
newdata = Boston[-train, ], n.trees = 5000)
mean((yhat.boost - boston.test)^2)
###
boost.boston <- gbm(medv ~ ., data = Boston[train, ],
distribution = "gaussian", n.trees = 5000,
interaction.depth = 4, shrinkage = 0.2, verbose = F)
yhat.boost <- predict(boost.boston,
newdata = Boston[-train, ], n.trees = 5000)
mean((yhat.boost - boston.test)^2)
###
library(BART)
4
x <- Boston[, 1:12]
###
library(BART)
x <- Boston[, 1:12]
y <- Boston[, "medv"]
xtrain <- x[train, ]
ytrain <- y[train]
xtest <- x[-train, ]
ytest <- y[-train]
bartfit <- gbart(xtrain, ytrain, x.test = xtest)
###
yhat.bart <- bartfit$yhat.test.mean
mean((ytest - yhat.bart)^2)
###
ord <- order(bartfit$varcount.mean, decreasing = T)
bartfit$varcount.mean[ord]
library(bartMachine)
install.packages("bartMachine")
# Data-analysis
library(rpart)
library(rpart.plot)
library(dplyr)
library(ggplot2)
library(data.tree)
library(networkD3)
library(plotly)
library(caret)
library(bartMachine)
# installing new package
install.packages("rJava")
library(bartMachine)
# installing new package
install.packages("rJava")
load(rJava)
load("rJava")
library(rJava)
# installing new package
install.packages("bartMachine")
library(bartMachine)
?bartMachine
??bartMachine
# Load packages
library(bartMachine)
# installing new package
install.packages("BART")
library(BART)
library(BART)
?bart
??bart
bart_model <- bartMachine(X = train_data[, -which(names(train_data) == "G3")],
y = train_data$G3,
num_trees = 200,
num_burn = 100,
num_samples = 100,
k = 2,
verbose = TRUE)
bart_model <-
help(bart)
load("C:/Users/timtj/GitHub/wissenschaftliches-arbeiten/R/student_por.RData")
# 1. Setting up data ----
sd <- student_por
# Removing these because that would just be cheating
sd$G2 <- NULL
sd$G1 <- NULL
# also do basic test if data is working
View(sd)
burn = 200; nd = 200
bf = wbart(formula = G3 ~ ., data = sd, nskip = burn, ndpost = nd, printevery = 50)
library(MASS)
attach(Boston)
names(Boston)
view(Boston)
Boston
y = sd$G3
x = sd[, -which(names(sd) == "G3")]
View(x)
p = ncol(x)
bf = wbart(x,y,nskip = burn, ndpost = nd, printevery = 50)
bf = wbart(x,y,nskip=burn,ndpost=nd,printevery=50)
bf = wbart(x,y,nskip=burn,ndpost=nd,printevery=5000) #print progress every 5000th MCMC iteration
sd
View(sd)
y = sd$G3
x = sd[,1:30]
View(x)
p = ncol(x)
bf = wbart(x,y,nskip=burn,ndpost=nd,printevery=50)
library(MASS)
attach(Boston)
names(Boston)
y = Boston$medv
x = Boston[,1:13]
p = ncol(x)
p = ncol(x)
bf = wbart(x,y,nskip=burn,ndpost=nd,printevery=50)
# Using the BART package
burn = 200; nd = 200
y = sd$G3
x = sd[,1:30]
x = data.frame(x)
p = ncol(x)
bf = wbart(x,y,nskip=burn,ndpost=nd,printevery=50)
# 1. Setting up data ----
sd <- data.frame(student_por)
# Removing these because that would just be cheating
sd$G2 <- NULL
# Removing these because that would just be cheating
sd$G2 <- NULL
sd$G1 <- NULL
library(dplyr)
sd <- sd %>%
mutate_if(is.character, as.factor)
burn = 200; nd = 200
y = sd$G3
x = sd[,1:30]
x = data.frame(x)
p = ncol(x)
bf = wbart(x,y,nskip=burn,ndpost=nd,printevery=50)
lmf = lm(G3~., sd)
lmf = lm(G3~., sd)
plot(bf$sigma,ylim=c(1.5,5),xlab="MCMC iteration",ylab="sigma draw",cex=.5)
View(sd)
load("C:/Users/timtj/GitHub/wissenschaftliches-arbeiten/R/student_por.RData")
# 1. Setting up data ----
sd <- data.frame(student_por)
sd <- sd %>%
mutate_if(is.character, as.factor)
# Removing these because that would just be cheating
sd$G2 <- NULL
sd$G1 <- NULL
# also do basic test if data is working
View(sd)
plot(sd$studytime, sd$G3, col = "blue", pch = 19)
grid()
burn = 200; nd = 200
y = sd$G3
x = sd[,1:30]
x = sd[,1:30]
p = ncol(x)
burn = 5000; nd = 5000
y = sd$G3
x = sd[,1:30]
p = ncol(x)
bf = wbart(x,y,nskip=burn,ndpost=nd,printevery=500)
burn = 1000; nd = 1000
y = sd$G3
x = sd[,1:30]
p = ncol(x)
bf = wbart(x,y,nskip=burn,ndpost=nd,printevery=500)
lmf = lm(G3~., sd)
plot(bf$sigma,ylim=c(1.5,5),xlab="MCMC iteration",ylab="sigma draw",cex=.5)
abline(h=summary(lmf)$sigma,col="red",lty=2) #least squares estimates
abline(v = burn,col="green")
title(main="sigma draws, green line at burn in, red line at least squares estimate",cex.main=.8)
thin = 20
ii = burn + thin*(1:(nd/thin))
acf(bf$sigma[ii],main="ACF of thinned post burn-in sigma draws")
bf20 = wbart(x,y,nskip=burn,ndpost=nd, ntree = 20,printevery=500)
fitmat = cbind(y,bf$yhat.train.mean,bf20$yhat.train.mean)
colnames(fitmat) = c("y","yhatBART","yhatBART20")
pairs(fitmat)
print(cor(fitmat))
dim(bf20$varcount)
dim(bf20$varcount)
#compute row percentages
percount20 = bf20$varcount/apply(bf20$varcount,1,sum)
# mean of row percentages
mvp20 =apply(percount20,2,mean)
#quantiles of row percentags
qm = apply(percount20,2,quantile,probs=c(.05,.95))
print(mvp20)
rgy = range(qm)
plot(c(1,p),rgy,type="n",xlab="variable",ylab="post mean, percent var use",axes=FALSE)
axis(1,at=1:p,labels=names(mvp20),cex.lab=0.7,cex.axis=0.7)
axis(2,cex.lab=1.2,cex.axis=1.2)
lines(1:p,mvp20,col="black",lty=4,pch=4,type="b",lwd=1.5)
for(i in 1:p) {
lines(c(i,i),qm[,i],col="blue",lty=3,lwd=1.0)
}
rgy = range(qm)
plot(c(1,p),rgy,type="n",xlab="variable",ylab="post mean, percent var use",axes=FALSE)
axis(1,at=1:p,labels=names(mvp20),cex.lab=0.7,cex.axis=0.7)
plot(c(1,p),rgy,type="n",xlab="variable",ylab="post mean, percent var use")
axis(1,at=1:p,labels=names(mvp20),cex.lab=0.7,cex.axis=0.7)
axis(2,cex.lab=1.2,cex.axis=1.2)
rgy = range(qm)
plot(c(1,p),rgy,type="n",xlab="variable",ylab="post mean, percent var use")
lines(1:p,mvp20,col="black",lty=4,pch=4,type="b",lwd=1.5)
for(i in 1:p) {
lines(c(i,i),qm[,i],col="blue",lty=3,lwd=1.0)
}
rgy = range(qm)
plot(c(1,p),rgy,type="n",xlab="variable",ylab="post mean, percent var use")
axis(1,at=1:p,labels=names(mvp20),cex.lab=0.7,cex.axis=0.7)
axis(2,cex.lab=1.2,cex.axis=1.2)
lines(1:p,mvp20,col="black",lty=4,pch=4,type="b",lwd=1.5)
for(i in 1:p) {
lines(c(i,i),qm[,i],col="blue",lty=3,lwd=1.0)
}
rgy = range(qm)
plot(c(1,p),rgy,type="n",xlab="variable",ylab="post mean, percent var use",axes=FALSE)
axis(1,at=1:p,labels=names(mvp20),cex.lab=0.7,cex.axis=0.7)
axis(2,cex.lab=1.2,cex.axis=1.2)
lines(1:p,mvp20,col="black",lty=4,pch=4,type="b",lwd=1.5)
for(i in 1:p) {
lines(c(i,i),qm[,i],col="blue",lty=3,lwd=1.0)
}
# Assuming qm is a matrix or data frame
p <- ncol(qm)
rgy <- range(qm, na.rm = TRUE)
# Create the plot
plot(c(1, p), rgy, type = "n", xlab = "variable",
ylab = "post mean, percent var use", axes = FALSE)
# Add x-axis
axis(1, at = 1:p, labels = colnames(qm), cex.lab = 0.7, cex.axis = 0.7)
# Add y-axis
axis(2, cex.lab = 1.2, cex.axis = 1.2)
# Add lines for mvp20 if it exists and has the correct length
if (exists("mvp20") && length(mvp20) == p) {
lines(1:p, mvp20, col = "black", lty = 4, pch = 4, type = "b", lwd = 1.5)
}
# Add vertical lines
for (i in 1:p) {
lines(c(i, i), qm[, i], col = "blue", lty = 3, lwd = 1.0)
}
percount = bf$varcount/apply(bf$varcount,1,sum)
mvp = apply(percount,2,mean)
plot(mvp20,xlab="variable number",ylab="post mean, percent var use",col="blue",type="b")
lines(mvp,type="b",col='red')
legend("topleft",legend=c("BART","BART20"),col=c("red","blue"),lty=c(1,1))
