# Add vertical lines
for (i in 1:p) {
lines(c(i, i), qm[, i], col = "blue", lty = 3, lwd = 1.0)
}
# compute row percentages
percount <- bart_model$varcount / apply(bart_model$varcount, 1, sum)
# mean of row percentages
mvp <- apply(percount, 2, mean)
# quantiles of row percentags
qm <- apply(percount, 2, quantile, probs = c(.001, .999))
print(mvp)
# Assuming qm is a matrix or data frame
p <- ncol(qm)
rgy <- range(qm, na.rm = TRUE)
# Create the plot
plot(c(1, p), rgy,
type = "n", xlab = "variable",
ylab = "post mean, percent var use", axes = FALSE
)
# Add x-axis
axis(1, at = 1:p, labels = colnames(qm), cex.lab = 0.7, cex.axis = 0.7)
# Add y-axis
axis(2, cex.lab = 1.2, cex.axis = 1.2)
# Add lines for mvp20 if it exists and has the correct length
lines(1:p, mvp, col = "black", lty = 4, pch = 4, type = "b", lwd = 1.5)
# Add vertical lines
for (i in 1:p) {
lines(c(i, i), qm[, i], col = "blue", lty = 3, lwd = 1.0)
}
# compute row percentages
percount <- bart_model$varcount / apply(bart_model$varcount, 1, sum)
# mean of row percentages
mvp <- apply(percount, 2, mean)
# quantiles of row percentags
qm <- apply(percount, 2, quantile, probs = c(.1, .9))
print(mvp)
# Assuming qm is a matrix or data frame
p <- ncol(qm)
rgy <- range(qm, na.rm = TRUE)
# Create the plot
plot(c(1, p), rgy,
type = "n", xlab = "variable",
ylab = "post mean, percent var use", axes = FALSE
)
# Add x-axis
axis(1, at = 1:p, labels = colnames(qm), cex.lab = 0.7, cex.axis = 0.7)
# Add y-axis
axis(2, cex.lab = 1.2, cex.axis = 1.2)
# Add lines for mvp20 if it exists and has the correct length
lines(1:p, mvp, col = "black", lty = 4, pch = 4, type = "b", lwd = 1.5)
# Add vertical lines
for (i in 1:p) {
lines(c(i, i), qm[, i], col = "blue", lty = 3, lwd = 1.0)
}
# compute row percentages
percount <- bart_model$varcount / apply(bart_model$varcount, 1, sum)
# mean of row percentages
mvp <- apply(percount, 2, mean)
# quantiles of row percentags
qm <- apply(percount, 2, quantile, probs = c(.00000001, .99999999))
print(mvp)
# Assuming qm is a matrix or data frame
p <- ncol(qm)
rgy <- range(qm, na.rm = TRUE)
# Create the plot
plot(c(1, p), rgy,
type = "n", xlab = "variable",
ylab = "post mean, percent var use", axes = FALSE
)
# Add x-axis
axis(1, at = 1:p, labels = colnames(qm), cex.lab = 0.7, cex.axis = 0.7)
# Add y-axis
axis(2, cex.lab = 1.2, cex.axis = 1.2)
lines(1:p, mvp, col = "black", lty = 4, pch = 4, type = "b", lwd = 1.5)
# Add vertical lines
for (i in 1:p) {
lines(c(i, i), qm[, i], col = "blue", lty = 3, lwd = 1.0)
}
# compute row percentages
percount <- bart_model$varcount / apply(bart_model$varcount, 1, sum)
# mean of row percentages
mvp <- apply(percount, 2, mean)
# quantiles of row percentags
qm <- apply(percount, 2, quantile, probs = c(0.5, 0.5))
print(mvp)
# Assuming qm is a matrix or data frame
p <- ncol(qm)
rgy <- range(qm, na.rm = TRUE)
# Create the plot
plot(c(1, p), rgy,
type = "n", xlab = "variable",
ylab = "post mean, percent var use", axes = FALSE
)
# Add x-axis
axis(1, at = 1:p, labels = colnames(qm), cex.lab = 0.7, cex.axis = 0.7)
# Add y-axis
axis(2, cex.lab = 1.2, cex.axis = 1.2)
lines(1:p, mvp, col = "black", lty = 4, pch = 4, type = "b", lwd = 1.5)
# Add vertical lines
for (i in 1:p) {
lines(c(i, i), qm[, i], col = "blue", lty = 3, lwd = 1.0)
}
# compute row percentages
percount <- bart_model$varcount / apply(bart_model$varcount, 1, sum)
# mean of row percentages
mvp <- apply(percount, 2, mean)
# quantiles of row percentags
qm <- apply(percount, 2, quantile, probs = c(0.1, 0.9))
print(mvp)
# Assuming qm is a matrix or data frame
p <- ncol(qm)
rgy <- range(qm, na.rm = TRUE)
# Create the plot
plot(c(1, p), rgy,
type = "n", xlab = "variable",
ylab = "post mean, percent var use", axes = FALSE
)
# Add x-axis
axis(1, at = 1:p, labels = colnames(qm), cex.lab = 0.7, cex.axis = 0.7)
# Add y-axis
axis(2, cex.lab = 1.2, cex.axis = 1.2)
lines(1:p, mvp, col = "black", lty = 4, pch = 4, type = "b", lwd = 1.5)
# Add vertical lines
for (i in 1:p) {
lines(c(i, i), qm[, i], col = "blue", lty = 3, lwd = 1.0)
}
# compute row percentages
percount <- bart_model$varcount / apply(bart_model$varcount, 1, sum)
# mean of row percentages
mvp <- apply(percount, 2, mean)
# quantiles of row percentags
qm <- apply(percount, 2, quantile, probs = c(0.3, 0.7))
print(mvp)
# Assuming qm is a matrix or data frame
p <- ncol(qm)
rgy <- range(qm, na.rm = TRUE)
# Create the plot
plot(c(1, p), rgy,
type = "n", xlab = "variable",
ylab = "post mean, percent var use", axes = FALSE
)
# Add x-axis
axis(1, at = 1:p, labels = colnames(qm), cex.lab = 0.7, cex.axis = 0.7)
# Add y-axis
axis(2, cex.lab = 1.2, cex.axis = 1.2)
lines(1:p, mvp, col = "black", lty = 4, pch = 4, type = "b", lwd = 1.5)
# Add vertical lines
for (i in 1:p) {
lines(c(i, i), qm[, i], col = "blue", lty = 3, lwd = 1.0)
}
percount <- bart_model$varcount / apply(bf$varcount, 1, sum)
percount <- bart_model$varcount / apply(bart_model$varcount, 1, sum)
mvp <- apply(percount, 2, mean)
plot(mvp20, xlab = "variable number", ylab = "post mean, percent var use", col = "blue", type = "b")
plot(mvp20, xlab = "variable number", ylab = "post mean, percent var use", col = "blue", type = "b")
lines(mvp, type = "b", col = "red")
legend("topleft", legend = c("BART", "BART20"), col = c("red", "blue"), lty = c(1, 1))
percount <- bart_model$varcount / apply(bart_model$varcount, 1, sum)
mvp <- apply(percount, 2, mean)
plot(mvp20, xlab = "variable number", ylab = "post mean, percent var use", col = "blue", type = "b")
lines(mvp, type = "b", col = "red")
legend("topleft", legend = c("BART", "BART20"), col = c("red", "blue"), lty = c(1, 1))
library(tidyverse)
library(rpart)
library(rpart.plot)
library(data.tree)
library(networkD3)
library(plotly)
library(caret)
library(BART)
library(lintr)
library(styler)
getwd()
setwd("C:/Users/timtj/GitHub/wissenschaftliches-arbeiten/R")
# Function to calculate MSE
calculate_mse <- function(actual, predicted) {
mean((actual - predicted)^2)
}
# Function to calculate MSE
calculate_mse <- function(actual, predicted) {
mean((actual - predicted)^2)
}
load("C:/Users/timtj/GitHub/wissenschaftliches-arbeiten/R/student_por.RData")
sd <- data.frame(student_por)
sd <- sd %>%
mutate_if(is.character, as.factor)
sd$sumalc <- sd$Walc + sd$Dalc
sd$Dalc <- NULL
sd$Walc <- NULL
# Removing these because that would just be cheating
sd$G2 <- NULL
sd$G1 <- NULL
# also do basic test if data is working
plot(sd$studytime, sd$G3, col = "blue", pch = 19)
grid()
# Split the data into training (70%) and validation (30%) sets
split_index <- createDataPartition(sd$G3, p = 0.7, list = FALSE)
train_data <- sd[split_index, ]
valid_data <- sd[-split_index, ]
tree_model <- rpart(G3 ~ ., data = train_data, control = rpart.control(cp = 0.025, minsplit = 5))
rpart.plot(tree_model, fallen.leaves = TRUE)
# Calculate MSE for training set
train_pred <- predict(tree_model, train_data)
train_mse <- mean((train_data$G3 - train_pred)^2)
# Calculate MSE for validation set
valid_pred <- predict(tree_model, valid_data)
valid_mse <- mean((valid_data$G3 - valid_pred)^2)
# Print results
cat("Training MSE:", train_mse, "\n")
cat("Validation MSE:", valid_mse)
m = tree_model$cptable
m
pruned_tree <- prune(tree_model, cp = 0.8)
rpart.plot(pruned_tree)
# for test tree
# Calculate MSE for training set
train_predPP <- predict(pruned_tree, train_data)
train_msePP <- mean((train_data$G3 - train_predPP)^2)
# Calculate MSE for validation set
valid_predPP <- predict(pruned_tree, valid_data)
valid_msePP <- mean((valid_data$G3 - valid_predPP)^2)
# Print results
cat("Training MSE:", train_msePP, "\n")
cat("Validation MSE:", valid_msePP)
11.33771 / 10.03354
tree_model <- rpart(G3 ~ ., data = sd)
rpart.plot(tree_model, shadow.col = "gray")
# Get variable importance
var_importance <- tree_model$variable.importance
# Variable Importance Plot
var_importance <- data.frame(
variable = names(tree_model$variable.importance),
importance = tree_model$variable.importance
)
ggplot(
var_importance,
aes(x = reorder(variable,importance),
y = importance)) +
geom_bar(stat = "identity", fill = "skyblue") +
coord_flip() +
theme_minimal() +
labs(title = "Variable Importance",
x = "Variables",
y = "Importance")
lm_model <- lm(G3 ~ ., data = sd)
# Summary
summary_lm <- summary(lm_model)
print(summary_lm)
sd$lm_pred <- predict(lm_model)
sd$tree_pred <- predict(tree_model)
mse_lm <- mean((sd$G3 - sd$lm_pred)^2)
mse_tree <- mean((sd$G3 - sd$tree_pred)^2)
print(mse_lm)
print(mse_tree)
ggplot(sd, aes(x = G3)) +
geom_point(aes(y = lm_pred, color = "Linear Regression"), alpha = 0.5) +
geom_point(aes(y = tree_pred, color = "Regression Tree"), alpha = 0.5) +
geom_abline(intercept = 0, slope = 1, linetype = "dashed") +
theme_minimal() +
labs(
title = "Predicted vs Actual G3 Scores",
x = "Actual G3", y = "Predicted G3",
color = "Model"
) +
scale_color_manual(values = c("Linear Regression" = "blue", "Regression Tree" = "red"))
sd_mini <- sd[, c("sumalc", "absences", "G3")]
tree_model <- rpart(G3 ~ sumalc + absences, data = sd_mini, control = rpart.control(cp = 0.002, minsplit = 40))
rpart.plot(tree_model)
sumalc_seq <- seq(min(sd_mini$sumalc), max(sd_mini$sumalc), length.out = 13)
absences_seq <- seq(min(sd_mini$absences), max(sd_mini$absences), length.out = 13)
grid <- expand.grid(sumalc = sumalc_seq, absences = absences_seq)
grid$G3_pred <- predict(tree_model, newdata = grid)
ggplot(grid, aes(x = sumalc, y = absences, fill = G3_pred)) +
geom_tile() +
geom_text(aes(label = round(G3_pred, 1)), size = 3) +
scale_fill_gradient(low = "darkred", high = "lightblue", guide = "none") +
labs(x = "Alcohol consumption", y = "Absences", title = "Predicting Exam score based on alcohol consumption and Absences") +
theme_minimal() +
geom_point(data = sd_mini, aes(x = sumalc, y = absences), color = "blue", size = 2, inherit.aes = FALSE) # Add blue points for actual data
# Create a complex tree
complex_tree <- rpart(G3 ~ ., data = train_data, control = rpart.control(cp = 0.002, minsplit = 5))
rpart.plot(complex_tree, main = "Initial Complex Regression Tree")
plotcp(complex_tree)
# Create a complex tree
complex_tree <- rpart(G3 ~ ., data = train_data, control = rpart.control(cp = 0.002, minsplit = 5))
rpart.plot(complex_tree, main = "Initial Complex Regression Tree")
plotcp(complex_tree)
# Find optimal CP value
opt_cp   <- complex_tree$cptable[which.min(complex_tree$cptable[, "xerror"]), "CP"]
opt_sp_1 <- complex_tree$cptable[which.min(complex_tree$cptable[, "xerror"]), "nsplit"]
print(opt_sp_1) # for xerror using cross validation
print(opt_cp) # for xerror
print(min(complex_tree$cptable[, "xerror"]))
print(min(complex_tree$cptable[, "rel error"]))
# Prune the tree
pruned_tree <- prune(complex_tree, cp = opt_cp)
rpart.plot(pruned_tree, main = "Optimal Pruned Tree")
# Calculate the MSEs
MSE_complex <- function() {
train_pred_complex <- predict(complex_tree, train_data)
test_pred_complex <- predict(complex_tree, valid_data)
train_mse_complex <- calculate_mse(train_data$G3, train_pred_complex)
test_mse_complex <- calculate_mse(valid_data$G3, test_pred_complex)
print(paste("Complex Tree - Training MSE:", train_mse_complex))
print(paste("Complex Tree -     Test MSE:", test_mse_complex))
}
MSE_pruned <- function() {
train_pred_pruned <- predict(pruned_tree, train_data)
test_pred_pruned <- predict(pruned_tree, valid_data)
train_mse_pruned <- calculate_mse(train_data$G3, train_pred_pruned)
test_mse_pruned <- calculate_mse(valid_data$G3, test_pred_pruned)
print(paste("Pruned Tree  - Training MSE:", train_mse_pruned))
print(paste("Pruned Tree  -     Test MSE:", test_mse_pruned))
}
MSE_complex()
MSE_pruned()
cp_table <- complex_tree$cptable
0.8469317 * 10.03636
0.1993765 * 10.03636
#[1] "Complex Tree - Training MSE: 2.00101337636504"
#[1] "Complex Tree -     Test MSE: 11.7359314930842"
#[1] "Pruned Tree  - Training MSE: 8.10972820563119"
#[1] "Pruned Tree  -     Test MSE: 8.05554703417732"
?sapply
cp_table[, "CP"]
cp_table <- complex_tree$cptable
first_mse <- sapply(0.140409142, function(cp) {
pruned <- prune(complex_tree, cp = cp)
train_pred <- predict(pruned, train_data)
test_pred <- predict(pruned, valid_data)
train_mse <- calculate_mse(train_data$G3, train_pred)
test_mse <- calculate_mse(valid_data$G3, test_pred)
c(train_mse, test_mse)
})
first_mse
first_mse <- sapply(cp_table[, "CP"], function(cp) {
pruned <- prune(complex_tree, cp = cp)
train_pred <- predict(pruned, train_data)
test_pred <- predict(pruned, valid_data)
train_mse <- calculate_mse(train_data$G3, train_pred)
test_mse <- calculate_mse(valid_data$G3, test_pred)
c(train_mse, test_mse)
})
num_splits <- cp_table[, "nsplit"]
plot(
cp_table[, "nsplit"],
cp_table[, "xerror"]
)
# Calculate MSE for each CP value
mse_values <- sapply(cp_table[, "CP"], function(cp) {
pruned <- prune(complex_tree, cp = cp)
train_pred <- predict(pruned, train_data)
test_pred <- predict(pruned, valid_data)
train_mse <- calculate_mse(train_data$G3, train_pred)
test_mse <- calculate_mse(valid_data$G3, test_pred)
c(train_mse, test_mse)
})
# Making data for ggplot
mse_data <- data.frame(
num_splits = rep(num_splits, 2),
mse = c(mse_values[1, ], mse_values[2, ]),
type = rep(c("Training MSE", "Test MSE"), each = length(num_splits))
)
temp_mse_data <- mse_data %>% filter(type == "Test MSE")
opt_splits <- temp_mse_data$num_splits[which.min(temp_mse_data$mse)]
min_mse <- min(temp_mse_data$mse)
print(opt_splits)
print(min_mse) # Should both be the same as in previous cells
ggplot(mse_data, aes(x = num_splits, y = mse, color = type)) +
geom_line() +
geom_point() +
scale_color_manual(values = c("blue", "red")) +
labs(
x = "Number of Splits", y = "Mean Squared Error",
title = "MSE vs Tree Complexity",
color = "MSE Type"
) +
theme_minimal() +
geom_vline(xintercept = opt_splits, linetype = "dashed", color = "purple") +
annotate("text",
x = opt_splits + 18, y = min_mse -1 , label = paste("Optimal number of splits =", opt_splits),
vjust = -1, color = "purple"
)
adj = mse_data[1,2]
len = length(cp_table[,"nsplit"])
data_conc = data.frame(
rep(cp_table[, "nsplit"],4),
c(cp_table[, "xerror"]*adj,
cp_table[, "rel error"]*adj,
c(mse_values[1,],mse_values[2,])
),
c(rep("xe_er",len),
rep("re_er",len),
rep("tr_er",len),
rep("te_er",len)
)
)
ggplot(data_conc, aes(x = data_conc[,1], y = data_conc[,2], color = data_conc[,3])) +
geom_line() +
geom_point() +
scale_color_manual(values = c("orange", "red","blue","green")) +
labs(
x = "Number of Splits", y = "Mean Squared Error",
title = "MSE vs Tree Complexity",
color = "MSE Type"
) +
theme_minimal()
tree_model <- rpart(G3 ~ ., data = train_data, control = rpart.control(cp = opt_cp-0.0001 , minsplit = 5))
rpart.plot(tree_model, fallen.leaves = TRUE)
# Create REGR
lm_model <- lm(G3 ~ ., data = train_data)
summary(lm_model)
# Prepare data for BART
x_train <- train_data[, !names(train_data) %in% "G3"]
y_train <- train_data$G3
x_valid <- valid_data[, !names(valid_data) %in% "G3"]
y_valid <- valid_data$G3
# Fit BART model
bart_model <- wbart(
x.train = x_train,
y.train = y_train,
x.test = x_valid,
nskip = 1000,
ntree = 200,
ndpost = 1000,
printevery = 500)
# Print results
comp <- function() {
cat("Training MSE   Tree:", train_mse_tree, "\n")
cat("Validation MSE Tree:", valid_mse_tree, "\n\n")
cat("Training MSE   Regr:", train_mse_regr, "\n")
cat("Validation MSE Regr:", valid_mse_regr, "\n\n")
cat("Training MSE   BART:", train_mse_bart, "\n")
cat("Validation MSE BART:", valid_mse_bart, "\n")
}
# tree error
# Calculate MSE for training set
train_pred_tree <- predict(tree_model, train_data)
train_mse_tree <- mean((train_data$G3 - train_pred_tree)^2)
# Calculate MSE for validation set
valid_pred_tree <- predict(tree_model, valid_data)
valid_mse_tree <- mean((valid_data$G3 - valid_pred_tree)^2)
# regr error
# Calculate MSE for training set
train_pred_regr <- predict(lm_model, train_data)
train_mse_regr <- mean((train_data$G3 - train_pred_regr)^2)
# Calculate MSE for validation set
valid_pred_regr <- predict(lm_model, valid_data)
valid_mse_regr <- mean((valid_data$G3 - valid_pred_regr)^2)
# BART error
# Calculate MSE for training set
train_pred_bart <- bart_model$yhat.train.mean
train_mse_bart <- mean((y_train - train_pred_bart)^2)
# Calculate MSE for validation set
valid_pred_bart <- bart_model$yhat.test.mean
valid_mse_bart <- mean((y_valid - valid_pred_bart)^2)
comp()
# Fit BART model
bart_model <- wbart(
x.train = x_train,
y.train = y_train,
x.test = x_valid,
nskip = 1000,
ntree = 200,
ndpost = 1000,
printevery = 500)
bart_minim <- wbart(
x.train = x_train,
y.train = y_train,
x.test = x_valid,
nskip = 100,
ntree = 100,
ndpost = 50,
printevery = 500)
fitmat <- cbind(train_data$G3, bart_model$yhat.train.mean, bart_minim$yhat.train.mean)
pairs(fitmat)
cor(fitmat)
bart_minim <- wbart(
x.train = x_train,
y.train = y_train,
x.test = x_valid,
nskip = 100,
ntree = 100,
ndpost = 50,
printevery = 500)
fitmat <- cbind(train_data$G3, bart_model$yhat.train.mean, bart_minim$yhat.train.mean)
pairs(fitmat)
cor(fitmat)
# compute row percentages
percount <- bart_model$varcount / apply(bart_model$varcount, 1, sum)
# mean of row percentages
mvp <- apply(percount, 2, mean)
# quantiles of row percentags
qm <- apply(percount, 2, quantile, probs = c(0.3, 0.7))
print(mvp)
# Assuming qm is a matrix or data frame
p <- ncol(qm)
rgy <- range(qm, na.rm = TRUE)
# Create the plot
plot(c(1, p), rgy,
type = "n", xlab = "variable",
ylab = "post mean, percent var use", axes = FALSE
)
# Add x-axis
axis(1, at = 1:p, labels = colnames(qm), cex.lab = 0.7, cex.axis = 0.7)
# Add y-axis
axis(2, cex.lab = 1.2, cex.axis = 1.2)
lines(1:p, mvp, col = "black", lty = 4, pch = 4, type = "b", lwd = 1.5)
# Add vertical lines
for (i in 1:p) {
lines(c(i, i), qm[, i], col = "blue", lty = 3, lwd = 1.0)
}
