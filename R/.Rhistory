cat("Training MSE   Tree:", train_mse, "\n")
cat("Validation MSE Tree:", valid_mse, "\n\n")
cat("Training MSE   Regr:", train_mseL, "\n")
cat("Validation MSE Regr:", valid_mseL, "\n\n")
}
comp()
# Split the data into training (70%) and validation (30%) sets
split_index <- createDataPartition(sd$G3, p = 0.7, list = FALSE)
train_data <- sd[split_index, ]
valid_data <- sd[-split_index, ]
burn = 1000; nd = 1000
y = sd$G3
y = train_data$G3
View(train_data)
print([1:10])
print(1:10)
print(1:10,16)
fix = c(1:28)
fix
fix = c(1:28, 30)
fix
burn = 1000; nd = 1000
fix = c(1:28, 30)
y = train_data$G3
x = train_data[,fix]
p = ncol(x)
bf = wbart(x,y,nskip=burn,ndpost=nd,printevery=500)
lmf = lm(G3~., sd)
lmf = lm(G3~., train_data)
plot(bf$sigma,ylim=c(1.5,5),xlab="MCMC iteration",ylab="sigma draw",cex=.5)
abline(h=summary(lmf)$sigma,col="red",lty=2) #least squares estimates
abline(v = burn,col="green")
title(main="sigma draws, green line at burn in, red line at least squares estimate",cex.main=.8)
thin = 20
ii = burn + thin*(1:(nd/thin))
acf(bf$sigma[ii],main="ACF of thinned post burn-in sigma draws")
# making small BART model to compare
bf20 = wbart(x,y,nskip=burn,ndpost=nd, ntree = 20,printevery=500)
fitmat = cbind(y,bf$yhat.train.mean,bf20$yhat.train.mean)
colnames(fitmat) = c("y","yhatBART","yhatBART20")
pairs(fitmat)
print(cor(fitmat))
dim(bf20$varcount)
# Split the data into training (70%) and validation (30%) sets
split_index <- createDataPartition(sd$G3, p = 0.7, list = FALSE)
train_data <- sd[split_index, ]
valid_data <- sd[-split_index, ]
burn = 1000; nd = 1000
fix = c(1:28, 30)
y = train_data$G3
x = train_data[,fix]
p = ncol(x)
bf = wbart(x,y,nskip=burn,ndpost=nd,printevery=500)
# Calculate MSE for training set
train_pred <- predict(bf, train_data)
View(bf)
# Data-analysis
library(rpart)
library(rpart.plot)
library(dplyr)
library(ggplot2)
library(data.tree)
library(networkD3)
library(plotly)
library(caret)
library(BART)
# 1. Setting up data ----
sd <- data.frame(student_por)
sd <- sd %>%
mutate_if(is.character, as.factor)
sd$sumalc <- sd$Walc + sd$Dalc
sd$Dalc <- NULL
sd$Walc <- NULL
# Removing these because that would just be cheating
sd$G2 <- NULL
sd$G1 <- NULL
sd = clean
tree_model <- rpart(G3 ~ ., data = sd)
rpart.plot(tree_model, shadow.col = "gray")
tree_model <- rpart(G3 ~ ., data = sd)
rpart.plot(tree_model, shadow.col = "gray")
rpart.plot(tree_model, extra = 101, fallen.leaves = TRUE, type = 4, main = "Regression Tree")
rpart.plot(tree_model, shadow.col = "gray")
load("C:/Users/timtj/GitHub/wissenschaftliches-arbeiten/R/student_por.RData")
# Data-analysis
library(rpart)
library(rpart.plot)
library(dplyr)
library(ggplot2)
library(data.tree)
library(networkD3)
library(plotly)
library(caret)
library(BART)
# 1. Setting up data ----
sd <- data.frame(student_por)
sd <- sd %>%
mutate_if(is.character, as.factor)
sd$sumalc <- sd$Walc + sd$Dalc
sd$Dalc <- NULL
sd$Walc <- NULL
# Removing these because that would just be cheating
sd$G2 <- NULL
sd$G1 <- NULL
clean = sd
plot(sd$studytime, sd$G3, col = "blue", pch = 19)
grid()
# Load required libraries
library(caret)
library(BART)
library(glmnet)
install.packages("glmnet")
# Load required libraries
library(caret)
library(BART)
library(glmnet)
split_index <- createDataPartition(sd$G3, p = 0.7, list = FALSE)
train_data <- sd[split_index, ]
test_data <- sd[-split_index, ]
# Prepare the data
X <- as.matrix(sd[, -which(names(sd) == "G3")])  # predictors
y <- sd$G3  # target variable
# Multivariate Linear Regression with Regularization (Elastic Net)
# We'll use cross-validation to find the best alpha and lambda values
cv_model <- cv.glmnet(X[split_index,], y[split_index], alpha = 0.5, nfolds = 5)
best_lambda <- cv_model$lambda.min
# Fit the final model
lm_model <- glmnet(X[split_index,], y[split_index], alpha = 0.5, lambda = best_lambda)
summary(lm_model)
# Prepare the data
X <- as.matrix(sd[, -which(names(sd) == "G3")])  # predictors
y <- sd$G3  # target variable
# Multivariate Linear Regression with Regularization (Elastic Net)
# We'll use cross-validation to find the best alpha and lambda values
cv_model <- cv.glmnet(X[split_index,], y[split_index], alpha = 0.5, nfolds = 5)
best_lambda <- cv_model$lambda.min
# Fit the final model
lm_model <- glmnet(X[split_index,], y[split_index], alpha = 0.5, lambda = best_lambda)
# BART model
burn <- 1000
nd <- 1000
bart_model <- wbart(X[split_index,], y[split_index], nskip = burn, ndpost = nd, printevery = 500)
burn = 1000; nd = 1000
fix = c(1:28, 30)
y = train_data$G3
x = train_data[,fix]
p = ncol(x)
bf = wbart(x,y,nskip=burn,ndpost=nd,printevery=500)
# Make predictions on the test set
lm_pred <- predict(lm_model, newx = X[-split_index,], s = best_lambda)
bart_pred <- predict(bart_model, newdata = X[-split_index,])
# Calculate MSE for both models on the test set
lm_mse <- mean((y[-split_index] - lm_pred)^2)
bart_mse <- mean((y[-split_index] - bart_pred)^2)
# Calculate R-squared for both models on the test set
lm_rsq <- 1 - sum((y[-split_index] - lm_pred)^2) / sum((y[-split_index] - mean(y[-split_index]))^2)
bart_rsq <- 1 - sum((y[-split_index] - bart_pred)^2) / sum((y[-split_index] - mean(y[-split_index]))^2)
# Print results
cat("Linear Regression (Elastic Net) Results:\n")
cat("MSE:", lm_mse, "\n")
cat("R-squared:", lm_rsq, "\n\n")
cat("BART Results:\n")
cat("MSE:", bart_mse, "\n")
cat("R-squared:", bart_rsq, "\n")
# Compare variable importance
lm_importance <- abs(coef(lm_model))[-1]  # Exclude intercept
bart_importance <- bartModelMatrix(bart_model)$varcount.mean
# TTTTTNNNN ----
# Load required libraries
library(caret)
library(rpart)
library(rpart.plot)
library(BART)
# Assuming 'sd' is your dataset
# Split the data into training (70%) and validation (30%) sets
set.seed(123)  # for reproducibility
split_index <- createDataPartition(sd$G3, p = 0.7, list = FALSE)
train_data <- sd[split_index, ]
valid_data <- sd[-split_index, ]
# BART model
# Prepare data for BART
x_train <- train_data[, !names(train_data) %in% "G3"]
y_train <- train_data$G3
x_valid <- valid_data[, !names(valid_data) %in% "G3"]
y_valid <- valid_data$G3
# Fit BART model
bart_model <- wbart(x.train = x_train, y.train = y_train, x.test = x_valid, ntree = 200, nskip = 100, ndpost = 1000)
# Calculate MSE for BART
train_pred_bart <- bart_model$yhat.train.mean
train_mse_bart <- mean((y_train - train_pred_bart)^2)
valid_pred_bart <- bart_model$yhat.test.mean
valid_mse_bart <- mean((y_valid - valid_pred_bart)^2)
# Comparison function
comp <- function() {
cat("Training MSE Tree:", train_mse_tree, "\n")
cat("Validation MSE Tree:", valid_mse_tree, "\n\n")
cat("Training MSE Regr:", train_mse_lm, "\n")
cat("Validation MSE Regr:", valid_mse_lm, "\n\n")
cat("Training MSE BART:", train_mse_bart, "\n")
cat("Validation MSE BART:", valid_mse_bart, "\n")
}
# Print results
comp()
# Plot BART results
plot(bart_model)
# Split the data into training (70%) and validation (30%) sets
split_index <- createDataPartition(sd$G3, p = 0.7, list = FALSE)
train_data <- sd[split_index, ]
valid_data <- sd[-split_index, ]
tree_model <- rpart(G3 ~ ., data = train_data, control = rpart.control(cp = 0.03, minsplit = 40))
rpart.plot(tree_model, fallen.leaves = TRUE)
# Calculate MSE for training set
train_pred <- predict(tree_model, train_data)
train_mse <- mean((train_data$G3 - train_pred)^2)
# Calculate MSE for validation set
valid_pred <- predict(tree_model, valid_data)
valid_mse <- mean((valid_data$G3 - valid_pred)^2)
lm_model <- lm(G3 ~ ., data = train_data)
# Summary
summary_lm <- summary(lm_model)
# Summary
summary_lm <- summary(lm_model)
# Summary
summary(lm_model)
# Calculate MSE for training set
train_predL <- predict(lm_model, train_data)
train_mseL <- mean((train_data$G3 - train_predL)^2)
# Calculate MSE for validation set
valid_predL <- predict(lm_model, valid_data)
valid_mseL <- mean((valid_data$G3 - valid_predL)^2)
# Print results
comp <- function(){
cat("Training MSE   Tree:", train_mse, "\n")
cat("Validation MSE Tree:", valid_mse, "\n\n")
cat("Training MSE   Regr:", train_mseL, "\n")
cat("Validation MSE Regr:", valid_mseL, "\n\n")
}
comp()
x = train_data[,fix]
View(x)
# BART model
# Prepare data for BART
x_train <- train_data[, !names(train_data) %in% "G3"]
y_train <- train_data$G3
# BART model
# Prepare data for BART
x_train <- train_data[, !names(train_data) %in% "G3"]
y_train <- train_data$G3
x_valid <- valid_data[, !names(valid_data) %in% "G3"]
y_valid <- valid_data$G3
bart_model <- wbart(x.train = x_train, y.train = y_train, x.test = x_valid, nskip = 1000, ndpost = 1000, printevery = 500)
# Print results
comp <- function(){
cat("Training MSE   Tree:", train_mse, "\n")
cat("Validation MSE Tree:", valid_mse, "\n\n")
cat("Training MSE   Regr:", train_mseL, "\n")
cat("Validation MSE Regr:", valid_mseL, "\n\n")
cat("Training MSE   BART:", train_mse_bart, "\n")
cat("Validation MSE BART:", valid_mse_bart, "\n")
}
comp()
# Split the data into training (70%) and validation (30%) sets
split_index <- createDataPartition(sd$G3, p = 0.7, list = FALSE)
train_data <- sd[split_index, ]
valid_data <- sd[-split_index, ]
tree_model <- rpart(G3 ~ ., data = train_data, control = rpart.control(cp = 0.03, minsplit = 40))
rpart.plot(tree_model, fallen.leaves = TRUE)
# Create TREE
tree_model <- rpart(G3 ~ ., data = train_data, control = rpart.control(cp = 0.03, minsplit = 40))
rpart.plot(tree_model, fallen.leaves = TRUE)
# Create REGR
lm_model <- lm(G3 ~ ., data = train_data)
summary(lm_model)
install.packages("lintr")
lint("<looking_at_data>.R>")
lint("<looking_at_data>.R>")
library(data.tree)
library(lintr)
lint("<looking_at_data>.R>")
setwd()
setwd(timtj)
setwd(Users)
setwd(C:\Users\timtj\GitHub\wissenschaftliches-arbeiten\R)
setwd("C:\Users\timtj\GitHub\wissenschaftliches-arbeiten\R")
setwd("C:/Users/timtj/GitHub/wissenschaftliches-arbeiten/R")
lint("<looking_at_data>.R>")
lint("<looking_at_data>.R")
getwd()
lint("<looking_at_data>.R")
lint("looking_at_data.R")
lint
getwd()
setwd("C:/Users/timtj/GitHub/wissenschaftliches-arbeiten/R")
lint("looking_at_data.R")
# Data-analysis
library(rpart)
library(rpart.plot)
library(dplyr)
library(ggplot2)
library(data.tree)
library(networkD3)
library(plotly)
library(caret)
library(BART)
library(lintr)
getwd()
setwd("C:/Users/timtj/GitHub/wissenschaftliches-arbeiten/R")
lint("looking_at_data.R")
install.packages(styler)
library(styler)
library(styler)
install.packages("styler")
library(styler)
getwd()
setwd("C:/Users/timtj/GitHub/wissenschaftliches-arbeiten/R")
input <- readLines("looking_at_data.R")
writeLines(style_text(input), con = "looking_at_data.R")
library(styler)
getwd()
setwd("C:/Users/timtj/GitHub/wissenschaftliches-arbeiten/R")
lint("looking_at_data.R")
library(lintr)
library(styler)
getwd()
setwd("C:/Users/timtj/GitHub/wissenschaftliches-arbeiten/R")
lint("looking_at_data.R")
getwd()
getwd()
setwd("C:/Users/timtj/GitHub/wissenschaftliches-arbeiten/R")
lint("looking_at_data.R")
input <- readLines("looking_at_data.R")
writeLines(style_text(input), con = "looking_at_data.R")
getwd()
setwd("C:/Users/timtj/GitHub/wissenschaftliches-arbeiten/R")
lint("looking_at_data.R")
input <- readLines("looking_at_data.R")
input <- readLines("looking_at_data.R")
input <- readLines("looking_at_data.R")
writeLines(style_text(input), con = "looking_at_data.R")
# Data-analysis
library(rpart)
library(rpart.plot)
library(dplyr)
library(ggplot2)
library(data.tree)
library(networkD3)
library(plotly)
library(caret)
library(BART)
library(lintr)
library(styler)
install.packages("...")
getwd()
setwd("C:/Users/timtj/GitHub/wissenschaftliches-arbeiten/R")
input <- readLines("looking_at_data.R")
writeLines(style_text(input), con = "looking_at_data.R")
getwd()
setwd("C:/Users/timtj/GitHub/wissenschaftliches-arbeiten/R")
lint("looking_at_data.R")
input <- readLines("looking_at_data.R")
writeLines(style_text(input), con = "looking_at_data.R")
input <- readLines("looking_at_data.R")
writeLines(style_text(input), con = "looking_at_data.R")
# Linting
lint("looking_at_data.R")
# Install from CRAN
install.packages("tidyverse")
library(tidyverse)
1 + 1
#| echo: false
2 * 2
1 + 1
1 + 1
1 + 1
#| echo: false
2 * 2
#| echo: true
2 * 2
#| echo: true
2 * 2
#| echo: true
2 * 2
#| echo: true
2 * 2
# Data-analysis
library(rpart)
library(rpart.plot)
library(data.tree)
library(networkD3)
library(plotly)
library(caret)
library(BART)
library(lintr)
library(styler)
library(tidyverse)
#install.packages("...")
39-2
# Create a complex tree
complex_tree <- rpart(G3 ~ ., data = train_data, control = rpart.control(cp = 0.0025, minsplit = 5))
# Find optimal CP value
opt_cp <- complex_tree$cptable[which.min(complex_tree$cptable[, "xerror"]), "CP"]
print(opt_cp)
# Calculate the MSEs
MSE_complex <- function() {
train_pred_complex <- predict(complex_tree, train_data)
test_pred_complex <- predict(complex_tree, test_data)
train_mse_complex <- calculate_mse(train_data$G3, train_pred_complex)
test_mse_complex <- calculate_mse(test_data$G3, test_pred_complex)
print(paste("Complex Tree - Training MSE:", train_mse_complex))
print(paste("Complex Tree -     Test MSE:", test_mse_complex))
}
MSE_pruned <- function() {
train_pred_pruned <- predict(pruned_tree, train_data)
test_pred_pruned <- predict(pruned_tree, test_data)
train_mse_pruned <- calculate_mse(train_data$G3, train_pred_pruned)
test_mse_pruned <- calculate_mse(test_data$G3, test_pred_pruned)
print(paste("Pruned Tree - Training MSE:", train_mse_pruned))
print(paste("Pruned Tree - Test MSE:", test_mse_pruned))
}
MSE_complex()
# For tree
# Calculate MSE for training set
train_pred <- predict(tree_model, train_data)
train_mse <- mean((train_data$G3 - train_pred)^2)
# Calculate MSE for validation set
valid_pred <- predict(tree_model, valid_data)
valid_mse <- mean((valid_data$G3 - valid_pred)^2)
# For regr
# Calculate MSE for training set
train_predL <- predict(lm_model, train_data)
train_mseL <- mean((train_data$G3 - train_predL)^2)
# Calculate MSE for validation set
valid_predL <- predict(lm_model, valid_data)
valid_mseL <- mean((valid_data$G3 - valid_predL)^2)
# Print results
comp <- function() {
cat("Training MSE   Tree:", train_mse, "\n")
cat("Validation MSE Tree:", valid_mse, "\n\n")
cat("Training MSE   Regr:", train_mseL, "\n")
cat("Validation MSE Regr:", valid_mseL, "\n\n")
}
comp()
88*87
88*87
library(rpart)
library(rpart.plot)
library(data.tree)
library(networkD3)
library(plotly)
library(caret)
library(BART)
library(lintr)
library(styler)
library(tidyverse)
#install.packages("...")
library(styler)
library(tidyverse)
library(rpart)
library(rpart.plot)
library(data.tree)
library(networkD3)
library(plotly)
library(caret)
library(BART)
library(lintr)
library(styler)
getwd()
setwd("C:/Users/timtj/GitHub/wissenschaftliches-arbeiten/R")
lint("looking_at_data.R")
lint("looking_at_data.qmd")
input <- readLines("looking_at_data.qmd")
writeLines(style_text(input), con = "looking_at_data.qmd")
getwd()
setwd("C:/Users/timtj/GitHub/wissenschaftliches-arbeiten/R")
lint("looking_at_data.qmd")
# not sure why not working anymore but makes sense
# input <- readLines("looking_at_data.qmd")
# writeLines(style_text(input), con = "looking_at_data.qmd")
getwd()
setwd("C:/Users/timtj/GitHub/wissenschaftliches-arbeiten/R")
lint("looking_at_data.qmd")
# not sure why not working anymore but makes sense
# input <- readLines("looking_at_data.qmd")
# writeLines(style_text(input), con = "looking_at_data.qmd")
load("C:/Users/timtj/GitHub/wissenschaftliches-arbeiten/R/student_por.RData")
sd <- data.frame(student_por)
sd <- sd %>%
mutate_if(is.character, as.factor)
sd$sumalc <- sd$Walc + sd$Dalc
sd$Dalc <- NULL
sd$Walc <- NULL
# Removing these because that would just be cheating
sd$G2 <- NULL
sd$G1 <- NULL
# also do basic test if data is working
plot(sd$studytime, sd$G3, col = "blue", pch = 19)
grid()
# Split the data into training (70%) and validation (30%) sets
split_index <- createDataPartition(sd$G3, p = 0.7, list = FALSE)
train_data <- sd[split_index, ]
valid_data <- sd[-split_index, ]
View(split_index)
library(tidyverse)
library(rpart)
library(rpart.plot)
library(data.tree)
library(networkD3)
library(plotly)
library(caret)
library(BART)
library(lintr)
library(styler)
