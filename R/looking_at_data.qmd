---
title: "looking_at_data"
author: "timothy"
format: html
editor: visual
---

Preliminaries

Packages

```{r}
library(tidyverse)
library(rpart)
library(rpart.plot)
library(data.tree)
library(networkD3)
library(plotly)
library(caret)
library(BART)
library(lintr)
library(styler)
```

Function to calculate MSE (That I should really use more)

$x-y^{44}$

Can I view this equation?

```{r}
# Function to calculate MSE
calculate_mse <- function(actual, predicted) {
    mean((actual - predicted)^2)
}
```

# 1. Setting up data

```{r}
load("C:/Users/timtj/GitHub/wissenschaftliches-arbeiten/R/student_por.RData")
sd <- data.frame(student_por)

sd <- sd %>%
    mutate_if(is.character, as.factor)

sd$sumalc <- sd$Walc + sd$Dalc
sd$Dalc <- NULL
sd$Walc <- NULL

# Removing these because that would just be cheating
sd$G2 <- NULL
sd$G1 <- NULL

# also do basic test if data is working
plot(sd$studytime, sd$G3, col = "blue", pch = 19)
grid()
```

Split data into training and test set

```{r}
# Split the data into training (70%) and validation (30%) sets
split_index <- createDataPartition(sd$G3, p = 0.7, list = FALSE)
train_data <- sd[split_index, ]
valid_data <- sd[-split_index, ]

```



# 2. Single tree on data sets

```{r}
tree_model <- rpart(G3 ~ ., data = train_data, control = rpart.control(cp = 0.025, minsplit = 5))
rpart.plot(tree_model, fallen.leaves = TRUE)

```

Calc error

```{r}
# Calculate MSE for training set
train_pred <- predict(tree_model, train_data)
train_mse <- mean((train_data$G3 - train_pred)^2)

# Calculate MSE for validation set
valid_pred <- predict(tree_model, valid_data)
valid_mse <- mean((valid_data$G3 - valid_pred)^2)

# Print results
cat("Training MSE:", train_mse, "\n")
cat("Validation MSE:", valid_mse)


```

## 2.1 Understanding why the error on test and on validation set are always the same

```{r}
m <- tree_model$cptable
m


```

```{r}
pr<-ned_tree <- prune(tree_model, cp = 0.8)

rpart.plot(pruned_tree)
```

```{r}
# for test tree
# Calculate MSE for training set
train_predPP <- predict(pruned_tree, train_data)
train_msePP <- mean((train_data$G3 - train_predPP)^2)

# Calculate MSE for validation set
valid_predPP <- predict(pruned_tree, valid_data)
valid_msePP <- mean((valid_data$G3 - valid_predPP)^2)

# Print results
cat("Training MSE:", train_msePP, "\n")
cat("Validation MSE:", valid_msePP)
```

So:

-   I train this small tree with a couple of splits only on the training data

-   Calculate it's MSE for both the training data and the test data

    -   I get two numbers

-   Then I look at the xerror and rel error, that were made using cross validation just using the training data, and then normalized so that rel error is 1 at the very start

-   I create a pruned version of the tree and look at it's training error at the very start

-   I multiply the rel error by that value

-   And all the rel errors \* that value are the exact same as the ...

-   ... as the errors I calculated with the MSE function

But obviously this is the case, they are both just using the training set, why wouldn't it be like that. not sure why I was so confused.

# 3. Simple comparison (no test set)

## 3.1 Basic Regression Tree

```{r}
tree_model <- rpart(G3 ~ ., data = sd)

rpart.plot(tree_model, shadow.col = "gray")

# Get variable importance
var_importance <- tree_model$variable.importance

# Variable Importance Plot
var_importance <- data.frame(
    variable = names(tree_model$variable.importance),
    importance = tree_model$variable.importance
)
ggplot(
    var_importance,
    aes(
        x = reorder(variable, importance),
        y = importance
    )
) +
    geom_bar(stat = "identity", fill = "skyblue") +
    coord_flip() +
    theme_minimal() +
    labs(
        title = "Variable Importance",
        x = "Variables",
        y = "Importance"
    )

```

## 3.2 Basic linear regression

```{r}
lm_model <- lm(G3 ~ ., data = sd)

# Summary
summary_lm <- summary(lm_model)
print(summary_lm)

```

## 3.3 Compare MSE of both models

```{r}
sd$lm_pred <- predict(lm_model)
sd$tree_pred <- predict(tree_model)

mse_lm <- mean((sd$G3 - sd$lm_pred)^2)
mse_tree <- mean((sd$G3 - sd$tree_pred)^2)

print(mse_lm)
print(mse_tree)

```

## 3.4 Visualize predictions

```{r}
ggplot(sd, aes(x = G3)) +
    geom_point(aes(y = lm_pred, color = "Linear Regression"), alpha = 0.5) +
    geom_point(aes(y = tree_pred, color = "Regression Tree"), alpha = 0.5) +
    geom_abline(intercept = 0, slope = 1, linetype = "dashed") +
    theme_minimal() +
    labs(
        title = "Predicted vs Actual G3 Scores",
        x = "Actual G3", y = "Predicted G3",
        color = "Model"
    ) +
    scale_color_manual(values = c("Linear Regression" = "blue", "Regression Tree" = "red"))
```

# 4. Plot for understanding trees

```{r}
sd_mini <- sd[, c("sumalc", "absences", "G3")]

tree_model <- rpart(G3 ~ sumalc + absences, data = sd_mini, control = rpart.control(cp = 0.002, minsplit = 40))
rpart.plot(tree_model)

sumalc_seq <- seq(min(sd_mini$sumalc), max(sd_mini$sumalc), length.out = 13)
absences_seq <- seq(min(sd_mini$absences), max(sd_mini$absences), length.out = 13)
grid <- expand.grid(sumalc = sumalc_seq, absences = absences_seq)

grid$G3_pred <- predict(tree_model, newdata = grid)

ggplot(grid, aes(x = sumalc, y = absences, fill = G3_pred)) +
    geom_tile() +
    geom_text(aes(label = round(G3_pred, 1)), size = 3) +
    scale_fill_gradient(low = "darkred", high = "lightblue", guide = "none") +
    labs(x = "Alcohol consumption", y = "Absences", title = "Predicting Exam score based on alcohol consumption and Absences") +
    theme_minimal() +
    geom_point(data = sd_mini, aes(x = sumalc, y = absences), color = "blue", size = 2, inherit.aes = FALSE) # Add blue points for actual data


```

# 5. Pruning

## 5.1 Make and prune large tree

```{r}
# Create a complex tree
complex_tree <- rpart(G3 ~ ., data = train_data, control = rpart.control(cp = 0.002, minsplit = 5))

rpart.plot(complex_tree, main = "Initial Complex Regression Tree")
plotcp(complex_tree)
```

```{r}
# Find optimal CP value
opt_cp <- complex_tree$cptable[which.min(complex_tree$cptable[, "xerror"]), "CP"]
opt_sp_1 <- complex_tree$cptable[which.min(complex_tree$cptable[, "xerror"]), "nsplit"]

print(opt_sp_1) # for xerror using cross validation
print(opt_cp) # for xerror
print(min(complex_tree$cptable[, "xerror"]))
print(min(complex_tree$cptable[, "rel error"]))

# Prune the tree
pruned_tree <- prune(complex_tree, cp = opt_cp)

rpart.plot(pruned_tree, main = "Optimal Pruned Tree")

```

## 5.2 Compare original and pruned tree

using the split of earlier data, for double not over fitting.

```{r}
# Calculate the MSEs
MSE_complex <- function() {
    train_pred_complex <- predict(complex_tree, train_data)
    test_pred_complex <- predict(complex_tree, valid_data)
    train_mse_complex <- calculate_mse(train_data$G3, train_pred_complex)
    test_mse_complex <- calculate_mse(valid_data$G3, test_pred_complex)

    print(paste("Complex Tree - Training MSE:", train_mse_complex))
    print(paste("Complex Tree -     Test MSE:", test_mse_complex))
}
MSE_pruned <- function() {
    train_pred_pruned <- predict(pruned_tree, train_data)
    test_pred_pruned <- predict(pruned_tree, valid_data)
    train_mse_pruned <- calculate_mse(train_data$G3, train_pred_pruned)
    test_mse_pruned <- calculate_mse(valid_data$G3, test_pred_pruned)

    print(paste("Pruned Tree  - Training MSE:", train_mse_pruned))
    print(paste("Pruned Tree  -     Test MSE:", test_mse_pruned))
}

MSE_complex()
MSE_pruned()
```

```{r}
cp_table <- complex_tree$cptable
```

## 5.3 Make pruning plot

```{r}
num_splits <- cp_table[, "nsplit"]

plot(
    cp_table[, "nsplit"],
    cp_table[, "xerror"]
)

# Calculate MSE for each CP value
mse_values <- sapply(cp_table[, "CP"], function(cp) {
    pruned <- prune(complex_tree, cp = cp)
    train_pred <- predict(pruned, train_data)
    test_pred <- predict(pruned, valid_data)
    train_mse <- calculate_mse(train_data$G3, train_pred)
    test_mse <- calculate_mse(valid_data$G3, test_pred)
    c(train_mse, test_mse)
})

# Making data for ggplot
mse_data <- data.frame(
    num_splits = rep(num_splits, 2),
    mse = c(mse_values[1, ], mse_values[2, ]),
    type = rep(c("Training MSE", "Test MSE"), each = length(num_splits))
)

temp_mse_data <- mse_data %>% filter(type == "Test MSE")
opt_splits <- temp_mse_data$num_splits[which.min(temp_mse_data$mse)]
min_mse <- min(temp_mse_data$mse)

print(opt_splits)
print(min_mse) # Should both be the same as in previous cells

```

If the opt_sp_1 for xerror is 2 and the op_splits using mse and the test set is also 2 that means the optimal cp is also the same for both.

Except for some reason sometimes the two MSEs are differen't because the one finds tree with minimal error using cross validation, and calculates the MSE with the test set for that one

The other one finds which size of tree has the minimal MSE for the test set, in doing so doing a second kind of overfitting

Make the plot

```{r}
ggplot(mse_data, aes(x = num_splits, y = mse, color = type)) +
    geom_line() +
    geom_point() +
    scale_color_manual(values = c("blue", "red")) +
    labs(
        x = "Number of Splits", y = "Mean Squared Error",
        title = "MSE vs Tree Complexity",
        color = "MSE Type"
    ) +
    theme_minimal() +
    geom_vline(xintercept = opt_splits, linetype = "dashed", color = "purple") +
    annotate("text",
        x = opt_splits + 18, y = min_mse - 1, label = paste("Optimal number of splits =", opt_splits),
        vjust = -1, color = "purple"
    )
```

other plot for fun

```{r}
adj <- mse_data[1, 2]
len <- length(cp_table[, "nsplit"])

data_conc <- data.frame(
    rep(cp_table[, "nsplit"], 3),
    c(
        cp_table[, "xerror"] * adj,
        cp_table[, "rel error"] * adj,
        c(mse_values[2, ])
    ),
    c(
        rep("Cross validation Error", len),
        rep("Training Error", len),
        rep("Test Error", len)
    )
)

ggplot(data_conc, aes(x = data_conc[, 1], y = data_conc[, 2], color = data_conc[, 3])) +
    geom_line() +
    geom_point() +
    scale_color_manual(values = c("orange", "red", "blue")) +
    labs(
        x = "Number of Splits", y = "Mean Squared Error",
        title = "MSE vs Tree Complexity",
        color = "MSE Type"
    ) +
    theme_minimal()
```

# 6. Comparing BART, Regression Tree and linear regression

## 6.1 Comparing the 3

Create tree

```{r}
tree_model <- rpart(G3 ~ ., data = train_data, control = rpart.control(cp = opt_cp, minsplit = 5))
rpart.plot(tree_model, fallen.leaves = TRUE)

```

Create regression

```{r}
# Create REGR
lm_model <- lm(G3 ~ ., data = train_data)
summary(lm_model)
```

Create BART

```{r}
# Prepare data for BART
x_train <- train_data[, !names(train_data) %in% "G3"]
y_train <- train_data$G3
x_valid <- valid_data[, !names(valid_data) %in% "G3"]
y_valid <- valid_data$G3



# Fit BART model
bart_model <- wbart(
    x.train = x_train,
    y.train = y_train,
    x.test = x_valid,
    nskip = 1000,
    ntree = 200,
    ndpost = 1000,
    printevery = 500
)
```

calculate the errors

```{r}
# Print results
comp <- function() {
    cat("Training MSE   Tree:", train_mse_tree, "\n")
    cat("Validation MSE Tree:", valid_mse_tree, "\n\n")
    cat("Training MSE   Regr:", train_mse_regr, "\n")
    cat("Validation MSE Regr:", valid_mse_regr, "\n\n")
    cat("Training MSE   BART:", train_mse_bart, "\n")
    cat("Validation MSE BART:", valid_mse_bart, "\n")
}

```

```{r}
# tree error
# Calculate MSE for training set
train_pred_tree <- predict(tree_model, train_data)
train_mse_tree <- mean((train_data$G3 - train_pred_tree)^2)

# Calculate MSE for validation set
valid_pred_tree <- predict(tree_model, valid_data)
valid_mse_tree <- mean((valid_data$G3 - valid_pred_tree)^2)


# regr error
# Calculate MSE for training set
train_pred_regr <- predict(lm_model, train_data)
train_mse_regr <- mean((train_data$G3 - train_pred_regr)^2)

# Calculate MSE for validation set
valid_pred_regr <- predict(lm_model, valid_data)
valid_mse_regr <- mean((valid_data$G3 - valid_pred_regr)^2)


# BART error
# Calculate MSE for training set
train_pred_bart <- bart_model$yhat.train.mean
train_mse_bart <- mean((y_train - train_pred_bart)^2)

# Calculate MSE for validation set
valid_pred_bart <- bart_model$yhat.test.mean
valid_mse_bart <- mean((y_valid - valid_pred_bart)^2)



comp()
```

## 6.2 Closer look at BART

```{r}
bart_minim <- wbart(
    x.train = x_train,
    y.train = y_train,
    x.test = x_valid,
    nskip = 100,
    ntree = 100,
    ndpost = 50,
    printevery = 500
)


fitmat <- cbind(train_data$G3, bart_model$yhat.train.mean, bart_minim$yhat.train.mean)

pairs(fitmat)

cor(fitmat)
```

```{r}
# BART error
# Calculate MSE for training set
train_pred_bart <- bart_minim$yhat.train.mean
train_mse_bart <- mean((y_train - train_pred_bart)^2)

# Calculate MSE for validation set
valid_pred_bart <- bart_minim$yhat.test.mean
valid_mse_bart <- mean((y_valid - valid_pred_bart)^2)

```

```{r}
print(train_mse_bart)
print(valid_mse_bart)
```

```{r}
# compute row percentages
percount <- bart_model$varcount / apply(bart_model$varcount, 1, sum)
# mean of row percentages
mvp <- apply(percount, 2, mean)
# quantiles of row percentags
qm <- apply(percount, 2, quantile, probs = c(0.3, 0.7))


# Assuming qm is a matrix or data frame
p <- ncol(qm)
rgy <- range(qm, na.rm = TRUE)

```

```{r}
# Create the plot
plot(c(1, p), rgy,
    type = "n", xlab = "variable",
    ylab = "post mean, percent var use", axes = FALSE
)

# Add x-axis
axis(1, at = 1:p, labels = colnames(qm), cex.lab = 0.7, cex.axis = 0.7)

# Add y-axis
axis(2, cex.lab = 1.2, cex.axis = 1.2)


lines(1:p, mvp, col = "black", lty = 4, pch = 4, type = "b", lwd = 1.5)


# Add vertical lines
for (i in 1:p) {
    lines(c(i, i), qm[, i], col = "blue", lty = 3, lwd = 1.0)
}
```

# All the stuff I'm not using

## N1. making Small tree for interactions

using the usual method

```{r}
tree_model_2splits <- rpart(G3 ~ ., data = sd, control = rpart.control(maxdepth = 2))

# Plot the tree
rpart.plot(tree_model_2splits, extra = 101, fallen.leaves = TRUE, type = 2, main = "Decision Tree with Two Splits")

sd$tree_pred_2splits <- predict(tree_model_2splits)

mse_tree <- mean((sd$G3 - sd$tree_pred_2splits)^2)

print(mse_tree)

```

MSE should be 7.783039

recreating the same tree by hand (I promise this will make sense later)

```{r}
# Split the data based on failures
failures_yes <- subset(sd, failures >= 1)
failures_no <- subset(sd, failures == 0)

# Further split the data
failures_yes_absences_yes <- subset(failures_yes, absences < 1)
failures_yes_absences_no <- subset(failures_yes, absences >= 1)

failures_no_higher_yes <- subset(failures_no, higher == "no")
failures_no_higher_no <- subset(failures_no, higher == "yes")

# Calculate the mean of G3 for each subset
mean_failures_yes_absences_yes <- mean(failures_yes_absences_yes$G3)
mean_failures_yes_absences_no <- mean(failures_yes_absences_no$G3)

mean_failures_no_higher_yes <- mean(failures_no_higher_yes$G3)
mean_failures_no_higher_no <- mean(failures_no_higher_no$G3)

# Calculate the MSE for each subset
mse_failures_yes_absences_yes <- mean((failures_yes_absences_yes$G3 - mean_failures_yes_absences_yes)^2)
mse_failures_yes_absences_no <- mean((failures_yes_absences_no$G3 - mean_failures_yes_absences_no)^2)

mse_failures_no_higher_yes <- mean((failures_no_higher_yes$G3 - mean_failures_no_higher_yes)^2)
mse_failures_no_higher_no <- mean((failures_no_higher_no$G3 - mean_failures_no_higher_no)^2)

# Calculate the total MSE
total_mse <- (mse_failures_yes_absences_yes * nrow(failures_yes_absences_yes) +
    mse_failures_yes_absences_no * nrow(failures_yes_absences_no) +
    mse_failures_no_higher_yes * nrow(failures_no_higher_yes) +
    mse_failures_no_higher_no * nrow(failures_no_higher_no)) / nrow(sd)

total_mse

```

MSE should also be 7.783039

Creating the same tree but with swapped splits by hand

```{r}
# Split the data based on failures
failures_yes <- subset(sd, failures >= 1)
failures_no <- subset(sd, failures == 0)

# Further split the data, swapping the splits
# For failures_yes, split by 'higher' instead of 'absences'
failures_yes_higher_yes <- subset(failures_yes, higher == "yes")
failures_yes_higher_no <- subset(failures_yes, higher == "no")

# For failures_no, split by 'absences' instead of 'higher'
failures_no_absences_yes <- subset(failures_no, absences < 12)
failures_no_absences_no <- subset(failures_no, absences >= 12)

# Calculate the mean of G3 for each subset
mean_failures_yes_higher_yes <- mean(failures_yes_higher_yes$G3)
mean_failures_yes_higher_no <- mean(failures_yes_higher_no$G3)

mean_failures_no_absences_yes <- mean(failures_no_absences_yes$G3)
mean_failures_no_absences_no <- mean(failures_no_absences_no$G3)

# Calculate the MSE for each subset
mse_failures_yes_higher_yes <- mean((failures_yes_higher_yes$G3 - mean_failures_yes_higher_yes)^2)
mse_failures_yes_higher_no <- mean((failures_yes_higher_no$G3 - mean_failures_yes_higher_no)^2)

mse_failures_no_absences_yes <- mean((failures_no_absences_yes$G3 - mean_failures_no_absences_yes)^2)
mse_failures_no_absences_no <- mean((failures_no_absences_no$G3 - mean_failures_no_absences_no)^2)

# Calculate the total MSE
total_mse_swapped <- (mse_failures_yes_higher_yes * nrow(failures_yes_higher_yes) +
    mse_failures_yes_higher_no * nrow(failures_yes_higher_no) +
    mse_failures_no_absences_yes * nrow(failures_no_absences_yes) +
    mse_failures_no_absences_no * nrow(failures_no_absences_no)) / nrow(sd)

total_mse_swapped

```
