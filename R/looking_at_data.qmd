---
title: "looking_at_data"
author: "timothy"
format: html
editor: visual
---

Preliminaries

Packages

```{r}
library(tidyverse)
library(rpart)
library(rpart.plot)
library(data.tree)
library(networkD3)
library(plotly)
library(caret)
library(BART)
library(lintr)
library(styler)
```

WD

```{r}
getwd()
setwd("C:/Users/timtj/GitHub/wissenschaftliches-arbeiten/R")
```

Function to calculate MSE (That I should really use more)

```{r}

# Function to calculate MSE
calculate_mse <- function(actual, predicted) {
  mean((actual - predicted)^2)
}
```

# 1. Setting up data

```{r}
load("C:/Users/timtj/GitHub/wissenschaftliches-arbeiten/R/student_por.RData")
sd <- data.frame(student_por)

sd <- sd %>%
  mutate_if(is.character, as.factor)

sd$sumalc <- sd$Walc + sd$Dalc
sd$Dalc <- NULL
sd$Walc <- NULL

# Removing these because that would just be cheating
sd$G2 <- NULL
sd$G1 <- NULL

# also do basic test if data is working
plot(sd$studytime, sd$G3, col = "blue", pch = 19)
grid()
```

Split data into training and test set

```{r}
# Split the data into training (70%) and validation (30%) sets
split_index <- createDataPartition(sd$G3, p = 0.7, list = FALSE)
train_data <- sd[split_index, ]
valid_data <- sd[-split_index, ]

```

# 2. Single tree on data sets

```{r}

tree_model <- rpart(G3 ~ ., data = train_data, control = rpart.control(cp = 0.025, minsplit = 5))
rpart.plot(tree_model, fallen.leaves = TRUE)

```

Calc error

```{r}


# Calculate MSE for training set
train_pred <- predict(tree_model, train_data)
train_mse <- mean((train_data$G3 - train_pred)^2)

# Calculate MSE for validation set
valid_pred <- predict(tree_model, valid_data)
valid_mse <- mean((valid_data$G3 - valid_pred)^2)

# Print results
cat("Training MSE:", train_mse, "\n")
cat("Validation MSE:", valid_mse)


```

## 2.1 Desperately trying to understand why the error on test and on validation set are always the same

```{r}
m = tree_model$cptable
m


```

```{r}
pruned_tree <- prune(tree_model, cp = 0.8)

rpart.plot(pruned_tree)
```

```{r}

# for test tree
# Calculate MSE for training set
train_predPP <- predict(pruned_tree, train_data)
train_msePP <- mean((train_data$G3 - train_predPP)^2)

# Calculate MSE for validation set
valid_predPP <- predict(pruned_tree, valid_data)
valid_msePP <- mean((valid_data$G3 - valid_predPP)^2)

# Print results
cat("Training MSE:", train_msePP, "\n")
cat("Validation MSE:", valid_msePP)



```

```{r}
11.33771 / 10.03354
```

```{r}
0.7381095 * 10.03354

```

So:

-   I train this small tree with a couple of splits only on the training data

-   Calculate it's MSE for both the training data and the test data

    -   I get two numbers

-   Then I look at the xerror and rel error, that were made using cross validation just using the training data, and then normalized so that rel error is 1 at the very start

-   I create a pruned version of the tree and look at it's training error at the very start

-   I multiply the rel error by that value

-   And all the rel errors \* that value are the exact same as the ...

-   ... as the errors I calculated with the MSE function

But obviously this is the case, they are both just using the training set, why wouldn't it be like that. not sure why I was so confused.

# 3. Simple comparison (no test set)

## 3.1 Basic Regression Tree

```{r}


tree_model <- rpart(G3 ~ ., data = sd)

rpart.plot(tree_model, shadow.col = "gray")

# Get variable importance
var_importance <- tree_model$variable.importance

# Variable Importance Plot
var_importance <- data.frame(
  variable = names(tree_model$variable.importance),
  importance = tree_model$variable.importance
)
ggplot(
  var_importance,
  aes(x = reorder(variable,importance),
      y = importance)) +
  geom_bar(stat = "identity", fill = "skyblue") +
  coord_flip() +
  theme_minimal() +
  labs(title = "Variable Importance",
       x = "Variables",
       y = "Importance")

```

## 3.2 Basic linear regression

```{r}

lm_model <- lm(G3 ~ ., data = sd)

# Summary
summary_lm <- summary(lm_model)
print(summary_lm)

```

## 3.3 Compare MSE of both models

```{r}


sd$lm_pred <- predict(lm_model)
sd$tree_pred <- predict(tree_model)

mse_lm <- mean((sd$G3 - sd$lm_pred)^2)
mse_tree <- mean((sd$G3 - sd$tree_pred)^2)

print(mse_lm)
print(mse_tree)

```

## 3.4 Visualize predictions

```{r}

ggplot(sd, aes(x = G3)) +
  geom_point(aes(y = lm_pred, color = "Linear Regression"), alpha = 0.5) +
  geom_point(aes(y = tree_pred, color = "Regression Tree"), alpha = 0.5) +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed") +
  theme_minimal() +
  labs(
    title = "Predicted vs Actual G3 Scores",
    x = "Actual G3", y = "Predicted G3",
    color = "Model"
  ) +
  scale_color_manual(values = c("Linear Regression" = "blue", "Regression Tree" = "red"))



```

# 4. Plot for understanding trees

```{r}


sd_mini <- sd[, c("Walc", "Dalc", "absences", "G3")]
sd_mini$sumalc <- sd_mini$Walc + sd_mini$Dalc
sd_mini <- sd_mini[, c("sumalc", "absences", "G3")]



tree_model <- rpart(G3 ~ sumalc + absences, data = sd_mini, control = rpart.control(cp = 0.002, minsplit = 40))
rpart.plot(tree_model)

sumalc_seq <- seq(min(sd_mini$sumalc), max(sd_mini$sumalc), length.out = 13)
absences_seq <- seq(min(sd_mini$absences), max(sd_mini$absences), length.out = 13)
grid <- expand.grid(sumalc = sumalc_seq, absences = absences_seq)

grid$G3_pred <- predict(tree_model, newdata = grid)

ggplot(grid, aes(x = sumalc, y = absences, fill = G3_pred)) +
  geom_tile() +
  geom_text(aes(label = round(G3_pred, 1)), size = 3) +
  scale_fill_gradient(low = "darkred", high = "lightblue", guide = "none") +
  labs(x = "Alcohol consumption", y = "Absences", title = "Predicting Exam score based on alcohol consumption and Absences") +
  theme_minimal() +
  geom_point(data = sd_mini, aes(x = sumalc, y = absences), color = "blue", size = 2, inherit.aes = FALSE) # Add blue points for actual data


```

# 5. Pruning

## 5.1 Make and prune large tree

```{r}
# Create a complex tree
complex_tree <- rpart(G3 ~ ., data = train_data, control = rpart.control(cp = 0.002, minsplit = 5))

rpart.plot(complex_tree, main = "Initial Complex Regression Tree")
plotcp(complex_tree)
```

```{r}
# Find optimal CP value
opt_cp   <- complex_tree$cptable[which.min(complex_tree$cptable[, "xerror"]), "CP"]
opt_sp_1 <- complex_tree$cptable[which.min(complex_tree$cptable[, "xerror"]), "nsplit"]



print(opt_sp_1) # for xerror using cross validation
print(opt_cp) # for xerror
print(min(complex_tree$cptable[, "xerror"]))
print(min(complex_tree$cptable[, "rel error"]))

# Prune the tree
pruned_tree <- prune(complex_tree, cp = opt_cp)

rpart.plot(pruned_tree, main = "Optimal Pruned Tree")

```

## 5.2 Compare original and pruned tree

using the split of earlier data, for double not over fitting.

```{r}

# Calculate the MSEs
MSE_complex <- function() {
  train_pred_complex <- predict(complex_tree, train_data)
  test_pred_complex <- predict(complex_tree, valid_data)
  train_mse_complex <- calculate_mse(train_data$G3, train_pred_complex)
  test_mse_complex <- calculate_mse(valid_data$G3, test_pred_complex)

  print(paste("Complex Tree - Training MSE:", train_mse_complex))
  print(paste("Complex Tree -     Test MSE:", test_mse_complex))
}
MSE_pruned <- function() {
  train_pred_pruned <- predict(pruned_tree, train_data)
  test_pred_pruned <- predict(pruned_tree, valid_data)
  train_mse_pruned <- calculate_mse(train_data$G3, train_pred_pruned)
  test_mse_pruned <- calculate_mse(valid_data$G3, test_pred_pruned)

  print(paste("Pruned Tree  - Training MSE:", train_mse_pruned))
  print(paste("Pruned Tree  -     Test MSE:", test_mse_pruned))
}

MSE_complex()
MSE_pruned()
```

```{r}
cp_table <- complex_tree$cptable


0.8469317 * 10.03636
0.1993765 * 10.03636



#[1] "Complex Tree - Training MSE: 2.00101337636504"
#[1] "Complex Tree -     Test MSE: 11.7359314930842"
#[1] "Pruned Tree  - Training MSE: 8.10972820563119"
#[1] "Pruned Tree  -     Test MSE: 8.05554703417732"


?sapply

cp_table[, "CP"]

```

```{r}

first_mse <- sapply(0.140409142, function(cp) {
  pruned <- prune(complex_tree, cp = cp)
  train_pred <- predict(pruned, train_data)
  test_pred <- predict(pruned, valid_data)
  train_mse <- calculate_mse(train_data$G3, train_pred)
  test_mse <- calculate_mse(valid_data$G3, test_pred)
  c(train_mse, test_mse)
})

first_mse

```

```{r}
first_mse <- sapply(cp_table[, "CP"], function(cp) {
  pruned <- prune(complex_tree, cp = cp)
  train_pred <- predict(pruned, train_data)
  test_pred <- predict(pruned, valid_data)
  train_mse <- calculate_mse(train_data$G3, train_pred)
  test_mse <- calculate_mse(valid_data$G3, test_pred)
  c(train_mse, test_mse)
})

```

## 5.3 Make pruning plot

```{r}


num_splits <- cp_table[, "nsplit"]

plot(
  cp_table[, "nsplit"],
  cp_table[, "xerror"]

)


# Calculate MSE for each CP value
mse_values <- sapply(cp_table[, "CP"], function(cp) {
  pruned <- prune(complex_tree, cp = cp)
  train_pred <- predict(pruned, train_data)
  test_pred <- predict(pruned, valid_data)
  train_mse <- calculate_mse(train_data$G3, train_pred)
  test_mse <- calculate_mse(valid_data$G3, test_pred)
  c(train_mse, test_mse)
})

# Making data for ggplot
mse_data <- data.frame(
  num_splits = rep(num_splits, 2),
  mse = c(mse_values[1, ], mse_values[2, ]),
  type = rep(c("Training MSE", "Test MSE"), each = length(num_splits))
)


opt_splits_alt <- temp_mse_data$num_splits[which.min(temp_mse_data$mse)]


temp_mse_data <- mse_data %>% filter(type == "Test MSE")
opt_splits <- temp_mse_data$num_splits[which.min(temp_mse_data$mse)]
min_mse <- min(temp_mse_data$mse)

print(opt_splits)
print(min_mse) # Should both be the same as in previous cells

```

If the opt_sp_1 for xerror is 2 and the op_splits using mse and the test set is also 2 that means the optimal cp is also the same for both.

Except for some reason sometimes the two MSEs are differen't because the one finds tree with minimal error using cross validation, and calculates the MSE with the test set for that one

The other one finds which size of tree has the minimal MSE for the test set, in doing so doing a second kind of overfitting

Make the plot

```{r}
ggplot(mse_data, aes(x = num_splits, y = mse, color = type)) +
  geom_line() +
  geom_point() +
  scale_color_manual(values = c("blue", "red")) +
  labs(
    x = "Number of Splits", y = "Mean Squared Error",
    title = "MSE vs Tree Complexity",
    color = "MSE Type"
  ) +
  theme_minimal() +
  geom_vline(xintercept = opt_splits, linetype = "dashed", color = "purple") +
  annotate("text",
    x = opt_splits + 18, y = min_mse -1 , label = paste("Optimal number of splits =", opt_splits),
    vjust = -1, color = "purple"
  )


```

other plot for fun

```{r}


mse_values
test_table = cp_table * 10.03636

adj = 10.03636
len = length(cp_table[,"nsplit"])
len

data_conc = data.frame(
    rep(cp_table[, "nsplit"],4),
    c(cp_table[, "xerror"]*adj,
      cp_table[, "rel error"]*adj,
      c(mse_values[1,],mse_values[2,])
      ),
    c(rep("xe_er",len),
      rep("re_er",len),
      rep("tr_er",len),
      rep("te_er",len)
  )
)

ggplot(data_conc, aes(x = data_conc[,1], y = data_conc[,2], color = data_conc[,3])) +
  geom_line() +
  geom_point() +
  scale_color_manual(values = c("green", "red","blue","yellow")) +
  labs(
    x = "Number of Splits", y = "Mean Squared Error",
    title = "MSE vs Tree Complexity",
    color = "MSE Type"
  )
  theme_minimal()
```

# 6. Comparing BART, Regression Tree and linear regression

Create tree

```{r}

tree_model <- rpart(G3 ~ ., data = train_data, control = rpart.control(cp = opt_cp-0.0001 , minsplit = 5))
rpart.plot(tree_model, fallen.leaves = TRUE)

```

Create regression

```{r}
# Create REGR
lm_model <- lm(G3 ~ ., data = train_data)
summary(lm_model)
```

Create BART

```{r}

# Prepare data for BART
x_train <- train_data[, !names(train_data) %in% "G3"]
y_train <- train_data$G3
x_valid <- valid_data[, !names(valid_data) %in% "G3"]
y_valid <- valid_data$G3



# Fit BART model
bart_model <- wbart(
  x.train = x_train,
  y.train = y_train,
  x.test = x_valid,
  nskip = 2000,
  ntree = 200,
  ndpost = 2000,
  printevery = 500)
```

calculate the errors

```{r}

# tree error
# Calculate MSE for training set
train_pred_tree <- predict(tree_model, train_data)
train_mse_tree <- mean((train_data$G3 - train_pred_tree)^2)

# Calculate MSE for validation set
valid_pred_tree <- predict(tree_model, valid_data)
valid_mse_tree <- mean((valid_data$G3 - valid_pred_tree)^2)


# regr error
# Calculate MSE for training set
train_pred_regr <- predict(lm_model, train_data)
train_mse_regr <- mean((train_data$G3 - train_pred_regr)^2)

# Calculate MSE for validation set
valid_pred_regr <- predict(lm_model, valid_data)
valid_mse_regr <- mean((valid_data$G3 - valid_pred_regr)^2)


# BART error
# Calculate MSE for training set
train_pred_bart <- bart_model$yhat.train.mean
train_mse_bart <- mean((y_train - train_pred_bart)^2)

# Calculate MSE for validation set
valid_pred_bart <- bart_model$yhat.test.mean
valid_mse_bart <- mean((y_valid - valid_pred_bart)^2)

# Print results
comp <- function() {
  cat("Training MSE   Tree:", train_mse_tree, "\n")
  cat("Validation MSE Tree:", valid_mse_tree, "\n\n")
  cat("Training MSE   Regr:", train_mse_regr, "\n")
  cat("Validation MSE Regr:", valid_mse_regr, "\n\n")
  cat("Training MSE   BART:", train_mse_bart, "\n")
  cat("Validation MSE BART:", valid_mse_bart, "\n")
}

comp()
```

# All the stuff I'm not using

## N1. 3D Scatter Plot with Decision Boundaries, need to fix, worked before not sure why not now

```{r}

x = sd


# Get the variables used for splitting
split_vars <- pruned_tree$frame$var[pruned_tree$frame$var != "<leaf>"]
split_vars <- unique(as.character(split_vars))

# If there are fewer than 2 splits, notify the user
if (1 == 1) {
  # Create a 3D scatter plot with decision boundaries
  plot_ly(sd,
    x = ~ get(split_vars[1]), y = ~ get(split_vars[2]), z = ~G3,
    type = "scatter3d", mode = "markers",
    marker = list(size = 3, color = ~G3, colorscale = "Viridis", opacity = 0.8)
  ) %>%
    add_markers() %>%
    layout(scene = list(
      xaxis = list(title = split_vars[1]),
      yaxis = list(title = split_vars[2]),
      zaxis = list(title = "G3")
    ))

  # Function to add a plane to the plot
  add_plane <- function(p, split_var, split_value, color) {
    var_range <- range(sd[[split_var]])
    other_var <- setdiff(split_vars, split_var)[1]
    other_range <- range(sd[[other_var]])

    if (split_var == split_vars[1]) {
      x <- rep(split_value, 2)
      y <- other_range
    } else {
      x <- var_range
      y <- rep(split_value, 2)
    }

    z <- matrix(rep(range(sd$G3), each = 2), nrow = 2)

    add_surface(p, x = x, y = y, z = z, opacity = 0.3, colorscale = list(c(0, 1), c(color, color)))
  }

  # Get split points
  splits <- pruned_tree$splits
  split_points <- splits[splits[, "count"] > 0, "index"]

  # Create the plot with decision boundaries
  p <- plot_ly(sd,
    x = ~ get(split_vars[1]), y = ~ get(split_vars[2]), z = ~G3,
    type = "scatter3d", mode = "markers",
    marker = list(size = 3, color = ~G3, colorscale = "Viridis", opacity = 0.8)
  ) %>%
    layout(scene = list(
      xaxis = list(title = split_vars[1]),
      yaxis = list(title = split_vars[2]),
      zaxis = list(title = "G3")
    ))

  # Add planes for each split
  for (i in 1:length(split_points)) {
    p <- add_plane(p, split_vars[i], split_points[i], color = c("red", "blue")[i])
  }

  # Display the plot
  p
}

```

## N2. makeing Small tree for interactions

using the usual method

```{r}


tree_model_2splits <- rpart(G3 ~ ., data = sd, control = rpart.control(maxdepth = 2))

# Plot the tree
rpart.plot(tree_model_2splits, extra = 101, fallen.leaves = TRUE, type = 2, main = "Decision Tree with Two Splits")

sd$tree_pred_2splits <- predict(tree_model_2splits)

mse_tree <- mean((sd$G3 - sd$tree_pred_2splits)^2)

print(mse_tree)

```

MSE should be 7.783039

recreating the same tree by hand (I promise this will make sense later)

```{r}


# Split the data based on failures
failures_yes <- subset(sd, failures >= 1)
failures_no <- subset(sd, failures == 0)

# Further split the data
failures_yes_absences_yes <- subset(failures_yes, absences < 1)
failures_yes_absences_no <- subset(failures_yes, absences >= 1)

failures_no_higher_yes <- subset(failures_no, higher == "no")
failures_no_higher_no <- subset(failures_no, higher == "yes")

# Calculate the mean of G3 for each subset
mean_failures_yes_absences_yes <- mean(failures_yes_absences_yes$G3)
mean_failures_yes_absences_no <- mean(failures_yes_absences_no$G3)

mean_failures_no_higher_yes <- mean(failures_no_higher_yes$G3)
mean_failures_no_higher_no <- mean(failures_no_higher_no$G3)

# Calculate the MSE for each subset
mse_failures_yes_absences_yes <- mean((failures_yes_absences_yes$G3 - mean_failures_yes_absences_yes)^2)
mse_failures_yes_absences_no <- mean((failures_yes_absences_no$G3 - mean_failures_yes_absences_no)^2)

mse_failures_no_higher_yes <- mean((failures_no_higher_yes$G3 - mean_failures_no_higher_yes)^2)
mse_failures_no_higher_no <- mean((failures_no_higher_no$G3 - mean_failures_no_higher_no)^2)

# Calculate the total MSE
total_mse <- (mse_failures_yes_absences_yes * nrow(failures_yes_absences_yes) +
  mse_failures_yes_absences_no * nrow(failures_yes_absences_no) +
  mse_failures_no_higher_yes * nrow(failures_no_higher_yes) +
  mse_failures_no_higher_no * nrow(failures_no_higher_no)) / nrow(sd)

total_mse

```

MSE should also be 7.783039

Creating the same tree but with swapped splits by hand

```{r}


# Split the data based on failures
failures_yes <- subset(sd, failures >= 1)
failures_no <- subset(sd, failures == 0)

# Further split the data, swapping the splits
# For failures_yes, split by 'higher' instead of 'absences'
failures_yes_higher_yes <- subset(failures_yes, higher == "yes")
failures_yes_higher_no <- subset(failures_yes, higher == "no")

# For failures_no, split by 'absences' instead of 'higher'
failures_no_absences_yes <- subset(failures_no, absences < 12)
failures_no_absences_no <- subset(failures_no, absences >= 12)

# Calculate the mean of G3 for each subset
mean_failures_yes_higher_yes <- mean(failures_yes_higher_yes$G3)
mean_failures_yes_higher_no <- mean(failures_yes_higher_no$G3)

mean_failures_no_absences_yes <- mean(failures_no_absences_yes$G3)
mean_failures_no_absences_no <- mean(failures_no_absences_no$G3)

# Calculate the MSE for each subset
mse_failures_yes_higher_yes <- mean((failures_yes_higher_yes$G3 - mean_failures_yes_higher_yes)^2)
mse_failures_yes_higher_no <- mean((failures_yes_higher_no$G3 - mean_failures_yes_higher_no)^2)

mse_failures_no_absences_yes <- mean((failures_no_absences_yes$G3 - mean_failures_no_absences_yes)^2)
mse_failures_no_absences_no <- mean((failures_no_absences_no$G3 - mean_failures_no_absences_no)^2)

# Calculate the total MSE
total_mse_swapped <- (mse_failures_yes_higher_yes * nrow(failures_yes_higher_yes) +
  mse_failures_yes_higher_no * nrow(failures_yes_higher_no) +
  mse_failures_no_absences_yes * nrow(failures_no_absences_yes) +
  mse_failures_no_absences_no * nrow(failures_no_absences_no)) / nrow(sd)

total_mse_swapped

```

## N3 linear model to look at BART not sure what this does

```{r}


lmf <- lm(G3 ~ ., train_data)

plot(bf$sigma, ylim = c(1.5, 5), xlab = "MCMC iteration", ylab = "sigma draw", cex = .5)
abline(h = summary(lmf)$sigma, col = "red", lty = 2) # least squares estimates
abline(v = burn, col = "green")
title(main = "sigma draws, green line at burn in, red line at least squares estimate", cex.main = .8)



thin <- 20
ii <- burn + thin * (1:(nd / thin))
acf(bf$sigma[ii], main = "ACF of thinned post burn-in sigma draws")

# making small BART model to compare
bf20 <- wbart(x, y, nskip = burn, ndpost = nd, ntree = 20, printevery = 500)


fitmat <- cbind(y, bf$yhat.train.mean, bf20$yhat.train.mean)
colnames(fitmat) <- c("y", "yhatBART", "yhatBART20")
pairs(fitmat)

print(cor(fitmat))


dim(bf20$varcount)




# compute row percentages
percount20 <- bf20$varcount / apply(bf20$varcount, 1, sum)
# mean of row percentages
mvp20 <- apply(percount20, 2, mean)
# quantiles of row percentags
qm <- apply(percount20, 2, quantile, probs = c(.05, .95))

print(mvp20)



# Assuming qm is a matrix or data frame
p <- ncol(qm)
rgy <- range(qm, na.rm = TRUE)

# Create the plot
plot(c(1, p), rgy,
  type = "n", xlab = "variable",
  ylab = "post mean, percent var use", axes = FALSE
)

# Add x-axis
axis(1, at = 1:p, labels = colnames(qm), cex.lab = 0.7, cex.axis = 0.7)

# Add y-axis
axis(2, cex.lab = 1.2, cex.axis = 1.2)

# Add lines for mvp20 if it exists and has the correct length
if (exists("mvp20") && length(mvp20) == p) {
  lines(1:p, mvp20, col = "black", lty = 4, pch = 4, type = "b", lwd = 1.5)
}

# Add vertical lines
for (i in 1:p) {
  lines(c(i, i), qm[, i], col = "blue", lty = 3, lwd = 1.0)
}






percount <- bf$varcount / apply(bf$varcount, 1, sum)
mvp <- apply(percount, 2, mean)
plot(mvp20, xlab = "variable number", ylab = "post mean, percent var use", col = "blue", type = "b")
lines(mvp, type = "b", col = "red")
legend("topleft", legend = c("BART", "BART20"), col = c("red", "blue"), lty = c(1, 1))






```

## N4. BART

Fit BART model

```{r}


# Using the BART package

burn <- 1000
nd <- 1000

y <- sd$G3
x <- sd[, 1:30]
p <- ncol(x)

bf <- wbart(x, y, nskip = burn, ndpost = nd, printevery = 500)

# linear model

lmf <- lm(G3 ~ ., sd)

plot(bf$sigma, ylim = c(1.5, 5), xlab = "MCMC iteration", ylab = "sigma draw", cex = .5)
abline(h = summary(lmf)$sigma, col = "red", lty = 2) # least squares estimates
abline(v = burn, col = "green")
title(main = "sigma draws, green line at burn in, red line at least squares estimate", cex.main = .8)



thin <- 20
ii <- burn + thin * (1:(nd / thin))
acf(bf$sigma[ii], main = "ACF of thinned post burn-in sigma draws")

# making small BART model to compare
bf20 <- wbart(x, y, nskip = burn, ndpost = nd, ntree = 20, printevery = 500)


fitmat <- cbind(y, bf$yhat.train.mean, bf20$yhat.train.mean)
colnames(fitmat) <- c("y", "yhatBART", "yhatBART20")
pairs(fitmat)

print(cor(fitmat))


dim(bf20$varcount)




# compute row percentages
percount20 <- bf20$varcount / apply(bf20$varcount, 1, sum)
# mean of row percentages
mvp20 <- apply(percount20, 2, mean)
# quantiles of row percentags
qm <- apply(percount20, 2, quantile, probs = c(.05, .95))

print(mvp20)



# Assuming qm is a matrix or data frame
p <- ncol(qm)
rgy <- range(qm, na.rm = TRUE)

# Create the plot
plot(c(1, p), rgy,
  type = "n", xlab = "variable",
  ylab = "post mean, percent var use", axes = FALSE
)

# Add x-axis
axis(1, at = 1:p, labels = colnames(qm), cex.lab = 0.7, cex.axis = 0.7)

# Add y-axis
axis(2, cex.lab = 1.2, cex.axis = 1.2)

# Add lines for mvp20 if it exists and has the correct length
if (exists("mvp20") && length(mvp20) == p) {
  lines(1:p, mvp20, col = "black", lty = 4, pch = 4, type = "b", lwd = 1.5)
}

# Add vertical lines
for (i in 1:p) {
  lines(c(i, i), qm[, i], col = "blue", lty = 3, lwd = 1.0)
}






percount <- bf$varcount / apply(bf$varcount, 1, sum)
mvp <- apply(percount, 2, mean)
plot(mvp20, xlab = "variable number", ylab = "post mean, percent var use", col = "blue", type = "b")
lines(mvp, type = "b", col = "red")
legend("topleft", legend = c("BART", "BART20"), col = c("red", "blue"), lty = c(1, 1))


```

## N5 Other approach using BART

```{r}

# Using the BART package

burn <- 1000
nd <- 1000

fix <- c(1:28, 30)

y <- train_data$G3
x <- train_data[, fix]
p <- ncol(x)

bf <- wbart(x, y, nskip = burn, ndpost = nd, printevery = 500)



# Prepare the data
X <- as.matrix(sd[, -which(names(sd) == "G3")]) # predictors
y <- sd$G3 # target variable

# Multivariate Linear Regression with Regularization (Elastic Net)
# We'll use cross-validation to find the best alpha and lambda values
cv_model <- cv.glmnet(X[split_index, ], y[split_index], alpha = 0.5, nfolds = 5)
best_lambda <- cv_model$lambda.min

# Fit the final model
lm_model <- glmnet(X[split_index, ], y[split_index], alpha = 0.5, lambda = best_lambda)

# BART model
burn <- 1000
nd <- 1000
bart_model <- wbart(X[split_index, ], y[split_index], nskip = burn, ndpost = nd, printevery = 500)


summary(lm_model)

# Make predictions on the test set
lm_pred <- predict(lm_model, newx = X[-split_index, ], s = best_lambda)
bart_pred <- predict(bart_model, newdata = X[-split_index, ])

# Calculate MSE for both models on the test set
lm_mse <- mean((y[-split_index] - lm_pred)^2)
bart_mse <- mean((y[-split_index] - bart_pred)^2)

# Calculate R-squared for both models on the test set
lm_rsq <- 1 - sum((y[-split_index] - lm_pred)^2) / sum((y[-split_index] - mean(y[-split_index]))^2)
bart_rsq <- 1 - sum((y[-split_index] - bart_pred)^2) / sum((y[-split_index] - mean(y[-split_index]))^2)

# Print results
cat("Linear Regression (Elastic Net) Results:\n")
cat("MSE:", lm_mse, "\n")
cat("R-squared:", lm_rsq, "\n\n")

cat("BART Results:\n")
cat("MSE:", bart_mse, "\n")
cat("R-squared:", bart_rsq, "\n")

# Compare variable importance
lm_importance <- abs(coef(lm_model))[-1] # Exclude intercept
bart_importance <- bartModelMatrix(bart_model)$varcount.mean

# Print top 10 most important variables for each model
cat("\nTop 10 Important Variables (Linear Regression):\n")
print(sort(lm_importance, decreasing = TRUE)[1:10])

cat("\nTop 10 Important Variables (BART):\n")
print(sort(bart_importance, decreasing = TRUE)[1:10])

# Visualize BART results
plot(bart_model)




# Calculate MSE for training set
train_pred <- predict(bf, train_data)
train_mse <- mean((train_data$G3 - train_pred)^2)

# Calculate MSE for validation set
valid_pred <- predict(tree_model, valid_data)
valid_mse <- mean((valid_data$G3 - valid_pred)^2)

# Print results




lm_model <- lm(G3 ~ ., data = train_data)

# Summary
summary_lm <- summary(lm_model)


# Calculate MSE for training set
train_predL <- predict(lm_model, train_data)
train_mseL <- mean((train_data$G3 - train_predL)^2)


# Calculate MSE for validation set
valid_predL <- predict(lm_model, valid_data)
valid_mseL <- mean((valid_data$G3 - valid_predL)^2)

# Print results
comp <- function() {
  cat("Training MSE   Tree:", train_mse, "\n")
  cat("Validation MSE Tree:", valid_mse, "\n\n")
  cat("Training MSE   Regr:", train_mseL, "\n")
  cat("Validation MSE Regr:", valid_mseL, "\n\n")
}

comp()

```
