\documentclass[12pt]{article}

\usepackage[
  paper=a4paper,
  left=12.5mm,
  right=25mm,
  top=25mm,
  bottom=40mm,
  bindingoffset=10mm]{geometry}		% page and binding margins can be adjust here
  
\usepackage{apacite} 				% % literature-References: American Psycholog. Assoc.
\usepackage{natbib}					

\setcitestyle{round,aysep={}} 		% indexation in (round) parentheses, between the author year
%\usepackage[latin1]{inputenc} 		% mutated vowel (Umlaute) in the text
\usepackage[english]{babel}				% orthography
\usepackage[T1]{fontenc}
\usepackage{lmodern}				% font family
\usepackage{microtype}				% for micro typography (for a better typeface)
\usepackage{blindtext}
\usepackage{graphicx} 				% for including graphs (pdf,png - but do avoid jpg)
\graphicspath{{./Graphics/}}          % path to the pictures

\usepackage{url}					%  formatting URL (e.g. in the literature) 
\usepackage[colorlinks,linkcolor=black,citecolor=black,urlcolor=black]{hyperref} 				% For hperlinks in PDF-documents   
  
\usepackage{tabularx} 				% for a better configuration of tables
\usepackage{longtable} 		
\usepackage{multicol}				
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{xcolor}
\usepackage{varioref}
		
\usepackage[active]{srcltx}

\usepackage{listings}				% algorithm

\usepackage{mdwlist}				% lists

\usepackage{setspace} 				% setting of the lines (rows)
\newtheorem{mydef}{Merksatz}  		% if examples or mnemotechnic verses are used with continuous numeration 
\newtheorem{bsp}{Beispiel}

\usepackage{todonotes}				% for the creation of ToDos in the editor
\usepackage{lscape}					% for the rotation of pages
\usepackage{amsmath}				% for the writting of mathematical formula
\usepackage{calc}
\usepackage{footnote}				% footnotes
\usepackage{tablefootnote}			% footnotes in tables
\hyphenation{voll-st\"andigen}		% for defining word devisions globally

\setcounter{tocdepth}{2}			% levels which are displayed in the table of contents
%_______________________________________________________ document__________________________________________________

\begin{document}
\include{./Title/title} 			% title

\onehalfspacing                  	
% line spacing which is set to 1.5 from this point on
%\tableofcontents 					% table of contents

% -----------------------------------
\pagenumbering{arabic}% Arabic page numbers (and reset to 1)

%_______________________________________________________ Inhaltsverzeichnis _____________________________________________

\section{Outline of the Paper}
\begin{enumerate}
    \item Motivation
    \item Basics of Regression Trees
    \item Comparing regression Trees and Linear Regression
    \item Overfitting and Pruning
    \item Ensemble methods and BART
    \item Conclusion
    \item References
\end{enumerate}

%_______________________________________________________ Einleitung ______________________________________________

\section{Introduction}
In this paper, we explore the topic of regression trees, a powerful machine learning technique for predictive modeling. Linear regression often performs poorly on many kinds of data, especially those with non-linear relationships and interaction effects. Regression trees offer an alternative approach that can be useful in many situations. We will discuss their advantages over traditional linear regression methods, cover the basics of regression trees, compare them with linear regression, address the issue of overfitting, and introduce advanced ensemble methods like Bayesian Additive Regression Trees (BART).

%_______________________________________________________ warum dieses Thema, motivation

\subsection{Motivation}
Linear regression performs poorly on many kinds of data, particularly those with non-linear relationships and interaction effects. What might be a better approach for these situations? Regression Trees can be useful in many situations where linear regression falls short.

[Figure: Can you guess where regression trees and where linear regression will perform better?]



%_______________________________________________________ einordnung in die Literatur 




%_______________________________________________________ was habe ich herausgefunden 

%_______________________________________________________ inhaltsangabe, zusammenfassung 


%_______________________________________________________ kurze gliederung der arbeit 




%_______________________________________________________ hauptteil _______________________________________________


%_______________________________________________________ modellbeschreibung

%_______________________________________________________ methodik

%_______________________________________________________ theorie 




\section{Basics of Regression Trees}
Regression trees split the predictor space into regions that minimize the RSS given by:

\begin{equation}
    \sum_{j=1}^{J} \sum_{i \in R_j} ( y_i- \hat{ y}_{R_j} )^2
\end{equation}

In each region $\hat{y}$ simply takes on the mean of all observations in that region. Typically, we can't find the optimal regions. Instead, we use a greedy algorithm called Recursive binary splitting to find the optimal split to minimize prediction error at each stage.

\subsection{Main Differences Between Linear Regression and Trees}
\begin{itemize}
    \item Regression trees do not assume a linear relationship between predictors and the response.
    \item Trees capture interaction effects naturally.
\end{itemize}




\section{Basics of Regression Trees}
\subsection{Motivation}
Linear regression performs poorly on many kinds of data, especially those with non-linear relationships and interaction effects. Regression trees offer a better approach for these situations.

\subsection{How Regression Trees Work}
Regression trees split the predictor space into regions that minimize the Residual Sum of Squares (RSS) given by:

\begin{equation}
    \sum_{j=1}^{J} \sum_{i \in R_j} ( y_i- \hat{y}_{R_j} )^2
\end{equation}

In each region, $\hat{y}$ simply takes on the mean of all observations in that region. Typically, we can't find the optimal regions. Instead, we use a greedy algorithm called Recursive binary splitting to find the optimal split to minimize prediction error at each stage.

\subsection{Main Differences Between Linear Regression and Trees}
\begin{itemize}
    \item Regression trees do not assume a linear relationship between predictors and the response.
    \item Trees capture interaction effects naturally.
\end{itemize}


--- bis hier habe ich verglichen

\section{Problems with Regression Trees}
Two main problems can arise with regression trees: overfitting and non-optimal splitting.

[Figure: Two Problems that can arise with Trees]

\section{Pruning}
Cost complexity pruning counteracts overfitting by removing non-essential splits. We grow a large Tree and then use the following formula:

\begin{equation}
    \sum_{m=1}^{|T|} \sum_{i: x_i \in R_m} (y_i - \hat{y}_{R_j})^2 + \alpha|T|
\end{equation}

The process involves:
\begin{itemize}
    \item Selecting a parameter $\alpha$
    \item For each $\alpha$, finding the subtree that minimizes the cost
    \item Using cross-validation to select the best $\alpha$
\end{itemize}

The objective is to achieve a good tradeoff between bias and variance. A large $\alpha$ results in very small trees, while a small $\alpha$ results in larger trees.

\section{Ensemble Methods}
Even with pruning, trees often perform worse than other ML methods. Ensemble methods improve results by combining many regression trees. Each one contributes a small part to the overall prediction. These can be independent of previous trees (e.g., Random Forests) or grown on the residuals of the current fit (e.g., Bayesian Additive Regression Trees - BART).

\subsection{Bayesian Additive Regression Trees (BART)}
BART models the response as a sum of many tree-based models plus noise:

\begin{equation}
    Y_i = \sum_{j=1}^{m} g(X_i; T_j, M_j) + \epsilon_i
\end{equation}

BART calculates the residuals of the current sum of Trees, then modifies one Tree to decrease the residuals. It then takes the average over all but the burn-in iterations. Unlike single trees, BART avoids overfitting by averaging the predictions of many trees and provides a probabilistic prediction, giving a measure of uncertainty.

%_______________________________________________________ simulationen 

\section{Simulations}

\subsection{Linear Data}
We ran simulations 400 times with regression trees having 4 terminal nodes. We compared the Mean Squared Error (MSE) for both methods. The data was generated as: $Y = \beta_0 + \beta_1X + \epsilon$, where $\epsilon \sim N(0, \sigma^2)$.

Results:
\begin{itemize}
    \item MSE for OLS model: 0.9917 
    \item MSE for regression tree model: 1.4935 
\end{itemize}

\subsection{Non-linear Data}
We generated non-linear data using 4 normal distributions (NW & SE = red, NE & SW = blue). The results showed:

\begin{itemize}
    \item Classification Tree MSE: 0.3757
    \item Linear Classification MSE: 0.5103
\end{itemize}

Trees capture interaction effects better. For example, a large Y is only indicative of red if X is small.

\subsection{Finding the Optimal Tree Size}
In our simulation:
\begin{itemize}
    \item Original Tree MSE on Test set = 198.2861 
    \item Pruned Tree MSE on Test set = 173.861 
\end{itemize}

The sweet spot balances variance and bias, minimizing overfitting. We use a training set to build the model and a test set to evaluate performance.

%_______________________________________________________ schlussbemerkungen __________________________________________

\section{Conclusion}
Regression trees are powerful tools for handling non-linear and interactive effects, often outperforming linear regression in these scenarios. They are also very easy to interpret. However, trees require pruning to combat overfitting. Ensemble methods, by averaging independent trees or fitting trees on the residuals, can significantly improve results. BART, in particular, is a sophisticated method offering good results in many scenarios. While regression trees have their strengths, it's important to choose the right tool for each specific data analysis task.

%_______________________________________________________ References ______________________________________________

\section{References}
\begin{itemize}
    \item James, G., Witten, D., Hastie, T., Tibshirani, R. (2021). An Introduction to Statistical Learning with Applications in R (Second Edition). Springer.
    \item Tan, Y. V., Roy, J. (2019). Bayesian additive regression trees and the General BART model. Statistics in Medicine, Band/Volume 38(25), 5048-5069.
    \item Chipman, H. A., George, E. I., McCulloch, R. E. (2010). BART: Bayesian additive regression trees. The Annals of Applied Statistics, Band/Volume 4(1), 266-298.
    \item Townshend, R. Lecture 10 - Decision Trees and Ensemble Methods | Stanford CS229: Machine Learning (Autumn 2018). https://www.youtube.com/watch?v=wr9gUr-eWdA, accessed 21.05.24.
\end{itemize}

All images were made using R. Also thanks to Claude and ChatGPT for making \LaTeX\ a lot nicer to use.

\end{document}