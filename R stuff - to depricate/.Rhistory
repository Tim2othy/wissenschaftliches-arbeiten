plot(
nonlinear_data$x, nonlinear_data$y,
col = as.character(nonlinear_data$group),
pch = 19,                     # Solid circle
cex = 1,                    # 1.3 times default size
xlab = "X-axis",                # X-axis label
ylab = "Y-axis",                # Y-axis label
)
grid()
# Train classification tree
tree_model <- rpart(group ~ x + y, data = nonlinear_data, method = "class", control = rpart.control(mindepth = 8, maxdepth = 10))
# Train linear regression model
lm_model <- glm(group ~ x + y, data = nonlinear_data, family = binomial)
# Make predictions
tree_pred <- predict(tree_model, nonlinear_data, type = "class")
lm_pred <- ifelse(predict(lm_model, nonlinear_data, type = "response") > 0.5, "Blue", "Red")
# Create a grid for prediction
grid <- expand.grid(x = seq(min(nonlinear_data$x), max(nonlinear_data$x), length.out = 200),
y = seq(min(nonlinear_data$y), max(nonlinear_data$y), length.out = 200))
# Make predictions on the grid
tree_grid_pred <- predict(tree_model, grid, type = "class")
lm_grid_pred <- ifelse(predict(lm_model, grid, type = "response") > 0.5, "Blue", "Red")
# Define colors
bg_red <- "pink"
bg_blue <- "lightblue"
point_red <- "red"
point_blue <- "blue"
# Plot
par(mfrow = c(1, 2))
# Tree model plot
plot(grid$x, grid$y, col = ifelse(tree_grid_pred == "Red", bg_red, bg_blue),
pch = 20, cex = 0.5, main = "Classification Tree", xlab = "X", ylab = "Y")
points(nonlinear_data$x, nonlinear_data$y,
col = ifelse(nonlinear_data$group == "Red", point_red, point_blue),
pch = 19)
# Add decision boundary for tree
contour(seq(min(nonlinear_data$x), max(nonlinear_data$x), length.out = 200),
seq(min(nonlinear_data$y), max(nonlinear_data$y), length.out = 200),
matrix(as.numeric(tree_grid_pred), 200, 200),
levels = 1.5, add = TRUE, drawlabels = FALSE, lwd = 2)
# Linear regression model plot
plot(grid$x, grid$y, col = ifelse(lm_grid_pred == "Red", bg_red, bg_blue),
pch = 20, cex = 0.5, main = "Linear Classification", xlab = "X", ylab = "Y")
# Linear regression model plot
plot(grid$x, grid$y, col = ifelse(lm_grid_pred == "Red", bg_red, bg_blue),
pch = 20, cex = 0.5, main = "Linear Classification", xlab = "X", ylab = "Y")
points(nonlinear_data$x, nonlinear_data$y,
col = ifelse(nonlinear_data$group == "Red", point_red, point_blue),
pch = 19)
plot(tree_model)
text(tree_model)
# Calculate MSE
tree_mse <- mean((actual - tree_pred_num)^2)
lm_mse <- mean((actual - lm_pred_num)^2)
# Calculate classification error rate
tree_error_rate <- mean(tree_pred != nonlinear_data$group)
lm_error_rate <- mean(lm_pred != nonlinear_data$group)
print(paste("Classification Tree Error Rate:", tree_error_rate))
print(paste("Linear Regression Error Rate:", lm_error_rate))
# Calculate classification error rate
tree_error_rate <- mean(tree_pred != nonlinear_data$group)
lm_error_rate <- mean(lm_pred != nonlinear_data$group)
print(paste("Classification Tree Error Rate:", tree_error_rate))
print(paste("Linear Regression Error Rate:", lm_error_rate))
n <- 60
x1 <- rnorm(n, mean = -2.9, sd = 1.1)
y1 <- rnorm(n, mean = 2.6, sd = 1.5)
x2 <- rnorm(n, mean = 2.6, sd = 1.2)
y2 <- rnorm(n, mean = -2.7, sd = 1.2)
x3 <- rnorm(n, mean = 3, sd = 1.6)
y3 <- rnorm(n, mean = 2.5, sd = 2)
x4 <- rnorm(n, mean = -2.5, sd = 1.3)
y4 <- rnorm(n, mean = -2.7, sd = 1.2)
# Combine into a data frame
nonlinear_data <- data.frame(
x = c(x1, x2, x3, x4)+6,
y = c(y1, y2, y3, y4)+6,
group = factor(rep(c("Red", "Blue"), each = 2 * n))
)
plot(
nonlinear_data$x, nonlinear_data$y,
col = as.character(nonlinear_data$group),
pch = 19,                     # Solid circle
cex = 1,                    # 1.3 times default size
xlab = "X-axis",                # X-axis label
ylab = "Y-axis",                # Y-axis label
)
grid()
# Train classification tree
tree_model <- rpart(group ~ x + y, data = nonlinear_data, method = "class", control = rpart.control(mindepth = 8, maxdepth = 10))
# Train linear regression model
lm_model <- glm(group ~ x + y, data = nonlinear_data, family = binomial)
# Make predictions
tree_pred <- predict(tree_model, nonlinear_data, type = "class")
lm_pred <- ifelse(predict(lm_model, nonlinear_data, type = "response") > 0.5, "Blue", "Red")
# Create a grid for prediction
grid <- expand.grid(x = seq(min(nonlinear_data$x), max(nonlinear_data$x), length.out = 200),
y = seq(min(nonlinear_data$y), max(nonlinear_data$y), length.out = 200))
# Make predictions on the grid
tree_grid_pred <- predict(tree_model, grid, type = "class")
lm_grid_pred <- ifelse(predict(lm_model, grid, type = "response") > 0.5, "Blue", "Red")
# Calculate classification error rate
tree_error_rate <- mean(tree_pred != nonlinear_data$group)
lm_error_rate <- mean(lm_pred != nonlinear_data$group)
print(paste("Classification Tree Error Rate:", tree_error_rate))
print(paste("Linear Regression Error Rate:", lm_error_rate))
# Tree model plot
plot(grid$x, grid$y, col = ifelse(tree_grid_pred == "Red", bg_red, bg_blue),
pch = 20, cex = 0.5, main = "Classification Tree", xlab = "X", ylab = "Y")
# Define colors
bg_red <- "pink"
bg_blue <- "lightblue"
point_red <- "red"
point_blue <- "blue"
# Tree model plot
plot(grid$x, grid$y, col = ifelse(tree_grid_pred == "Red", bg_red, bg_blue),
pch = 20, cex = 0.5, main = "Classification Tree", xlab = "X", ylab = "Y")
points(nonlinear_data$x, nonlinear_data$y,
col = ifelse(nonlinear_data$group == "Red", point_red, point_blue),
pch = 19)
# Train classification tree
tree_model <- rpart(group ~ x + y, data = nonlinear_data, method = "class", control = rpart.control(mindepth = 2, maxdepth = 2))
# Train linear regression model
lm_model <- glm(group ~ x + y, data = nonlinear_data, family = binomial)
# Make predictions
tree_pred <- predict(tree_model, nonlinear_data, type = "class")
lm_pred <- ifelse(predict(lm_model, nonlinear_data, type = "response") > 0.5, "Blue", "Red")
# Calculate classification error rate
tree_error_rate <- mean(tree_pred != nonlinear_data$group)
lm_error_rate <- mean(lm_pred != nonlinear_data$group)
print(paste("Classification Tree Error Rate:", tree_error_rate))
print(paste("Linear Regression Error Rate:", lm_error_rate))
# Tree model plot
plot(grid$x, grid$y, col = ifelse(tree_grid_pred == "Red", bg_red, bg_blue),
pch = 20, cex = 0.5, main = "Classification Tree", xlab = "X", ylab = "Y")
# Train classification tree
tree_model <- rpart(group ~ x + y, data = nonlinear_data, method = "class", control = rpart.control(mindepth = 2, maxdepth = 2))
# Train linear regression model
lm_model <- glm(group ~ x + y, data = nonlinear_data, family = binomial)
# Make predictions
tree_pred <- predict(tree_model, nonlinear_data, type = "class")
lm_pred <- ifelse(predict(lm_model, nonlinear_data, type = "response") > 0.5, "Blue", "Red")
# Create a grid for prediction
grid <- expand.grid(x = seq(min(nonlinear_data$x), max(nonlinear_data$x), length.out = 200),
y = seq(min(nonlinear_data$y), max(nonlinear_data$y), length.out = 200))
# Make predictions on the grid
tree_grid_pred <- predict(tree_model, grid, type = "class")
lm_grid_pred <- ifelse(predict(lm_model, grid, type = "response") > 0.5, "Blue", "Red")
# Define colors
bg_red <- "pink"
bg_blue <- "lightblue"
point_red <- "red"
point_blue <- "blue"
# Plot
par(mfrow = c(1, 2))
# Tree model plot
plot(grid$x, grid$y, col = ifelse(tree_grid_pred == "Red", bg_red, bg_blue),
pch = 20, cex = 0.5, main = "Classification Tree", xlab = "X", ylab = "Y")
points(nonlinear_data$x, nonlinear_data$y,
col = ifelse(nonlinear_data$group == "Red", point_red, point_blue),
pch = 19)
library(rpart)
library(caret)
n <- 60
x1 <- rnorm(n, mean = -2.9, sd = 1.1)
y1 <- rnorm(n, mean = 2.6, sd = 1.5)
x2 <- rnorm(n, mean = 2.6, sd = 1.2)
y2 <- rnorm(n, mean = -2.7, sd = 1.2)
x3 <- rnorm(n, mean = 3, sd = 1.6)
y3 <- rnorm(n, mean = 2.5, sd = 2)
x4 <- rnorm(n, mean = -2.5, sd = 1.3)
y4 <- rnorm(n, mean = -2.7, sd = 1.2)
# Combine into a data frame
nonlinear_data <- data.frame(
x = c(x1, x2, x3, x4)+6,
y = c(y1, y2, y3, y4)+6,
group = factor(rep(c("Red", "Blue"), each = 2 * n))
)
plot(
nonlinear_data$x, nonlinear_data$y,
col = as.character(nonlinear_data$group),
pch = 19,                     # Solid circle
cex = 1,                    # 1.3 times default size
xlab = "X-axis",                # X-axis label
ylab = "Y-axis",                # Y-axis label
)
library(rpart)
library(caret)
n <- 60
x1 <- rnorm(n, mean = -2.9, sd = 1.1)
y1 <- rnorm(n, mean = 2.6, sd = 1.5)
x2 <- rnorm(n, mean = 2.6, sd = 1.2)
y2 <- rnorm(n, mean = -2.7, sd = 1.2)
x3 <- rnorm(n, mean = 3, sd = 1.6)
y3 <- rnorm(n, mean = 2.5, sd = 2)
x4 <- rnorm(n, mean = -2.5, sd = 1.3)
y4 <- rnorm(n, mean = -2.7, sd = 1.2)
# Combine into a data frame
nonlinear_data <- data.frame(
x = c(x1, x2, x3, x4)+6,
y = c(y1, y2, y3, y4)+6,
group = factor(rep(c("Red", "Blue"), each = 2 * n))
)
# Train classification tree
tree_model <- rpart(group ~ x + y, data = nonlinear_data, method = "class", control = rpart.control(mindepth = 2, maxdepth = 2))
# Train linear regression model
lm_model <- glm(group ~ x + y, data = nonlinear_data, family = binomial)
# Make predictions
tree_pred <- predict(tree_model, nonlinear_data, type = "class")
lm_pred <- ifelse(predict(lm_model, nonlinear_data, type = "response") > 0.5, "Blue", "Red")
# Calculate classification error rate
tree_error_rate <- mean(tree_pred != nonlinear_data$group)
lm_error_rate <- mean(lm_pred != nonlinear_data$group)
print(paste("Classification Tree Error Rate:", tree_error_rate))
print(paste("Linear Regression Error Rate:", lm_error_rate))
library(rpart)
library(caret)
run_model_comparison <- function(n_runs = 400, n_points = 60) {
tree_errors <- numeric(n_runs)
lm_errors <- numeric(n_runs)
for (i in 1:n_runs) {
# Generate non-linear data
x1 <- rnorm(n_points, mean = -2.9, sd = 1.1)
y1 <- rnorm(n_points, mean = 2.6, sd = 1.5)
x2 <- rnorm(n_points, mean = 2.6, sd = 1.2)
y2 <- rnorm(n_points, mean = -2.7, sd = 1.2)
x3 <- rnorm(n_points, mean = 3, sd = 1.6)
y3 <- rnorm(n_points, mean = 2.5, sd = 2)
x4 <- rnorm(n_points, mean = -2.5, sd = 1.3)
y4 <- rnorm(n_points, mean = -2.7, sd = 1.2)
nonlinear_data <- data.frame(
x = c(x1, x2, x3, x4) + 6,
y = c(y1, y2, y3, y4) + 6,
group = factor(rep(c("Red", "Blue"), each = 2 * n_points))
)
# Train classification tree
tree_model <- rpart(group ~ x + y, data = nonlinear_data, method = "class",
control = rpart.control(mindepth = 2, maxdepth = 2))
# Train linear regression model
lm_model <- glm(group ~ x + y, data = nonlinear_data, family = binomial)
# Make predictions
tree_pred <- predict(tree_model, nonlinear_data, type = "class")
lm_pred <- ifelse(predict(lm_model, nonlinear_data, type = "response") > 0.5, "Blue", "Red")
# Calculate classification error rate
tree_errors[i] <- mean(tree_pred != nonlinear_data$group)
lm_errors[i] <- mean(lm_pred != nonlinear_data$group)
}
# Calculate average error rates
avg_tree_error <- mean(tree_errors)
avg_lm_error <- mean(lm_errors)
# Return results
list(
avg_tree_error = avg_tree_error,
avg_lm_error = avg_lm_error,
tree_errors = tree_errors,
lm_errors = lm_errors
)
}
View(run_model_comparison)
library(rpart)
library(caret)
run_model_comparison <- function(n_runs = 400, n_points = 60) {
tree_errors <- numeric(n_runs)
lm_errors <- numeric(n_runs)
for (i in 1:n_runs) {
# Generate non-linear data
x1 <- rnorm(n_points, mean = -2.9, sd = 1.1)
y1 <- rnorm(n_points, mean = 2.6, sd = 1.5)
x2 <- rnorm(n_points, mean = 2.6, sd = 1.2)
y2 <- rnorm(n_points, mean = -2.7, sd = 1.2)
x3 <- rnorm(n_points, mean = 3, sd = 1.6)
y3 <- rnorm(n_points, mean = 2.5, sd = 2)
x4 <- rnorm(n_points, mean = -2.5, sd = 1.3)
y4 <- rnorm(n_points, mean = -2.7, sd = 1.2)
nonlinear_data <- data.frame(
x = c(x1, x2, x3, x4) + 6,
y = c(y1, y2, y3, y4) + 6,
group = factor(rep(c("Red", "Blue"), each = 2 * n_points))
)
# Train classification tree
tree_model <- rpart(group ~ x + y, data = nonlinear_data, method = "class",
control = rpart.control(mindepth = 2, maxdepth = 2))
# Train linear regression model
lm_model <- glm(group ~ x + y, data = nonlinear_data, family = binomial)
# Make predictions
tree_pred <- predict(tree_model, nonlinear_data, type = "class")
lm_pred <- ifelse(predict(lm_model, nonlinear_data, type = "response") > 0.5, "Blue", "Red")
# Calculate classification error rate
tree_errors[i] <- mean(tree_pred != nonlinear_data$group)
lm_errors[i] <- mean(lm_pred != nonlinear_data$group)
}
# Calculate average error rates
avg_tree_error <- mean(tree_errors)
avg_lm_error <- mean(lm_errors)
# Return results
list(
avg_tree_error = avg_tree_error,
avg_lm_error = avg_lm_error,
tree_errors = tree_errors,
lm_errors = lm_errors
)
}
# Run the function
results <- run_model_comparison(n_runs = 400, n_points = 60)
# Print average error rates
print(paste("Average Classification Tree Error Rate:", results$avg_tree_error))
print(paste("Average Linear Regression Error Rate:", results$avg_lm_error))
library(rpart)
library(caret)
run_model_comparison <- function(n_runs = 400, n_points = 60) {
tree_errors <- numeric(n_runs)
lm_errors <- numeric(n_runs)
for (i in 1:n_runs) {
# Generate non-linear data
x1 <- rnorm(n_points, mean = -2.9, sd = 1.1)
y1 <- rnorm(n_points, mean = 2.6, sd = 1.5)
x2 <- rnorm(n_points, mean = 2.6, sd = 1.2)
y2 <- rnorm(n_points, mean = -2.7, sd = 1.2)
x3 <- rnorm(n_points, mean = 3, sd = 1.6)
y3 <- rnorm(n_points, mean = 2.5, sd = 2)
x4 <- rnorm(n_points, mean = -2.5, sd = 1.3)
y4 <- rnorm(n_points, mean = -2.7, sd = 1.2)
nonlinear_data <- data.frame(
x = c(x1, x2, x3, x4) + 6,
y = c(y1, y2, y3, y4) + 6,
group = factor(rep(c("Red", "Blue"), each = 2 * n_points))
)
# Train classification tree
tree_model <- rpart(group ~ x + y, data = nonlinear_data, method = "class",
control = rpart.control(mindepth = 2, maxdepth = 2))
# Train linear regression model
lm_model <- glm(group ~ x + y, data = nonlinear_data, family = binomial)
# Make predictions
tree_pred <- predict(tree_model, nonlinear_data, type = "class")
lm_pred <- ifelse(predict(lm_model, nonlinear_data, type = "response") > 0.5, "Blue", "Red")
# Calculate classification error rate
tree_errors[i] <- mean(tree_pred != nonlinear_data$group)
lm_errors[i] <- mean(lm_pred != nonlinear_data$group)
}
# Calculate average error rates
avg_tree_error <- mean(tree_errors)
avg_lm_error <- mean(lm_errors)
# Return results
list(
avg_tree_error = avg_tree_error,
avg_lm_error = avg_lm_error,
tree_errors = tree_errors,
lm_errors = lm_errors
)
}
# Run the function
results <- run_model_comparison(n_runs = 400, n_points = 60)
# Print average error rates
print(paste("Average Classification Tree Error Rate:", results$avg_tree_error))
print(paste("Average Linear Regression Error Rate:", results$avg_lm_error))
# Load necessary libraries
library(rpart)
library(rpart.plot)
library(rpart.plot)
# Step 1: Generate a polynomial function with some noise
n <- 200
x <- runif(n, -10, 35)-rnorm(n,0,3)
y <- (-1-rnorm(1,0,0.1))*x - (0.3-rnorm(1,0,0.002))*x^2 + (0.01-rnorm(1,0,0.0004))*x^3 + rnorm(n, sd=7)
data <- data.frame(x = x, y = y)
plot(data)
# Step 2: Train a regression tree with more complexity
tree_model <- rpart(y ~ x, data = data, control = rpart.control(cp = 0, minsplit = 2))
# Plot the initial complex tree
rpart.plot(tree_model, main = "Initial Complex Regression Tree")
# Calculate initial MSE
initial_pred <- predict(tree_model, data)
initial_mse <- mean((data$y - initial_pred)^2)
cat("Initial MSE: ", initial_mse, "\n")
plot(
data,
col = rgb(0.2, 0.7, 1,1),  # Light blue color with 60% opacity
pch = 19,                     # Solid circle
cex = 0.7,                    # 1.3 times default size
xlab = "X-axis",                # X-axis label
ylab = "Y-axis",                # Y-axis label
)
grid()
# Overlay the regression tree predictions
# To overlay the regression tree, we need to plot its predictions as segments
# We can do this by plotting a step function
ord <- order(x)  # order x to plot the step function correctly
lines(x[ord], initial_pred[ord], col="red", lwd=2, type="s")
# Step 3: Prune the tree
# Use cross-validation to find the optimal complexity parameter (cp)
printcp(tree_model)  # Display the CP table
?printcp
# Find the optimal cp value that minimizes the cross-validated error
opt_index <- which.min(tree_model$cptable[,"xerror"])
opt_cp <- tree_model$cptable[opt_index, "CP"]
# Prune the tree using the optimal cp value
pruned_tree <- prune(tree_model, cp = opt_cp)
# Plot the pruned tree
rpart.plot(pruned_tree, main = "Pruned Regression Tree")
# Calculate pruned MSE
pruned_pred <- predict(pruned_tree, data)
pruned_mse <- mean((data$y - pruned_pred)^2)
cat("Pruned MSE: ", pruned_mse, "\n")
# Step 4: Plot how the pruning changes the MSE with respect to tree size
# Extract CP table for plotting
cp_table <- tree_model$cptable
# Calculate the number of terminal nodes for each cp value
num_nodes <- cp_table[, "nsplit"] + 1
plot(
x, y,
col = rgb(0.2, 0.7, 1,1),  # Light blue color with 60% opacity
pch = 19,                     # Solid circle
cex = 1,                    # 1.3 times default size
xlab = "X-axis",                # X-axis label
ylab = "Y-axis",                # Y-axis label
)
grid()
# Plot the MSE (xerror) against the number of terminal nodes
plot(num_nodes, cp_table[, "xerror"], pch=20, col = rgb(0.2, 0.7, 1, 1), type = "b", cex = 1,
xlab = "Tree Size", ylab = "Error",
ylim = c(0, max(cp_table[, "xerror"], cp_table[, "xstd"]) * 1.1),  # Extend y-axis to -1
)
# Add training error (MSE) to the plot
points(num_nodes, cp_table[, "xstd"], pch=20, col = "red", type = "b", cex = 1)
# Add a legend
legend("topright", legend = c("Cross-Validation", "Training"),
col = c(rgb(0.2, 0.7, 1, 1), "red"), pch = 20, lty = 1)
# Add grid lines
grid()
# Add a vertical line and text to indicate the optimal number of terminal nodes
abline(v = num_nodes[opt_index], col = "purple", lty = 2)
text(num_nodes[opt_index], min(cp_table[, "xerror"]),
labels = paste("Optimal Size =", num_nodes[opt_index]), pos = 4, col = "black")
# Step 5: Summarize the MSE before and after pruning
cat("Summary:\n")
cat("Initial MSE: ", initial_mse, "\n")
cat("Pruned MSE: ", pruned_mse, "\n")
n <- 200
x2 <- runif(n, -10, 35)-rnorm(n,0,3)
y2 <- (-1-rnorm(1,0,0.1))*x2 - (0.3-rnorm(1,0,0.002))*x2^2 + (0.01-rnorm(1,0,0.0004))*x2^3 + rnorm(n, sd=7)
data2 <- data.frame(x = x2, y = y2)
plot(data2)
initial_pred2 <- predict(tree_model, data2)
initial_mse2 <- mean((data2$y - initial_pred2)^2)
print(initial_mse2)
pruned_pred2 <- predict(pruned_tree, data2)
pruned_mse2 <- mean((data2$y - pruned_pred2)^2)
print(pruned_mse2)
library(rpart)
library(rpart.plot)
run_experiment <- function() {
# Generate data
n <- 200
x <- runif(n, -10, 35) - rnorm(n, 0, 3)
y <- (-1-rnorm(1,0,0.1))*x - (0.3-rnorm(1,0,0.002))*x^2 + (0.01-rnorm(1,0,0.0004))*x^3 + rnorm(n, sd=7)
train_data <- data.frame(x = x, y = y)
# Train initial tree
tree_model <- rpart(y ~ x, data = train_data, control = rpart.control(cp = 0, minsplit = 2))
# Prune the tree
opt_index <- which.min(tree_model$cptable[,"xerror"])
opt_cp <- tree_model$cptable[opt_index, "CP"]
pruned_tree <- prune(tree_model, cp = opt_cp)
# Generate test data
x2 <- runif(n, -10, 35) - rnorm(n, 0, 3)
y2 <- (-1-rnorm(1,0,0.1))*x2 - (0.3-rnorm(1,0,0.002))*x2^2 + (0.01-rnorm(1,0,0.0004))*x2^3 + rnorm(n, sd=7)
test_data <- data.frame(x = x2, y = y2)
# Calculate MSE for initial and pruned models
initial_pred <- predict(tree_model, test_data)
initial_mse <- mean((test_data$y - initial_pred)^2)
pruned_pred <- predict(pruned_tree, test_data)
pruned_mse <- mean((test_data$y - pruned_pred)^2)
return(c(initial_mse, pruned_mse))
}
# Run the experiment 400 times
num_runs <- 400
results <- replicate(num_runs, run_experiment())
# Calculate average MSE for both models
avg_initial_mse <- mean(results[1,])
avg_pruned_mse <- mean(results[2,])
# Print results
cat("Average Initial MSE:", avg_initial_mse, "\n")
cat("Average Pruned MSE:", avg_pruned_mse, "\n")
# Load necessary libraries
library(rpart)
library(rpart.plot)
# Step 1: Generate a polynomial function with some noise
n <- 200
x <- runif(n, -10, 35)-rnorm(n,0,3)
y <- (-1-rnorm(1,0,0.1))*x - (0.3-rnorm(1,0,0.002))*x^2 + (0.01-rnorm(1,0,0.0004))*x^3 + rnorm(n, sd=7)
data <- data.frame(x = x, y = y)
plot(data)
# Step 2: Train a regression tree with more complexity
tree_model <- rpart(y ~ x, data = data, control = rpart.control(cp = 0, minsplit = 2))
# Plot the initial complex tree
rpart.plot(tree_model, main = "Initial Complex Regression Tree")
# Calculate initial MSE
initial_pred <- predict(tree_model, data)
initial_mse <- mean((data$y - initial_pred)^2)
cat("Initial MSE: ", initial_mse, "\n")
plot(
data,
col = rgb(0.2, 0.7, 1,1),  # Light blue color with 60% opacity
pch = 19,                     # Solid circle
cex = 0.7,                    # 1.3 times default size
xlab = "X-axis",                # X-axis label
ylab = "Y-axis",                # Y-axis label
)
grid()
# Overlay the regression tree predictions
# To overlay the regression tree, we need to plot its predictions as segments
# We can do this by plotting a step function
ord <- order(x)  # order x to plot the step function correctly
lines(x[ord], initial_pred[ord], col="red", lwd=2, type="s")
