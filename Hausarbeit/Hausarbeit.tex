\documentclass[12pt]{article}
\usepackage[left=3cm, right=2cm, top=2cm, bottom=2cm]{geometry}

\usepackage[english]{babel}				% orthography
\usepackage[T1]{fontenc}
\usepackage{times}				% font family
\usepackage{microtype}				% for micro typography (for a better typeface)
\usepackage{setspace}
\setstretch{1.5} % 1.5 line spacing
\newtheorem{mydef}{Merksatz}  		% if examples or mnemotechnic verses are used with continuous numeration
\newtheorem{bsp}{Beispiel}
\usepackage{blindtext}
\usepackage{fancyhdr}
\usepackage{longtable}
\usepackage{multicol}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{xcolor}
\usepackage{varioref}
\usepackage[active]{srcltx}
\usepackage{listings}				% algorithm
\usepackage{mdwlist}				% lists
\usepackage{calc}
\usepackage{tablefootnote}			% footnotes in tables
\hyphenation{voll-st\"andigen}		% for defining word devisions globally

\usepackage[utf8]{inputenc}
\usepackage{amsmath}

\usepackage{wrapfig}
\usepackage{array}  % For better control over column formatting

\usepackage{graphicx}
\graphicspath{{./Graphics/}}          % path to the pictures

\usepackage[hidelinks]{hyperref}
\hypersetup{colorlinks=true,
   linkcolor=blue,
   filecolor=magenta,
   urlcolor=cyan,
   citecolor=blue,
   pdfborderstyle={/S/U/W 0}
}

\usepackage[round]{natbib}
\bibliographystyle{plainnat}

\begin{document}

\include{./Title/title}

\pagenumbering{arabic}% Arabic and reset to 1

\tableofcontents

%\newpage

\section{Introduction}

A large part of modern economic research is built upon linear regression. To conduct and understand economic research, it is particularly crucial to identify where it performs well and where it's limitations lie and other methods can improve upon it. Regression Trees are a powerful machine-learning technique, that can be useful in many situations where linear regression falls short. Particularly for data that includes non-linear relationships and interaction effects Linear regression will often perfor poorly. And Regression Trees can be a important alternative. Regression Trees also have numerous extensions that greatly improve their usefulness and aplicabillity that it is worthwhile to study.


The CART algorithm or variations on it, that most tree based methods use was first introduced in \citep{breiman1984}. Since then many extensions have been developed such as Random Forets, a very popular Tree based ML method. \citep{biau2016} serves as a good introduction to that method. \citep{hastie2021} serves as an excellent modern introduction to Regression trees and gives a gentile introduction to the topic and many extensions. \citep{tan2019} give a deeper dive on BART, one powerful ensemble method.


In this paper I will explain the theory behind regression trees and, compare them to linear regression using a series of simple simulations and on a dataset of student test performance. Simulations can allow for comparison under ideal circumstances, allowing one to focus on the issue one is interested in, while real world data can give a more realistic perspective on a methods actual performance. I will focus especially on the regression Trees ability to naturally capture interaction effects, and how pruning can combat overfitting. I will also highlight how ensemble methods, a kind of extension to regression trees, can enhance their performance substantially.


In section 2 I will give a brief overview of the theory of regression trees, and where and where not we might expect them to perform well. And will also explain the bias-variance trade-off and explain how pruning helps and ensemble methods that are extensions to trees.

In section 3 I will run a number of simulations comparing the performance of Linear Regression and Regression Trees under ideal conditions.

In section 4 I will apply these methods to a dataset of student test performance. Highlighting pruning and the BART ensemble method. Also showing some of the shortcomings of Regression Trees.

Section 5 is the conclusion and will sum up what I have found and further interesting areas where research could be done.

This dataset used in this paper is publicly available at \href{https://www.kaggle.com/datasets/uciml/student-alcohol-consumption}{Kaggle: Student Alcohol Consumption Dataset}. All code used for the simulations and data analysis is available at \href{https://github.com/Tim2othy/wissenschaftliches-arbeiten}{my GitHub}.


\section{Regression Trees}

Regression Trees are a type of supervised machine-learning algorithm that recursively splits the predictor space into smaller rectangular subregions. This process approximates an unknown function $f$ by minimizing a measure of loss at each split. Unlike linear regression, Regression Trees don't make assumptions about linearity or non-interaction between different dimensions, making them particularly useful for complex, non-linear relationships in data.

The core mechanism of Regression Trees involves splitting the predictor space into regions that minimize the residual sum of squares, given by:

\begin{equation}
    \sum_{j=1}^{J} \sum_{i \in R_j} ( y_i- \hat{y}_{R_j})^2
\end{equation}

In each region, $\hat{y}$ typically takes on the mean of all observations in that region. Each branching of the tree divides the predictor space into one extra reason, for numerical variables typically of the form $\{x \le c\}$ or $\{x > c\}$. This process continues until a specified threshold is reached, such as a minimum number of observations in each leaf node or a maximum tree depth. Figure~\ref{red_visual} shows one way to visualize the way a Regression Tree will carve up the predictor space and make predictions for each reagion.

The algorithm employs a greedy approach called Recursive binary splitting to find the optimal split at each stage to minimize prediction error.

\begin{figure}\centering
    \includegraphics[scale=0.40]{red_visual.png}
    \caption{Visualization of Regression Tree's predictions}
    \label{red_visual}
\end{figure}

Regression Trees offer several advantages over traditional linear regression methods.  Furthermore, they naturally account for interaction effects. For instance, if, in ones dataset, being tall increases income but only for men, a Regression Tree will naturally partition the data along gender and only along hight for the male half. Wheras for a linear regression to capture this interaction effect would require one to explecitly include this interaction on one of ones regressors. This ability to model complex interactions without manual specification is a significant strength of the method.

One major weakness of Regression Trees however is their tendency to overfit on the data. To capture interesting relationships one must allow ones Tree to grow deep with multple levels of splits. Their flexibility allowing them to create very specific rules also allows them to capture noise in the training data rather than true underlying patterns. Figure~\ref{overfitting_tree}, illustrates how a Regression Tree can overfit a dataset.

\begin{figure}
    \centering
    \includegraphics[scale=0.50]{image.png}
    \caption{Regression Tree overfitting}
    \label{overfitting_tree}
\end{figure}


\subsection{Pruning}

The main way to adress the overfitting problem is to, in keeping with the arborial theme, "prune" one's regression tree. In cost complexity pruning one starts by growing a large tree is grown that is bound to overfit the training data. Then, a complexity parameter $\alpha$ is introduced to penalize the tree's size. The pruning process is guided by the following formula:

\begin{equation}
    \sum_{m=1}^{|T|} \sum_{i: x_i \in R_m} (y_i - \hat{y}_{R_j})^2 + \alpha|T|
\end{equation}

Where $|T|$ is the number of terminal nodes in the tree. The tree shrinks in a predictable fashin as the cost complexity parameter $\alpha$, is increased and it becomes harder for each split to justify its existence. As one prunes a large tree by increasing $\alpha$, the tree naturally shrinks in a well-behaved fashion, allowing one to select the subtree with the lowest cross-validation error. Evaluating a model on different data then it was trained on allows us to see it's real prformance and select a subtree that achieves a good trade-off between bias and variance. In the \texttt{rpart} library cross-validation is performed automatically each time one grows a tree.


\subsection{Ensemble Methods}
While pruning can improve the performance of regression trees, they often still underperform compared to other machine-learning methods. This has lead to the development of a number of ensemble methods, which combine many regression trees to create more robust and accurate predictions. As the well know jury theorem shows, if predictors are (sufficiently) uncorrelated and a predictor's answer is better than pure chance adding more predictors will increase the quality of the average of all answers \citep{condorcet1785}. Ensemble methods leverage this "wisdom of crowds" effect.

Two ways this can be done is by Growing many independent trees and averaging them. Random Forests are a very popular ML method that does this. Or by iterativly growing trees on the residuals of the current model, improving the overall model over multiple generations, as in boosting or Bayesian Additive Regression Trees (BART).

Random Forests create an ensemble by growing many decorrelated trees. Each tree is trained on a random subset of the training data and is only allowed to consider a random subset of features at each split. The final prediction is then formed by averaging the predictions of all trees, typically around 500. This approach reduces overfitting while maintaining the ability to capture complex patterns in the data \citep{biau2016}.

I will go into slightly more detail for the BART method, the method I used on the dataset. BART model the data as a sum of many Trees plus noise:

\begin{equation}
    Y_i = \sum_{j=1}^{m} g(X_i; T_j, M_j) + \epsilon_i
\end{equation}

Where $m$ is the number of trees, typically around 200. And $g(X_i; T_j, M_j)$ represents the contribution of the $j$-th tree. $T_j$ and $M_j$ represent the tree's structure and predictions respectively.

BART emyloys a Bayesian approach, incorporating a prior that discourages trees from growing too large. Specifically, the probability that a node at depth $d$ is non-terminal is given by $\alpha(1 + d)^{-\beta}$. This prior on shallow trees, restricts the trees to be weak learners that only explain a small portion of the overall data and are better suited to being combined through summation and averaging. In doing this BART avoids overfitting and leverages the strengths of ensemble learning. Additionally, BART employs other priors to guide the estimation of terminal node parameters, though a detailed discussion of these is beyond the scope of this paper. This Bayesian approach enables the model to provide both accurate predictions and quantative measures of uncertainty.

Default values for these hyperparameters of $\alpha = 0.95$ and $\beta = 2$ are recommended by \citep{chipman2010}. And generally provide good performance.

The BART algorithm proceeds by first generating e.g., 200 trees, without any splits. For each Tree $j$, considering all the current trees except $j$, the residual error $R_{-j}$ is calculated:

\begin{equation}
    R_{-j} = Y - \sum_{t\not=j} g(X,T_j,M_j).
\end{equation}

Then a change to that tree's structure is proposed, such as pruning or growing a node, or changing a splitting rule, and is accepted or rejected based on its posterior probability. Meaning the split has to be in accordance with the model parameters as well as fit the data well enough. This process is repeated for all $m$ trees. That constitutes one iteration of the algorithm. Typically, BART uses 1000 burn-in iterations followed by 1000 sampling iterations. An estimate for $f(x)$ is obtained by taking the average over all sample iterations.

A more complete exposition can be found in \citep{tan2019} or \citep{chipman2010}.

Ensemble methods have demonstrated remarkable success across various applications, often surpassing individual regression trees and many other machine learning techniques.


\section{Simulations}

Simulations can serve as a testing ground for statistical methods, allowing for easier repeatability and eliminating many of the complications that arise when using datasets. Simulations are especially useful in comparing two different methods. In this section I will compare Linear regression and regression trees on linear and non-linear simulated data, each simulation having been run 400 times to average out noise. The regression Trees were limited to four terminal nodes for both simulations.

\begin{figure}
    \centering
    \includegraphics[scale=0.25]{OLS vs Tree.png}
    \caption{Linear Relation between Variables}
    \label{OLS_VS_TREE}
\end{figure}

For the first simulation the data follow a simple linear relationship with $Y = \beta_0 + \beta_1X + \epsilon$, with $\epsilon$ normally distributed. For this simulation the mean squared error (MSE) for the Linear regression is \texttt{0.992} and \texttt{1.494} for the Tree. Figure~\ref{OLS_VS_TREE} shows quite clearly that this is the kind of data where it will be hard to beat linear regression.

The linear regression also wins out on intuitiveness. The derivative of the line of best fit can be interpreted as the expected increase in variable $y$ as the $x$ variable increases. While the Tree's step function lacks this.

In this second simulation, the data have a non-linear relationship. The data was generated using 4 normal distributions with varying variance, in the four corners of a two dimensional space. With the datapoints of the North-West and South-East distributions coloured in red and the North-Eastern and South-Western points in blue.

\begin{figure}
    \centering
    \includegraphics[scale=0.30]{NLD Pred.png}
    \caption{Complex, overfitting Tree}
\end{figure}

Using classifiers instead of regressors for this simulation the error rates of the tree and linear classifier are \texttt{37.57\%} and \texttt{51.03\%} respectively. The interaction effect between the two dimensions is not captured by the linear model, while the regression trees naturally captures it.


\section{Predicting grades}

Next, I will apply Tree-based methods and also linear regression to a dataset to showcase differences in a more realistic situation. This dataset of student performance includes 649 students and looks at 33 variables.

\subsection{Pruning}

For all experiments on the dataset, I split the data into training (70\%) and validation (30\%) sets.

To prevent overfitting and ensure meaningful insights, it is crucial to use cross-validation and prune the regression tree. Without pruning, the model risks fitting to noise in the data, leading to misleading conclusions. While one will generally want to use the in built automatic pruning, manually pruning trees can provide valuable intuition about how regression trees tend to overfit. Instead of relying on the built-in 10-fold cross-validation to automatically select the optimal tree, I first experimented with manual pruning.

By growing a large tree with a complexity parameter of \( \text{cp} = 0.01 \), I obtained a tree with a training MSE of 4.521 and a validation MSE of 9.027, clearly indicating overfitting with the large tree. A smaller tree grown with \( \text{cp} = 0.025 \) resulted in a training MSE of 7.841 and a only slightly worse validation MSE of 8.651.

\begin{figure}
    \centering
    \includegraphics[scale=0.30]{small_manual_tree.pdf}
    \caption{Smaller, better Tree}
    \label{small_tree}
\end{figure}

While it may seem counterintuitive that this simpler tree (Figure~\ref{small_tree}) outperforms the larger one, the larger tree fits the noise rather than the underlying signal, which explains its poorer generalization.





To find the best possible tree, one should employ proper automatic pruning. For this experiment I grew and pruned a large tree on the training data.

\begin{figure}
    \centering
    \includegraphics[scale=0.30]{triple_pruning_plot.pdf}
    \caption{Pruning finds the lowest Test error}
    \label{prune}
\end{figure}



One challenge in pruning is that, even when using separate training and validation sets, searching for the optimal complexity parameter \( \text{cp} \) to minimize test error can lead to a situation where a particular subtree coincidentally performs very well on the test set by chance. Figure~\ref{prune} shows how some subtrees can appear especially good for a particular value of \( \text{cp} \). However, selecting this tree might still result in overfitting. To mitigate this, it's important to use a seperate validation and test sets, not allowing oneself to experiment around with the test set until one finds one tree that happens to perform especially on it by chance.






Sometimes it's so extreme that the optimal pruned tree using cross-validation, once it's evaluated on the test set, gets an MSE on the training data of 5.272, but on the Test set MSE of 11.4923, clearly overfitting and just lucky during cross-validation.

Here one can see very well how the Cross-validation error helps against overfitting, it doesn't decrease monotonically, but still overfits to a large extent. The optimal tree using cross validation has 9 splits while the optimal one on the test error only has 1.


A different problem is that the results can vary quite widely. Different splits in training and test data might lead to MSEs on the test set for the pruned tree between 6.9 and 9.

An optimal value for texttt{cp} which in this case is 0.028 and 0.035.







The pruned tree performs just one single split on failures, splitting the 15\% of students who have previously failed the exam into one region (Estimate = 8.4) and the 85\% of students with 0 failures into a second region (13 = estimate). This seems like something a human could have also done by hand.

The fact that the test error is lower than the training error should not be surprising; with only 1 split, the tree couldn't overfit even if it tried. It will always make almost the exact same split anyway. It's just chance if it will get a lower or higher error on the test set.








In extreme cases, the pruned tree selected via cross-validation might overfit, achieving a training MSE of 5.272 but a much higher test MSE of 11.492, indicating that it was merely lucky during cross-validation.

This highlights how cross-validation helps combat overfitting. As seen, the cross-validation error doesn't decrease monotonically, and the model can still overfit substantially. In this case, the optimal tree using cross-validation had 9 splits, while the tree minimizing test error had only 1 split.

Another issue is the variability in results. Different splits of the training and test sets can lead to test set MSEs for the pruned tree ranging from 6.9 to 9. The optimal \( \text{cp} \) values in these cases varied between 0.028 and 0.035.

Typically, the results look like this:

\begin{wrapfigure}{r}{0.5\textwidth}
    \centering
    \begin{tabular}{| l | c | c |}
        \hline
        Model        & Training MSE & Valid or Test MSE \\
        \hline
        Complex Tree & 1.715        & 10.813            \\
        Pruned Tree  & 8.527        & 8.189             \\
        \hline
    \end{tabular}
    \caption{MSE values for different models}
\end{wrapfigure}



The pruned tree ends up making just one split based on the number of previous failures. It separates the 15\% of students who had failed the exam before (with an estimated score of 8.4) from the remaining 85\% who had no failures (with an estimated score of 13). This simple tree is something a human could likely deduce manually.

It is not surprising that the test error is lower than the training error; with only one split, the tree couldn’t overfit even if it tried. Since it always makes almost the same split, any difference in error between the training and test sets is due to random chance.













\subsection{Comparing Linear Regression, Regression Trees, and BART}

After examining the tree's performance, it's informative to compare it to the results obtained from linear regression. These are the typical results one gets for Regression Trees and multivariate linear regression. The linear regression is not optimized in any way to counter overfitting. Even without any pruning, linear regression outperforms the tree by a wide margin:

Invoking Condorcet to assist us, we can use ensemble methods. Using BART with the default hyperparameters, that is, a burn-in and sampling period of 2000 iterations each and 200 Trees per iteration, results in the following MSEs for BART:

\begin{wrapfigure}{r}{0.5\textwidth}
    \centering
    \begin{tabular}{| l | c | c |}
        \hline
        Model & Training MSE & Validation MSE \\
        \hline
        Tree  & 8.527        & 8.189          \\
        Regr  & 6.787        & 6.865          \\
        BART  & 5.777        & 6.523          \\
        \hline
    \end{tabular}
    \caption{MSE values for different models}
\end{wrapfigure}

Depending on how the data is split into training and test sets, and other random factors, the numbers for all three models can vary quite widely. However, the single Tree is almost always outperformed by linear regression, and BART outperforms them both.

In return, BART is far less easy to interpret than the other two methods. We did not attempt to improve the performance of the regression model. Presumably, its performance can be increased in some way. But there are, of course, also other datasets where complicated ensemble methods significantly outperform linear methods.


\section{Conclusion}

The simulations in this paper highlighted the extremes where Regression trees or linear regression are at their most powerful, and show how regression trees are adept at handling non-linear and interactive effects. Certainly there is enough room in the statistical world for both models to have their own nitche.

The section on real world data highlights the importance of avoiding overfitting, both by not using a seperate sets at all and by overfitting in harder to notice ways. And shows how Ensemble methods such as BART, while often losing most interpretability can achieve very impressive resuts.

The most obvious problem with simple regression trees is that they overfit immediately without doing anything useful. It seems unimaginable to be able to do only one split on a dataset this large, with that many variables. For example, if one has a dataset where students who drink more alcohol score slightly lower on a test, a linear regression might incorporate this subtle trend, possibly slightly improving performance on the test set. If this effect is mere noise, the coefficient will not be too large and will not drive up the test error too much. In contrast, at least on this dataset, regression trees seem unable not to hyper-focus on one area, that turns out to almost always just be noise, resulting in just one singly branching generally being optimal. While the liner regression doesn't overfit with over a dozen coefficients the tree manages to overfit with just two splits.

Although the data might just be especially suited for linear regression.

While BART often outperforms linear regression, it sacrifices interpretability, which is a key advantage of both linear models and regression trees, only slightly outperforming linear regression in predictive ability.

Regression trees certainly have their applications. As simulations demonstrate, for highly non-linear data, even a simple tree can outperform linear regression by a significant margin. However, in real-world datasets, linear regression frequently performs better and provides greater interpretability. Perhaps trees excel only when strong interaction effects exist, which might be less common in typical datasets.

Ultimately, the choice of model depends on the characteristics of the data. While one can always find datasets where trees outperform other models, determining the most appropriate method for a given task remains a critical area of research with many unanswered questions.

Because linear regression is used so much is such an important area of research as even a small increase in our statistical sophistication or knowlegde about when which model is superior will have large positive effects.

%\newpage

\bibliography{sources}

\end{document}