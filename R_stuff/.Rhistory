# Convert rpart object to data.tree
to_data.tree <- function(model) {
nodes <- data.frame(node = as.numeric(rownames(model$frame)),
var = model$frame$var,
n = model$frame$n,
dev = model$frame$dev,
yval = model$frame$yval)
edges <- data.frame(model$splits)
edges$parent <- match(as.numeric(rownames(edges)), nodes$node)
edges$child <- match(edges$index, nodes$node)
tree <- Node$new("Root")
add_node <- function(parent, child_id) {
child <- nodes[nodes$node == child_id, ]
child_node <- parent$AddChild(child$var)
child_node$node <- child$node
child_node$n <- child$n
child_node$dev <- child$dev
child_node$yval <- child$yval
children <- edges[edges$parent == child_id, ]
if (nrow(children) > 0) {
add_node(child_node, children$child[1])
add_node(child_node, children$child[2])
}
}
add_node(tree, 1)
return(tree)
}
tree_data <- to_data.tree(tree_model)
tree_network <- ToDataFrameNetwork(tree_data, "name", "pathString")
# Create the interactive plot
networkD3::diagonalNetwork(tree_network, fontSize = 10, nodeStroke = "black")
tree_data <- to_data.tree(tree_model)
tree_network <- ToDataFrameNetwork(tree_data, "name", "pathString")
# Create the interactive plot
networkD3::diagonalNetwork(tree_network, fontSize = 10, nodeStroke = "black")
library(rpart)
library(rpart.plot)
library(dplyr)
library(ggplot2)
library(data.tree)
library(networkD3)
sd <- student_por
sd$G2 <- NULL
sd$G1 <- NULL
View(sd)
plot(sd$age, sd$G3, col = "blue", pch = 19)
grid()
tree_model <- rpart(G3 ~ ., data = sd)
# Visualize the tree
rpart.plot(tree_model, shadow.col = "gray")
rpart.plot(tree_model, extra = 101, fallen.leaves = TRUE, type = 4, main = "Decision Tree Visualization")
# Get variable importance
var_importance <- tree_model$variable.importance
print(var_importance)
.C
dq+u~!!
# Variable Importance Plot
var_importance <- data.frame(
variable = names(tree_model$variable.importance),
importance = tree_model$variable.importance
)
# Variable Importance Plot
var_importance <- data.frame(
variable = names(tree_model$variable.importance),
importance = tree_model$variable.importance
)
ggplot(var_importance, aes(x = reorder(variable, importance), y = importance)) +
geom_bar(stat = "identity", fill = "skyblue") +
coord_flip() +
theme_minimal() +
labs(title = "Variable Importance", x = "Variables", y = "Importance")
# Now doing the normal regression
# Run linear regression
lm_model <- lm(G3 ~ ., data = sd)
# Summary
summary_lm <- summary(lm_model)
print(summary_lm)
# Now doing the normal regression
lm_model <- lm(G3 ~ ., data = sd)
# Summary
summary_lm <- summary(lm_model)
print(summary_lm)
# Extract coefficients and p-values
coef_summary <- data.frame(
Estimate = round(summary_lm$coefficients[, "Estimate"], 4),
P_value = round(summary_lm$coefficients[, "Pr(>|t|)"], 4)
)
print(coef_summary)
# Plot residuals
plot(lm_model, which = 1)  # Residuals vs Fitted
plot(lm_model, which = 2)  # Normal Q-Q plot
# Compare predictions: Linear Regression vs Regression Tree
sd$lm_pred <- predict(lm_model)
sd$tree_pred <- predict(tree_model)
# Calculate MSE for both models
mse_lm <- mean((sd$G3 - sd$lm_pred)^2)
mse_tree <- mean((sd$G3 - sd$tree_pred)^2)
cat("MSE (Linear Regression):", mse_lm, "\n")
cat("MSE (Regression Tree):", mse_tree, "\n")
# Visualize predictions
ggplot(sd, aes(x = G3)) +
geom_point(aes(y = lm_pred, color = "Linear Regression"), alpha = 0.5) +
geom_point(aes(y = tree_pred, color = "Regression Tree"), alpha = 0.5) +
geom_abline(intercept = 0, slope = 1, linetype = "dashed") +
theme_minimal() +
labs(title = "Predicted vs Actual G3 Scores",
x = "Actual G3", y = "Predicted G3",
color = "Model") +
scale_color_manual(values = c("Linear Regression" = "blue", "Regression Tree" = "red"))
# Variable importance comparison
lm_importance <- abs(coef(lm_model)[-1])  # Exclude intercept
importance_df <- data.frame(
Variable = names(lm_importance),
Linear_Regression = lm_importance,
Regression_Tree = var_importance$importance[match(names(lm_importance), var_importance$variable)]
)
importance_df <- importance_df[order(-importance_df$Linear_Regression), ]
# Plot variable importance comparison
importance_long <- tidyr::pivot_longer(importance_df, cols = c("Linear_Regression", "Regression_Tree"), names_to = "Model", values_to = "Importance")
ggplot(importance_long, aes(x = reorder(Variable, Importance), y = Importance, fill = Model)) +
geom_bar(stat = "identity", position = "dodge") +
coord_flip() +
theme_minimal() +
labs(title = "Variable Importance Comparison", x = "Variables", y = "Importance") +
scale_fill_manual(values = c("Linear_Regression" = "blue", "Regression_Tree" = "red"))
install.packages('plotly')
# Load necessary libraries
library(rpart)
library(rpart.plot)
library(plotly)
# Assuming 'sd' is your dataset
# Create a tree with just two splits
tree_model_2splits <- rpart(G3 ~ ., data = sd, control = rpart.control(maxdepth = 2))
# Print the tree structure
print(tree_model_2splits)
# Plot the tree
rpart.plot(tree_model_2splits, extra = 101, fallen.leaves = TRUE, type = 4, main = "Decision Tree with Two Splits")
# Assuming 'sd' is your dataset
# Create a tree with just two splits
tree_model_2splits <- rpart(G3 ~ ., data = sd, control = rpart.control(maxdepth = 1))
# Print the tree structure
print(tree_model_2splits)
# Plot the tree
rpart.plot(tree_model_2splits, extra = 101, fallen.leaves = TRUE, type = 4, main = "Decision Tree with Two Splits")
# Assuming 'sd' is your dataset
# Create a tree with just two splits
tree_model_2splits <- rpart(G3 ~ ., data = sd, control = rpart.control(maxdepth = 2))
# Print the tree structure
print(tree_model_2splits)
# Plot the tree
rpart.plot(tree_model_2splits, extra = 101, fallen.leaves = TRUE, type = 4, main = "Decision Tree with Two Splits")
# Get the variables used for splitting
split_vars <- tree_model_2splits$frame$var[tree_model_2splits$frame$var != "<leaf>"]
split_vars <- unique(as.character(split_vars))
# If there are fewer than 2 splits, notify the user
if(length(split_vars) < 2) {
cat("The tree made fewer than 2 splits. Adjust your data or tree parameters.")
} else {
# Create a 3D scatter plot with decision boundaries
plot_ly(sd, x = ~get(split_vars[1]), y = ~get(split_vars[2]), z = ~G3,
type = "scatter3d", mode = "markers",
marker = list(size = 3, color = ~G3, colorscale = "Viridis", opacity = 0.8)) %>%
add_markers() %>%
layout(scene = list(xaxis = list(title = split_vars[1]),
yaxis = list(title = split_vars[2]),
zaxis = list(title = "G3")))
# Function to add a plane to the plot
add_plane <- function(p, split_var, split_value, color) {
var_range <- range(sd[[split_var]])
other_var <- setdiff(split_vars, split_var)[1]
other_range <- range(sd[[other_var]])
if(split_var == split_vars[1]) {
x <- rep(split_value, 2)
y <- other_range
} else {
x <- var_range
y <- rep(split_value, 2)
}
z <- matrix(rep(range(sd$G3), each = 2), nrow = 2)
add_surface(p, x = x, y = y, z = z, opacity = 0.3, colorscale = list(c(0, 1), c(color, color)))
}
# Get split points
splits <- tree_model_2splits$splits
split_points <- splits[splits[,"count"] > 0, "index"]
# Create the plot with decision boundaries
p <- plot_ly(sd, x = ~get(split_vars[1]), y = ~get(split_vars[2]), z = ~G3,
type = "scatter3d", mode = "markers",
marker = list(size = 3, color = ~G3, colorscale = "Viridis", opacity = 0.8)) %>%
layout(scene = list(xaxis = list(title = split_vars[1]),
yaxis = list(title = split_vars[2]),
zaxis = list(title = "G3")))
# Add planes for each split
for(i in 1:length(split_points)) {
p <- add_plane(p, split_vars[i], split_points[i], color = c("red", "blue")[i])
}
# Display the plot
p
}
# Create the plot with decision boundaries
p <- plot_ly(sd, x = ~get(split_vars[1]), y = ~get(split_vars[2]), z = ~G3,
type = "scatter3d", mode = "markers",
marker = list(size = 3, color = ~G3, colorscale = "Viridis", opacity = 0.8)) %>%
layout(scene = list(xaxis = list(title = split_vars[1]),
yaxis = list(title = split_vars[2]),
zaxis = list(title = "G3")))
# Add planes for each split
for(i in 1:length(split_points)) {
p <- add_plane(p, split_vars[i], split_points[i], color = c("red", "blue")[i])
}
# Display the plot
p
# Data-analysis
install.packages('...')
library(rpart)
library(rpart.plot)
library(dplyr)
library(ggplot2)
library(data.tree)
library(networkD3)
sd <- student_por
sd$G2 <- NULL
sd$G1 <- NULL
View(sd)
plot(sd$age, sd$G3, col = "blue", pch = 19)
grid()
plot(sd$sex, sd$G3, col = "blue", pch = 19)
plot(sd$studytime, sd$G3, col = "blue", pch = 19)
tree_model <- rpart(G3 ~ ., data = sd)
# Visualize the tree
rpart.plot(tree_model, shadow.col = "gray")
rpart.plot(tree_model, extra = 101, fallen.leaves = TRUE, type = 4, main = "Decision Tree Visualization")
# Get variable importance
var_importance <- tree_model$variable.importance
print(var_importance)
# Variable Importance Plot
var_importance <- data.frame(
variable = names(tree_model$variable.importance),
importance = tree_model$variable.importance
)
ggplot(var_importance, aes(x = reorder(variable, importance), y = importance)) +
geom_bar(stat = "identity", fill = "skyblue") +
coord_flip() +
theme_minimal() +
labs(title = "Variable Importance", x = "Variables", y = "Importance")
# Now doing the normal regression
lm_model <- lm(G3 ~ ., data = sd)
# Summary
summary_lm <- summary(lm_model)
print(summary_lm)
# Extract coefficients and p-values
coef_summary <- data.frame(
Estimate = round(summary_lm$coefficients[, "Estimate"], 4),
P_value = round(summary_lm$coefficients[, "Pr(>|t|)"], 4)
)
print(coef_summary)
tree_model <- rpart(G3 ~ ., data = sd)
# Visualize the tree
rpart.plot(tree_model, shadow.col = "gray")
rpart.plot(tree_model, extra = 101, fallen.leaves = TRUE, type = 4, main = "Regression Tree")
# Visualize the tree
rpart.plot(tree_model, shadow.col = "gray")
# Removing variables that were least important
# during regression and trees
sd$traveltime <- NULL
View(sd)
plot(sd$studytime, sd$G3, col = "blue", pch = 19)
grid()
tree_model <- rpart(G3 ~ ., data = sd)
# Visualize the tree
rpart.plot(tree_model, shadow.col = "gray")
rpart.plot(tree_model, extra = 101, fallen.leaves = TRUE, type = 4, main = "Regression Tree")
# Get variable importance
var_importance <- tree_model$variable.importance
print(var_importance)
# Variable Importance Plot
var_importance <- data.frame(
variable = names(tree_model$variable.importance),
importance = tree_model$variable.importance
)
ggplot(var_importance, aes(x = reorder(variable, importance), y = importance)) +
geom_bar(stat = "identity", fill = "skyblue") +
coord_flip() +
theme_minimal() +
labs(title = "Variable Importance", x = "Variables", y = "Importance")
# Now doing the normal regression
lm_model <- lm(G3 ~ ., data = sd)
# Summary
summary_lm <- summary(lm_model)
print(summary_lm)
# Extract coefficients and p-values
coef_summary <- data.frame(
Estimate = round(summary_lm$coefficients[, "Estimate"], 4),
P_value = round(summary_lm$coefficients[, "Pr(>|t|)"], 4)
)
print(coef_summary)
# Using this as the Data
sd <- student_por
# Removing these because that would just be cheating
sd$G2 <- NULL
sd$G1 <- NULL
# Removing variables that were least important
# during regression and trees
sd$traveltime <- NULL
sd$Medu <- NULL
sd$guardian <- NULL
sd$reason <- NULL
sd$Mjob <- NULL
sd$address <- NULL
View(sd)
plot(sd$studytime, sd$G3, col = "blue", pch = 19)
grid()
tree_model <- rpart(G3 ~ ., data = sd)
# Visualize the tree
rpart.plot(tree_model, shadow.col = "gray")
rpart.plot(tree_model, extra = 101, fallen.leaves = TRUE, type = 4, main = "Regression Tree")
# Get variable importance
var_importance <- tree_model$variable.importance
print(var_importance)
# Variable Importance Plot
var_importance <- data.frame(
variable = names(tree_model$variable.importance),
importance = tree_model$variable.importance
)
ggplot(var_importance, aes(x = reorder(variable, importance), y = importance)) +
geom_bar(stat = "identity", fill = "skyblue") +
coord_flip() +
theme_minimal() +
labs(title = "Variable Importance", x = "Variables", y = "Importance")
# Now doing the normal regression
lm_model <- lm(G3 ~ ., data = sd)
# Summary
summary_lm <- summary(lm_model)
print(summary_lm)
# Extract coefficients and p-values
coef_summary <- data.frame(
Estimate = round(summary_lm$coefficients[, "Estimate"], 4),
P_value = round(summary_lm$coefficients[, "Pr(>|t|)"], 4)
)
print(coef_summary)
# Plot residuals
plot(lm_model, which = 1)  # Residuals vs Fitted
plot(lm_model, which = 2)  # Normal Q-Q plot
# Compare predictions: Linear Regression vs Regression Tree
sd$lm_pred <- predict(lm_model)
sd$tree_pred <- predict(tree_model)
# Calculate MSE for both models
mse_lm <- mean((sd$G3 - sd$lm_pred)^2)
mse_tree <- mean((sd$G3 - sd$tree_pred)^2)
cat("MSE (Linear Regression):", mse_lm, "\n")
cat("MSE (Regression Tree):", mse_tree, "\n")
print("MSE (Linear Regression):", mse_lm, "\n")
print(mse_lm)
print(mse_tree)
# Visualize predictions
ggplot(sd, aes(x = G3)) +
geom_point(aes(y = lm_pred, color = "Linear Regression"), alpha = 0.5) +
geom_point(aes(y = tree_pred, color = "Regression Tree"), alpha = 0.5) +
geom_abline(intercept = 0, slope = 1, linetype = "dashed") +
theme_minimal() +
labs(title = "Predicted vs Actual G3 Scores",
x = "Actual G3", y = "Predicted G3",
color = "Model") +
scale_color_manual(values = c("Linear Regression" = "blue", "Regression Tree" = "red"))
# Variable importance comparison
lm_importance <- abs(coef(lm_model)[-1])  # Exclude intercept
importance_df <- data.frame(
Variable = names(lm_importance),
Linear_Regression = lm_importance,
Regression_Tree = var_importance$importance[match(names(lm_importance), var_importance$variable)]
)
importance_df <- importance_df[order(-importance_df$Linear_Regression), ]
# Plot variable importance comparison
importance_long <- tidyr::pivot_longer(importance_df, cols = c("Linear_Regression", "Regression_Tree"), names_to = "Model", values_to = "Importance")
ggplot(importance_long, aes(x = reorder(Variable, Importance), y = Importance, fill = Model)) +
geom_bar(stat = "identity", position = "dodge") +
coord_flip() +
theme_minimal() +
labs(title = "Variable Importance Comparison", x = "Variables", y = "Importance") +
scale_fill_manual(values = c("Linear_Regression" = "blue", "Regression_Tree" = "red"))
# Visualize predictions
ggplot(sd, aes(x = G3)) +
geom_point(aes(y = lm_pred, color = "Linear Regression"), alpha = 0.5) +
geom_point(aes(y = tree_pred, color = "Regression Tree"), alpha = 0.5) +
geom_abline(intercept = 0, slope = 1, linetype = "dashed") +
theme_minimal() +
labs(title = "Predicted vs Actual G3 Scores",
x = "Actual G3", y = "Predicted G3",
color = "Model") +
scale_color_manual(values = c("Linear Regression" = "blue", "Regression Tree" = "red"))
install.packages('plotly')
# Load necessary libraries
library(rpart)
library(rpart.plot)
library(plotly)
# Assuming 'sd' is your dataset
# Create a tree with just two splits
tree_model_2splits <- rpart(G3 ~ ., data = sd, control = rpart.control(maxdepth = 2))
# Print the tree structure
print(tree_model_2splits)
# Plot the tree
rpart.plot(tree_model_2splits, extra = 101, fallen.leaves = TRUE, type = 4, main = "Decision Tree with Two Splits")
# Using this as the Data
sd <- student_por
# Removing these because that would just be cheating
sd$G2 <- NULL
sd$G1 <- NULL
# Removing variables that were least important
# during regression and trees
sd$traveltime <- NULL
sd$Medu <- NULL
sd$guardian <- NULL
sd$reason <- NULL
sd$Mjob <- NULL
sd$address <- NULL
View(sd)
plot(sd$studytime, sd$G3, col = "blue", pch = 19)
grid()
# Assuming 'sd' is your dataset
# Create a tree with just two splits
tree_model_2splits <- rpart(G3 ~ ., data = sd, control = rpart.control(maxdepth = 2))
# Print the tree structure
print(tree_model_2splits)
# Plot the tree
rpart.plot(tree_model_2splits, extra = 101, fallen.leaves = TRUE, type = 4, main = "Decision Tree with Two Splits")
# Define custom splitting function
custom_split <- function(y, wt, x, parms, continuous) {
# First split on failures
failures_split <- y$failures >= 1
if (length(unique(failures_split)) == 1) {
return(NULL)  # No split possible
}
# For records with failures >= 1, split on higher
higher_split <- y$higher == "yes" & failures_split
# For records with failures < 1, split on absences
absences_split <- y$absences >= 1 & !failures_split
# Combine splits
split <- as.integer(failures_split) + 2 * as.integer(higher_split) + 2 * as.integer(absences_split)
# Calculate improvement
improvement <- var(y$G3) - (sum((split == 0) * wt) * var(y$G3[split == 0]) +
sum((split == 1) * wt) * var(y$G3[split == 1]) +
sum((split == 2) * wt) * var(y$G3[split == 2]) +
sum((split == 3) * wt) * var(y$G3[split == 3])) / sum(wt)
list(goodness = improvement,
direction = split)
}
# Create the custom tree model
custom_tree <- rpart(G3 ~ failures + higher + absences, data = sd,
method = list(eval = custom_split),
control = rpart.control(maxdepth = 2, minsplit = 1, minbucket = 1))
# Plot the tree
rpart.plot(custom_tree, extra = 101, fallen.leaves = TRUE, type = 4,
main = "Custom Decision Tree with Specified Splits")
# Create the custom tree model
custom_tree <- rpart(G3 ~ failures + higher + absences, data = sd,
method = list(eval = custom_split),
control = rpart.control(maxdepth = 2, minsplit = 1, minbucket = 1))
# Define custom splitting function
custom_split <- function(x, y, wt, parms, continuous) {
# First split on failures
failures_split <- x[, "failures"] >= 1
if (length(unique(failures_split)) == 1) {
return(NULL)  # No split possible
}
# For records with failures >= 1, split on higher
higher_split <- x[, "higher"] == "yes" & failures_split
# For records with failures < 1, split on absences
absences_split <- x[, "absences"] >= 1 & !failures_split
# Combine splits
split <- as.integer(failures_split) + 2 * as.integer(higher_split) + 2 * as.integer(absences_split)
# Calculate improvement
improvement <- var(y) - (sum((split == 0) * wt) * var(y[split == 0]) +
sum((split == 1) * wt) * var(y[split == 1]) +
sum((split == 2) * wt) * var(y[split == 2]) +
sum((split == 3) * wt) * var(y[split == 3])) / sum(wt)
list(goodness = improvement,
direction = split)
}
# Create the custom tree model
custom_tree <- rpart(G3 ~ failures + higher + absences, data = sd,
method = list(split = custom_split),
control = rpart.control(maxdepth = 2, minsplit = 1, minbucket = 1))
### third try custom split
# Load required libraries
library(rpart)
library(rpart.plot)
# Create the tree model with manual splits
tree_model <- rpart(G3 ~ failures + higher + absences, data = sd,
control = rpart.control(maxdepth = 2, minsplit = 2, minbucket = 1, cp = -1),
model = TRUE)
# Manually modify the tree structure
tree_model$splits <- tree_model$splits[1:3, ]
tree_model$splits[1, "index"] <- which(names(sd) == "failures")
tree_model$splits[2, "index"] <- which(names(sd) == "higher")
tree_model$splits[3, "index"] <- which(names(sd) == "absences")
tree_model$splits[1, "ncat"] <- 1  # Numeric split
tree_model$splits[2, "ncat"] <- 2  # Categorical split (yes/no)
tree_model$splits[3, "ncat"] <- 1  # Numeric split
tree_model$splits[1, "adj"] <- 0.5  # Split at failures >= 1
tree_model$splits[2, "adj"] <- 1.0  # Split at higher == "yes"
tree_model$splits[3, "adj"] <- 0.5  # Split at absences >= 1
# Recalculate the frame
tree_model$frame$var[2:4] <- c("failures", "higher", "absences")
tree_model$frame$ncompete <- tree_model$frame$nsurrogate <- 0
tree_model$frame$yval <- sapply(split(sd$G3, tree_model$where), mean)
# Plot the modified tree
rpart.plot(tree_model, extra = 101, fallen.leaves = TRUE, type = 4,
main = "Manual Decision Tree with Specified Splits")
rstudioapi::addTheme('https://raw.githubusercontent.com/johnnybarrels/rstudio-one-dark-pro-theme/master/OneDarkPro.rstheme', apply=TRUE, force=TRUE)
rstudioapi::addTheme('https://raw.githubusercontent.com/johnnybarrels/rstudio-one-dark-pro-theme/master/OneDarkPro.rstheme', apply=TRUE, force=TRUE)
# Data-analysis
install.packages('rstudioapi')
rstudioapi::addTheme('https://raw.githubusercontent.com/johnnybarrels/rstudio-one-dark-pro-theme/master/OneDarkPro.rstheme', apply=TRUE, force=TRUE)
